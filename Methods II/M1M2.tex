\documentclass[10pt]{scrartcl}

%\usepackage[blacklinks]{drangreport}
 \usepackage[bluelinks]{drangreport}
 \usepackage{enumerate}

\usepackage{stmaryrd}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\usepackage[makeroom]{cancel}
\newcommand{\vertequiv}{\rotatebox{90}{$\,\equiv$}}
\newcommand{\equivto}[2]{\underset{\scriptstyle\overset{\mkern4mu\vertequiv}{#2}}{#1}}
%\renewcommand*\rmdefault{cmss}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\title{Methods II}

\let\Im\relax
\DeclareMathOperator*{\Im}{Im}

\newcommand{\herm}[1]{\overline{#1}^\Trans}
\renewcommand{\l}{\lambda}
\DeclareMathOperator*{\ch}{ch}
\renewcommand{\chi}{\textstyle\ch}
\usepackage{wrapfig}



\usepackage{circuitikz}
%\renewcommand*\rmdefault{cmss}



\hypersetup{pdfinfo={
Title={M1M2: Mathematical Methods II Lecture Notes},
Author={Karim Bacchus},
Keywords ={Mathematical Methods II, M1M2, Lecture Notes, Imperial College, Maths},
}}


\begin{document}

\NotesSubTitle{1st}{Spring 2016}{Mathematical Methods II}{Prof.~M.}{Barahona}
{
\subsection*{Syllabus}

\textit{This course continues and extends the techniques introduced in M1M1, with further differential equations and partial differentiation.}


\emph{Differential Equations:} First and second order differential equations. Homogeneous and inhomogeneous linear differential equations. Systems of linear differential equations – matrix solution. Phase Plane Analysis: Qualitative analysis of solutions of differential equations and stability. Bifurcation of first order non-linear differential equations.

\emph{Multivariable Calculus:}
 Partial differentiation: Definitions, implicit partial differentiation, total differential, change of variables. Taylor’s theorem, stationary pointe and their classification, contours. Definitions and physical meaning of grad, div, curl.  Optimisation and Lagrange multipliers. Area under curves, arc length, surface area and volume of revolution; double integrals – geometry, mass, moments of inertia; simple triple integrals.\\

\subsection*{Appropriate books}

For the first part of the course on Ordinary Differential Equations:

{\shortskip 
W.~E.~Boyce. \emph{Elementary differential equations and boundary value problems}, Wiley.

M.~Braun, \emph{Differential equations and their applications}, Springer Verlag.

F. Diacu, \emph{An introduction to differential equations}, Freeman

R. Redheffer, \emph{Differential equations: theory and applications}, Jones and Bartlett.\\


For the section on qualitative analysis of nonlinear ODEs:

S.H. Strogatz, \emph{Nonlinear Dynamics and Chaos: With Applications to Physics, Biology, Chemistry and Engineering}, Westview Press\\


For the second part of the course on Introduction to multivariate calculus:

H.M. Schey, \emph{Div, grad, curl and all that}, Norton and Company
}}


\TableofContents

%!TEX root = M1M2.tex
\setcounter{page}{3}

\addcontentsline{toc}{part}{Differential Equations}{}

\sektion{ODEs}
\subsektion{Notation, definitions}
\vspace*{5pt}


For \lecturemarker{1}{14 Jan}
this section we're only going to consider ordinary ($\equiv$ functions of one variable) differential equations:

\begin{align*}
&f: \R \to \R & f(x) \in \R\\
&\vec{f}: \R \to \R^n & \begin{pmatrix}f_1(x)\\\vdots\\f_n(x)\end{pmatrix} \in \R^n
\end{align*}
Where $x$ is the \emph{independent variable}. 

We can differentiate these functions up to order $k$ as such:
\[
\begin{aligned}
  \frac{df}{dx} &= \lim_{\Delta \to 0} \frac{f(x + \Delta) - f(x)}{\Delta} = g(x)\\[0.2cm]
  \frac{dg}{dx} &= \lim_{\Delta \to 0} \frac{g(x + \Delta) - g(x)}{\Delta} = \frac{d^2f}{dx^2}\\
  \vdots
\end{aligned}
\]~


\begin{definition}
An \emph{ordinary differential equation} of order $k$ is of the form
\[F\left(x,f(x),\frac{df}{dx},\dots,\frac{d^kf}{dx^k}\right) = 0\]

where $x$ is the independent variable.
\end{definition}

We want to find $f(x)$ to such that $F\left(x,f(x),\frac{df}{dx},\dots,\frac{d^kf}{dx^k}\right) = 0$.

\begin{itemize}
\item \emph{Ordinary differential equations (ODE)} occur when $f: \R \to R$ and $f(x)$ is a function of one variable. Usually $x$ is the independent variable (sometimes $t$). 

\item The \emph{order of the ODE} is the order of the highest derivative involved. 
\item The \emph{degree of the ODE} is the degree of the highest derivative.
\end{itemize}

e.g. The differential equation \[\dfrac{d^2y}{dx^2} - \left(x + \dfrac{dy}{dx}\right)^{1/5} = 0\]
 has order of $2$ and degree $5$.~\\ 

\begin{example}[Sources of ODEs]
\begin{enumerate}
  \item Mechanics: 
  \[\dfrac{dx}{dt} = v \implies F(t, x) = 0 = \dfrac{dx}{dt} - v \tag{Newton's Laws}\]
  
  \item Population Dynamics: \[\dfrac{dx}{dt} = Kx \implies x = x_0e^{kt}\tag{Maltus}\]
  
  \item Chemical Reactions, e.g. Mass-action Kinetics: 
  \[A +A \xrightarrow{k_1} B,\quad  \frac{d[B]}{dt} = k_1[A]^2\]
  \item Electrical Engineering: For a circuit, $I_1 + I_2 = I_T$ and $V= I_1R$. 
   \begin{center} 
     \begin{circuitikz}
      \draw (0,0)
      to[short] (0,2) % The voltage source
      to[short,i = $I_1$] (1,2);
      \draw (0,2) 
      to[R = $R$] (3,2)
      to[short] (3,0) % The resistor
      to[C=$C$] (0,0);
      \draw (0,0) 
      to [short, i= $I_2$] (1,0);
      \draw (3,1)
      to[short] (4,1);
      \draw (-1,1)
      to[short,i = $I_T$] (0,1);
   \end{circuitikz}  
   \end{center}  

Also $C\dfrac{dV}{dt} = \dfrac{dQ}{dt} = I_2$ so \[I_T = \frac{V}{R} + C\frac{dV}{dt}\]

\item Economics \& Game Theory
\end{enumerate}
\end{example}

\subsektion{Types of Differential Equations}
\begin{enumerate}
  \item ODE: $f: \R \to R$: \[F\left(x,f(x),\frac{df}{dx},\dots\right) = 0\]
 

  \item System of ODEs: $\vec{f}: \R \to \R^d$: 
   \[\begin{pmatrix}
f_1(x)\\ \vdots \\ f_d(x)	
\end{pmatrix} = \vec{f}(x) = \begin{cases}
 \dots\\ F_i\left(x,\dots,f_i(x),\dots,\frac{df_i}{dx},\dots\right) = 0\\
 \dots	
 \end{cases}
 \]
 
 \item Difference Equations (Maps, Discrete Systems): $f: \ZZ \to \R$
 \item Partial Differential Equations (PDEs): $f: \R^n \to \R$
\end{enumerate}~


\lecturemarker{2}{18 Jan}

\begin{definition}
The \textbf{Implicit form} of the ODE is
\[F\left(x,y,\frac{dy}{dx},\dots,\frac{d^ky}{dx^k}\right)= 0\]
\end{definition}

\begin{definition}
The \textbf{Explicit form} of the ODE is
\[\frac{d^ky}{dx^k} = G\left(x,y,\dots,\frac{d^{k-1}y}{dx^{k-1}}\right)\]
\end{definition}

\begin{definition}
$y_{PS}$ is a \emph{particular solution} if 
\[F\left(x,y_{PS},\frac{dy_{PS}}{dx},\dots,\frac{d^ky_{PS}}{dx^k}\right)= 0\]
\end{definition}

\begin{note}
Proofs about the existence and uniqueness of the solutions are covered next year. (M2AA2)
\end{note}

\begin{definition}
$y_{GS} = y_{GS}(x;\ c_1,\dots,c_k)$ is the \emph{general solution} for an ODE of order $k$.

 The $k$ parameters, $\{c_i\}_{i=1}^k$ are initial / boundary conditions $\equiv$ constants of integration.
 %define the family of solutions. These parameters can be fixed or obtained from the initial/boundary conditions.
\end{definition}\vspace*{5pt}

\begin{example}[Velocity]
\[\frac{dx}{dt} = v_*\]
This is a first order ODE $\implies \{c_1\}$, so the general solution is:
\[x(t) = v_*t + c_1\]
In this case, $c_1$ would be the initial position, $x_0$.
\end{example}



\subsektion{First Order ODEs}
The explicit form for 1st Order ODEs is
\[\frac{dx}{dt} = F(x,t)\]
\[x_{GS} = x_{GS}(t)\]

\begin{enumerate}[(1)]
% todo make into list
\item \textbf{Separable Equations}

\[
\begin{aligned}
  \frac{dx}{dt} &= F_1(x)F_2(t)\\
  \implies \int \frac{dx}{F_1(x)} &= \int F_2(t)\,dt + C
\end{aligned}
\]~

\begin{example}[Malthus' Equation (population dynamics)]
\[
\begin{aligned}
  \frac{dx}{dt} &= Kx\\
  \implies \int \frac{dx}{x} &= K\int\, dt\\
  \implies \log x &= Kt + C\\
  \implies x(t) &= \equalto{e^c}{x(0)}\,e^{kt}
\end{aligned}
\]
\end{example}

\item \textbf{Linear 1st Order ODE}

The general form of a $k$th order linear ODE is: 
\[\sum_{i=0}^k \alpha_i(x) \frac{d^iy}{dx^i} = \beta (x)\]
Where $\alpha_i(x)$ are only depend on the independent variable $(x)$. 

So the 1st Order ODE is: 
\[\frac{dy}{dx} + p(x)y = q(x)\]

Solved by finding an \emph{integrating factor}, $I(x)$:
\begin{equation}
\label{eqn:intfac}\ I(x)[LHS] = I(x)\left[\frac{dy}{dx} + p(x)y\right] = \frac{d(Iy)}{dx}
\end{equation}

Once it's in this form, we can integrate and solve as usual: 
\[
\begin{aligned}
  \frac{d(Iy)}{dx} &= I(x)q(x)\\
  \implies Iy &= \int I(x)q(x)\,dx\\
  \implies y &= \frac{1}{I} \int I(x)q(x)\,dx + C
\end{aligned}
\]

Finding $I(x)$ (by~\ref{eqn:intfac}): 
\begin{align*}
    \frac{d(Iy)}{dx} &= I\frac{dy}{dx} + y\frac{dI}{dx}\\
    &= I\frac{dy}{dx} + Ip(x)y
\end{align*}
\[
\begin{aligned}
 \implies  \frac{dI}{dx} &= Ip(x)\\[.1cm]
  \implies \int \frac{dI}{I} &= \int p(x)\,dx\\[.1cm]
  \implies \Aboxed{I(x) &= Ke^{\int p(x)\,dx}}
\end{aligned}
\]
Then our solution would be
\[y_{GS} = e^{-\int p(x)\,dx}\cdot \int e^{\int p\,dx}q(x)\,dx + c_1 e^{-\int p(x)\,dx}\]
Note that the $K$'s end up cancelling out, so we haven't added to the number of unknown constants in our final solution. It's a good idea to just remember the integrating factor $I(x) = e^{\int p(x)}\,dx$ rather than memorise the full solution of $y$. 

\item \textbf{(Dimensionally) Homogeneous Equations}
These are equations of the form:
\[\frac{dy}{dx} = f\left(\frac{y}{x}\right)\]

We solve them by substituting in $u = \dfrac{y}{x}$, so $y = ux$ (\textit{redifine ratio as the independent variable}):

\[
    \frac{dy}{dx} = u + x \frac{du}{dx}
\]
\[
\begin{aligned}
  \implies u + x\frac{du}{dx} &= f(u)\\
  \implies \int \frac{du}{f(u) -u} &= \int \frac{dx}{x} + C
\end{aligned}
\]
\begin{note}These are often just called homogeneous equations in the literature.\end{note}


\item \textbf{Bernoulli Equation}

These are equations of the form: \[\dfrac{dy}{dx} + p(x)y = Q(x)y^n,\ n \in \N\]
\[\implies \frac{1}{y^n}\frac{dy}{dx} + y^{1-n}p(x) = Q(x)\]

Substitute $u = y^{1-n}$, then $\dfrac{du}{dx} = (1-n)y^{-n}\dfrac{dy}{dx}$, so the equation becomes

\[  \frac{du}{dx} + (1-n)p(x)u = (1-n)Q(x)\]

This is a 1st order linear ODE, which we can solve for $u = u(x)$ using an integrating factor. Then we solve for $y = u^{\frac{1}{1-n}}$. 
\end{enumerate}

\lecturemarker{3}{19th Jan}
\subsektion{Second Order ODEs}
The implicit form of second order ODE is:
\[F_C\left(x,y,\frac{dy}{dx}, \frac{d^2y}{dx^2}\right) = 0\]

Assume we can find the explicit form: 
\[\frac{d^2y}{dx^2} = F\left(x,y,\frac{dy}{dx}\right)\]

We'll consider several different cases
\begin{enumerate}[(1)]
\item $\dfrac{d^2y}{dx^2} = f(x)$

We solve this by double integration. Let $u = \dfrac{dy}{dx}$, then $\dfrac{du}{dx} = F(x)$ and
\[u = \underbrace{\int f(x)\,dx}_{g(x)} + c_1\]
Then 
\[\begin{aligned}\frac{dy}{dx} &= g(x) + c_1\\
y &= \int g(x)\,dx + c_1x + c_2	
\end{aligned}
\]~

\begin{example}[Mechanics] Writing acceleration as $\frac{d^2x}{dt^2}$
	\[
\begin{aligned}
  \frac{d^2x}{dt^2} &= a(t)\\[.2cm]
\implies   \frac{dx}{dt} &= u = \int a(t)\,dt + c_1\\[.2cm]
\implies  \frac{dx}{dt} &= v(t) + c_1\\[.2cm]
\implies   x(t) &= \int v(t)\,dt + c_1t + c_2
\end{aligned}
\]
\end{example}

\item $\dfrac{d^2y}{dx^2} = f\left(x,\dfrac{dy}{dx}\right)$

We again use the substitution $u = \dfrac{dy}{dx} \implies \dfrac{du}{dx} = f(x,u).$\\[.2cm]

\begin{example}[Geometry]
Radius of curvature: 
\[\rho(x) = \frac{\left[1 + \left(\frac{dy}{dx}\right)^2\right]^{3/2}}{\frac{d^2y}{dx^2}}\]

Find the family of cures with constant radius of curvature: $\rho(x) = R,~\forall x$. 

\[
\begin{aligned}
  R &= \frac{\left[1 + \left(\frac{dy}{dx}\right)^2\right]^{3/2}}{\frac{d^2y}{dx^2}}\\[.3cm]
\implies   R\frac{d^2y}{dx^2} &= \left[1 + \left(\frac{dy}{dx}\right)^2\right]^{3/2} = f\left(\frac{dy}{dx}\right)\\
\end{aligned}
\]

Let $u = \dfrac{dy}{dx}$. Then we have
\[R\frac{dy}{dx} = [1 + u^2]^{3/2}\]

This is a separable ODE, so we just have to rearrange and integrate: 
\begin{align*}
	\int \frac{du}{(1+u^2)^{3/2}} &= \frac{1}{R}\int \,dx + c_1\\[.2cm]
	&= \left[\frac{x}{R} + c_1\right]
\end{align*}

\textsc{Step 1:} Let $u = \tan t$, so $1 + u^2 = \frac{1}{\cos^2t}$ and $\mathrm{d}u = \frac{\mathrm{d}t}{\cos^2t}$. Then the LHS becomes
\[
\begin{aligned}
  \int \frac{\frac{1}{\cos^2t}\,dt}{\frac{1}{\cos^2t}} &= \int \cos t\,dt\\
  &= \sin t\\
  &= \sqrt{1-\cos^2t}\\
  &= \frac{u}{\sqrt{1+u^2}}
\end{aligned}
\]

So we are left with
\[\frac{u}{\sqrt{1+u^2}} = \left[\frac{x}{R}+c_1\right] \tag{$*$}\]
\textsc{Step 2:} Now let $X = \frac{x}{R} + c_1$. Then $(*)$ becomes
\setlength{\jot}{8pt}
\[
\begin{aligned}
  \frac{u^2}{1+u^2} &= X^2\\
\implies   u^2(1-X^2) &= X^2\end{aligned}
\]

Recall that $u = \frac{dy}{dx}$, so we have
\setlength{\jot}{8pt}
\[
\begin{aligned}
  u = \frac{X}{\sqrt{1-X^2}} &= \frac{dy}{dx}
\end{aligned}
  \]
Also  $\mathrm{d}X = \frac{\mathrm{d}x}{R}$, so 
\[
    \frac{1}{R}\frac{dy}{dX} = \frac{X}{\sqrt{1-X^2}}
\]

Integrating
  \[\begin{aligned}  
  \frac{1}{R}\int\,dy &= \int \frac{X\,dX}{\sqrt{1-X^2}}\\
  \implies   \frac{1}{R}\, y &= -\sqrt{1-X^2} + c_2\\
\implies   \left(\frac{y}{R}-c_2\right)^2 &= 1-\left(\frac{x}{R} + c_1\right)^2
\end{aligned}
\]

Letting $k_1 = Rc_1,~ k_2 = Rc_2$ our solution is
\[(x-k_1)^2 + (y-k_2)^2 = R^2\]
i.e. a circle, as expected to have a constant radius of curvature.	
\end{example}\vsp

\item $\dfrac{d^2y}{dx^2} = f(y)$

Again, we use the substitution $u = \dfrac{dy}{dx}\implies \dfrac{du}{dx} = f(y)$. Since \[\dfrac{du}{dx} = \dfrac{du}{dy}\dfrac{dy}{dx}
  = u\dfrac{du}{dy}
  =\dfrac{d(u^2/2)}{dy}\]the equation becomes \[\dfrac{d(u^2/2)}{dy} = f(y)\]

\textsc{1st Step:} Integrate the separable equation\[
\begin{aligned}
  \frac{u^2}{2} &= \int f(y)\,dy + c_1\\
  \implies u &= u(y)
\end{aligned}
\]
This is not what we were after..\\

\textsc{2nd Step:}
\[
\begin{aligned}
  \frac{u^2}{2} &= g(y) + c_1\\
  u^2 &= 2g(y) + 2c_1\\
  \frac{dy}{dx} &= \sqrt{2g(y) + 2c_1}\\
  \int \frac{dy}{\sqrt{2g(y)+2c_1}} &= \int \,dx + c_1\\
  x &= g(y)\\
  y &= g^{-1}(x)
\end{aligned}
\]~\\

\begin{example}[Harmonic Spring]\lecturemarker{4}{5 Oct}
	Hooke's Law: $F = -kx$
	
	\[m\frac{d^2x}{dt^2} = -kx\]
	
\textsc{1st Step:}
Let $u = \dfrac{dx}{dt}$
\[
\begin{aligned}
  m\frac{d^2x}{dt^2} = m\frac{du}{dt} &= -kx\\
  m\frac{du}{dx}\frac{dx}{dt} &= mu\frac{du}{dx} = m\frac{d(u^2)/2}{dx}
\end{aligned}
\]
\[
\begin{aligned}
  \frac{d(u^2/2)}{dx} &= -\frac{k}{m}x\\
  \frac{u^2}{2} &= -\frac{k}{m}\int x\,dx = -\frac{k}{2m}x^2 + c_1\\
\end{aligned}
\]
\[\frac{m}{2}u^2 + \frac{k}{2}x^2 = mc_1 \equiv E, \text{ energy}\]

\textsc{2nd Step:}
\[
\begin{aligned}
  u = \frac{dx}{dt} &= \sqrt{\frac{2E -kx^2}{m}}\\
  \int \frac{dx}{\frac{\sqrt{2E-kx^2}}{m}} &= \int \,dt = t +c\\
  \frac{1}{\sqrt{\frac{2E}{m}}}\int \frac{dx}{\sqrt{1-\frac{k}{2E}x^2}} &=  \frac{1}{\sqrt{\frac{2E}{m}}} \int \frac{dx\sqrt{\frac{k}{2E}}}{\sqrt{1-\left(\sqrt{\frac{k}{2E}}x\right)^2}}\frac{1}{\sqrt{\frac{k}{2E}}}\\
  &= \frac{1}{\sqrt{\frac{k}{m}}}\arcsin\left(\sqrt{\frac{k}{2E}}x\right) = t+c\\
  x(t) &= \sqrt{\frac{2E}{k}}\sin\left(\sqrt{\frac{k}{m}}t + c_2\right)\\
  &= A\sin(\omega t + \phi)
\end{aligned}
\]
\end{example}

\item $\dfrac{d^2y}{dx^2} = f\left(y,\dfrac{dy}{dx}\right)$

\textsc{1st Step:}

Let $u = \dfrac{dy}{dx}$, then 
\[\frac{d(u^2)/2}{dy} = f(y,u)\]
Find $u = u(y)$.\\

\textsc{2nd Step:}
Let $\dfrac{dy}{dx} = u(y)$. Integrate and solve:
\[\int \frac{dy}{u(y)} = \int\,dx + c\]~

\begin{example}[Spring]
	\[m\frac{d^2x}{dt^2} = -kx - 2\beta\left(\frac{dx}{dt}\right)^2\]
	
	1st Step: 
	\[\frac{d^2u}{dt^2} = \frac{d(u^2/2)}{dx} = -\frac{k}{m}x - 2\frac{\beta}{m}u^2\]
	
	Let $U = \dfrac{u^2}{2}$, so 
	\[\frac{dU}{dx} + 4\frac{\beta}{m}U = -\frac{k}{m}x\]
	\[\implies U = ce^{-4\frac{\beta}{m}x} - \frac{k}{4\beta}\left(x - \frac{m}{4\beta}\right) = U(x) = \frac{u^2}{2}\]

	2nd Step: 
	\[
\begin{aligned}
  \frac{dx}{dt} = u &= \sqrt{2ce^{-4\frac{\beta}{m}x} - \frac{k}{2\beta}\left(x-\frac{m}{4\beta}\right)}\\
  \int \frac{dx}{\sqrt{\dots}} &= \int \,dt + c_2
\end{aligned}
\]
\end{example}
\end{enumerate}


\sektion{Linear Equations (ODEs)}


A linear ODE of order $k$ is:
\[\sum_{i=0}^k \alpha_i(x) \frac{d^iy}{dx^i} = f(x) \tag{$*$}\]

For a 1st order linear ODE, the solution is found by an Integrating Factor. 

The general structure of the solution of a linear ODE is: 
\[\frac{d}{dx}[y] = \mathcal{D}[y]\]

Linearity of $\mathcal{D}$:
\[\mathcal{D}(\lambda_1y_1 + \lambda_2y_2) = \lambda_1\mathcal{D}(y_1) +\lambda_2\mathcal{D}(y_2)\]

It follows inductively that $\mathcal{D}^i$ is also linear. Back to our linear ODE $(*)$: 
\[
\begin{aligned}
  \sum_{i=0}^k \equalto{\alpha_i(x)}{\mathcal{L}_{\vec{\alpha}}[y]}\mathcal{D}^i[y] &= f(x)\\
  \implies \mathcal{L}_{\vec{\alpha}}[y_1 + y_2] &= \mathcal{L}_{\vec{\alpha}}[y_1] + \mathcal{L}_{\vec{\alpha}}[y_2]
\end{aligned}
\]

Implications for the structures of the solutions of ODEs:

\[\mathcal{L}_{\vec{\alpha}}[y] = f(x)\]

Solve: Find $y_{GS}(x; c_1,\dots,c_k)$. We can split the solution into two parts: 

\begin{enumerate}
  \item Homogeneous problem: 
  \[\mathcal{L}_{\vec{\alpha}} = 0\vspace*{-5pt}\]
    \end{enumerate}\vspace*{5pt}
    
    \begin{definition}
  The solution to the homogenous problem, $y_{GS}^{(H)}$ is the \emph{Complementary function}: 	
    \[y_{GS}^{(H)}(x;c_1,\dots,c_k) = y_{CF}\]
  \end{definition}\vspace*{5pt}


\begin{enumerate}
  \item[(ii)] Particular solution: 
  
  We find one (any) solution to the full (non-homogeneous) problem:
    \[\mathcal{L}_{\vec{\alpha}}[y_{PI}] = f(x)\]
    
\end{enumerate}  

    
\begin{definition}
  $y_{PI}(x)$ is called the \emph{particular integral}.
  \end{definition}
 
  From linearity: 
  \[\mathcal{L}_{\vec{\alpha}}[y_{GS}^{(H)}(x;c_1,\dots,c_k)+y_{PI}] = f(x)\]
  
  The solution of the full problem is written as the solution to the two singular problems. 

\begin{theorem}\lecturemarker{5}{5 Oct}
The solutions to $\mathcal{L}_{\vec{\alpha}}[y]=0$ form a vector space of dimension $k$. 	
\end{theorem}
\textit{Proof.} See next year. 

$(\mathcal{F}, +)$ form a group, i.e. it has the properties:
\begin{enumerate}
  \item Closure: $\mathcal{L}[y_1] = 0, \mathcal{L}[y_2] = 0, \mathcal{L}[y_1 + y_2] = 0$
  \item Associativity
  \item Identity element: $\mathcal{L}[0] = 0$
  \item Inverses exist
\end{enumerate}

The groups: $(\R, +, \cdot)$ and $(\mathcal{F}, +)$ together form a vector Space: $(\mathcal{F},+,\dot{\R})$. The solutions to $\mathcal{L}_{\vec{\alpha}}[y] = 0$ can be expressed as a linear combination:
\[y = \sum_{i=1}^kc_iy_i(x)\]
where $\left\{y_i(x)\right\}_{i=1}^k$ form a basis. So for the complementary function:
\[y_{CF} = y_{GS}^{(H)}(x;c_1,\dots,c_k) = \sum_{i=1}^kc_iy_i(x)\]

To solve the homogenous problem we need to find $k$ solutions of the problem that are linearly independent. 

\emph{How do we check that a set of $k$ functions are linearly independent?}\\

\begin{definition}
The \emph{Wronskian} matrix:
\[W = 
\begin{pmatrix}
y_1 & \dots & y_k\\
y'_1 & \dots & y'_k\\
\vdots & & \vdots\\
y^{(k-1)}_1 & \dots & y^{(k-1)}_k	
\end{pmatrix}
\]	
\end{definition}

\begin{theorem}
A set is linearly independent $\iff \det(W) \neq 0$	
\end{theorem}
\begin{proof}

First $\det(W) \neq 0 \implies$ linear independence: 

By contradiction. Assume our vectors are linear dependent. Then $\exists c_i \neq 0 \mbox{ such that }$
 \[ \sum_{i=1}^k c_i\cdot y_i(x) = 0\]
 
 Taking derivatives: 
 \[\begin{rcases*}
\sum_{i=1}^k c_iy_i = 0\\
\vdots\\
\sum_{i=1}^k c_iy_i^{(k-1)} = 0	
\end{rcases*} k \text{ equations}
\]

$\exists \vec{c} \neq 0_v$, $W(x)\vec{x} = 0_v \implies \det(W) = 0$. So by the contrapositive: $\det(W) \neq 0 \implies$ linear independence.\\

We prove the other direction: A family of linearly independent solutions $\implies \det(W) \neq 0$. Linearly independent solutions means that we will have $k$ initial conditions: 
\[\{y_0, y_0',\dots,y_0^{(k-1)}\}\]

and the $k$ constant $\{c_i\}$ should be able to describe them. 

	 \[\begin{rcases*}
y_0 = \sum_{i=1}^k c_iy_i(0) \\
\vdots\\
y_0^{(k-1)} = \sum_{i=1}^k c_iy_i^{(k-1)}(0)	
\end{rcases*} W(0)\,\vec{c} = \begin{pmatrix}
 y_0 \\ \vdots\\ y_0^{(k-1)}
 \end{pmatrix} \equiv \vec{y}_0	
\]

This solution $\implies \det(W) \neq 0$. 
\end{proof}~

\begin{example}
Do $\{\sin x,\cos x\}$ form a basis? 

\[
\begin{aligned}
  W &= \begin{pmatrix}
 \sin x & \cos x\\
 \cos x & -\sin x	
 \end{pmatrix}\\
 \det(W) &= -1 \neq 0
\end{aligned}
\]
So they do form a basis.
\end{example}

\emph{Can we choose any particular integral? Is it possible to have more than one $y_{PI}$?} Yes! There are infinitely many in fact, and any will work...

%We have $y$ and $y_{CF}$ such that $\mathcal{L}[y] = f(x)$, $\mathcal{L}[y_{CF}] = 0$ and $y_{CF} = \{y_i\}_{i=1}^k$.

Suppose we have two particular solutions: $y_{PI_1}$, $y_{PI_2}$. Then  $\mathcal{L}[y_{PI_1}] = f(x)$ and $ \mathcal{L}[y_{PI_2}] = f(x)$. Hence
\[
  \mathcal{L}\equalto{[y_{PI_1} - y_{PI_2}]}{\sum_{i=1}^ka_iy_i} = 0
\]
\[
\begin{aligned}
  &\implies y_{GS} = \sum_{i=1}^kc_iy_i + y_{PI_1}\\
  &\text{or }y_{GS} = \sum_{i=1}^kc_iy_i + y_{PI_2} = \sum_{i=1}^k(c_i+a_i)y_i
\end{aligned}
\]

One extra bit about solutions of linear ODEs:

\[\mathcal{L}_{\vec{\alpha}}[y] = f_1(x) + f_2(x) = f(x)\]
\[
\begin{aligned}
  \mbox{1st Step: }& \mathcal{L}_{\vec{\alpha}}[y_{CF}] = 0\\
  \mbox{2nd Step: }& \mathcal{L}_{\vec{\alpha}}[y+{y_{PI_1}}] = f_1(x)\\
  \mbox{3rd Step: }& \mathcal{L}_{\vec{\alpha}}[y+{y_{PI_2}}] = f_2(x)
\end{aligned}
\]
\[y_{GS}(x;c_1,\dots,c_k) = y_{CF}(x;c_1,\dots,c_k) + y_{PI_1}(x) + y_{PI_2}(x)\]

Back to calculations: 

Our linear ODEs with constant coefficients are of the form:
\[\mathcal{L}_{\vec{\alpha}}[y] = \sum_{i=1}^k\alpha_i(x)\frac{d^i}{dx^i}[y] = f(x)\]\vspace*{5pt}

\subsektion{1st order Linear ODEs}
The general form for 1st order Linear ODEs with constant coefficients is
\[\mathcal{L}_{(\alpha_1,\alpha_2)}[y] = \alpha_1\frac{dy}{dx} + \alpha_2y = f(x)\]

1st step: Solve Homogeneous problem:
\[
\begin{aligned}
  \mathcal{L}[y]_{CF} &= 0\\
  \alpha_1\frac{dy}{dx} + \alpha_2y &= 0
  \end{aligned}\]
\[\begin{aligned}
  \frac{dy}{dx} &= \frac{-\alpha_2}{\alpha_1}y\\
  y_{CF} = c_1e^{\frac{-\alpha_2}{\alpha_1}x} &= y_{GS}^{(H)}(x;c_1)
\end{aligned}
\]

2nd Step: \lecturemarker{6}{5 Oct}
Ansatz $\equiv$ Educated Guess.

 We find one or any particular solution of the full problem. For instance if we have $\mathcal{L}[y_{PI}] = f(x)$ and $f(x)$ is a polynomial, then $y_{PI}$ will be a polynomial (closed under $\mathcal{L}$)\\

\begin{example}
\[\mathcal{L} = \alpha_1\frac{dy}{dx} + \alpha_2y = x\]	

$\mathcal{L}[f \in \mbox{polynomial}] \in \mbox{polynomial}$

We use the \emph{Method of undetermined coefficients (MUC)}

\[
\begin{aligned}
  y_{PI} &= ax^2 + bx + c\\
  \mathcal{L}[y_{PI}] &= \alpha_1(2ax + b) + \alpha_2(ax^2 + bx + c)\\
  &= a\alpha_2x^2 + (2a\alpha_1 + b\alpha_2)x + (\alpha_1b + \alpha_2c)
\end{aligned}
\]
$\forall x$, we have: 
\[
\begin{aligned}
  a\alpha_2 & = 0 \implies a = 0\\
  2a\alpha_1 + b\alpha_2 &= 1\implies b = \frac{1}{\alpha_2}\\
  \alpha_1b + \alpha_2c &= 0 \implies c = \frac{-\alpha_1}{\alpha_2^2}
\end{aligned}
\]
We match orders to find $(a,b,c)$. So 

\[y_{PI} = \frac{1}{\alpha_2}\left(x-\frac{\alpha_1}{\alpha_2}\right)\]

The general solution of the full problem is then: 

\[y_{GS}(x;c_1) = y_{CF} + y_{PI} = c_1e^{-\frac{\alpha_2}{\alpha_1}x} + \frac{1}{\alpha_2}\left(x-\frac{\alpha_1}{\alpha_2}\right)\]
\end{example}~\vspace*{5pt}

\begin{example}
\[\mathcal{L}_{(\alpha_1,\alpha_2)}[y] = e^{bx}\]

Ansatz: $y_{PI} = Ae^{bx}$	

\[
\begin{aligned}
  \mathcal{L}[y_{PI}] &= \alpha_1(Abe^{bx}) \alpha_2(Ae^{bx})\\
  &= (\alpha_1Ab + \alpha_2A)e^{bx} = e^{bx}
\end{aligned}
\]
$\forall x$, $A = \frac{1}{\alpha_1b + \alpha_2}$

\[
  y_{GS}(x;c_1) = \underbrace{c_1e^{\frac{-\alpha_2}{\alpha_1}x}}_{y_{CF}} + \underbrace{\frac{1}{\alpha_1b+\alpha_2}e^{bx}}_{y_{PI}}
\]
where $b \neq \frac{-\alpha_2}{\alpha_1}$
\end{example}~\vspace*{5pt}

\begin{example}
\[\mathcal{L}_{(\alpha_1,\alpha_2)}[y] = e^{\frac{-\alpha_2}{\alpha_1}x}\]	
Naive ansatz: $y_{PI}^{(\mathrm{bad})} = Ae^{-\frac{\alpha_2}{\alpha_1}x}$

This won't work because $\mathcal{L}[y_{PI}^{(\mathrm{bad})}] = 0 \neq f(x)$

Not-so-naive ansatz: 
\[y_{PI} = A(x)e^{\frac{-\alpha_2}{\alpha_1}x}\]

We then use \emph{variation of parameters} (Lagrange) for $A(x)$: 

\[
\begin{aligned}
  \mathcal{L}[y_{PI}] &= \alpha_1\left[\frac{dA}{dx}e^{\frac{-\alpha_2}{\alpha_1}x} - \frac{\alpha_2}{\alpha_1}Ae^{\frac{-\alpha_2}{\alpha_1}x}\right] + \alpha_2\left[Ae^{-\frac{\alpha_2}{\alpha_1}x}\right]\\
  &= e^{-\frac{\alpha_2}{\alpha_1}x}
\end{aligned}
\]

Then $\forall x$ 
\[\left[\alpha_1\frac{dA}{dx}\right]e^{-\frac{\alpha_2}{\alpha_1}x} = e^{-\frac{\alpha_2}{\alpha_1}x}\]

So $\alpha_1\frac{dA}{dx} = 1$, $A(x) = \frac{1}{\alpha_1}x + c_2$

\[
\begin{aligned}
  y_{GS} = y_{CF} + y_{PI} &= c_1e^{-\frac{\alpha_2}{\alpha_1}x} + \left(\frac{1}{\alpha_1}x + c_2\right)e^{-\frac{\alpha_2}{\alpha_1}x}\\
  &= \underbrace{c'}_{c_1 + c_2}e^{-\frac{\alpha_2}{\alpha_1}x} + \frac{1}{\alpha_1}xe^{-\frac{\alpha_2}{\alpha_1}x}
\end{aligned}
\]
\end{example}\vspace*{5pt}

\subsektion{2nd order linear ODEs}
General form of 2nd order linear ODEs with constant coefficients:

\[\mathcal{L}_{(\alpha_2,\alpha_1,\alpha_0)}[y] = \alpha_2\frac{d^2y}{dx^2} + \alpha_1\frac{dy}{dx} + \alpha_0y = f(x)\]

The general solution is then:
\[
\begin{aligned}
  y_{GS}(x;c_1,c_2) = &~\equalto{y_{CF}}{}(x;c_1,c_2) + y_{PI}(x)\\
  &y_{GS}^{(H)} = c_1y_1(x) + c_2y_2(x)
\end{aligned}
\]

The basis of the vector space associated with $\mathcal{L}_{(\alpha_2,\alpha_1,\alpha_0)}$ is then $\{y_1(x),y_2(x)\} \equiv \mathcal{B}$. 

1st Step: Homogenous Problem. 
\[\mathcal{L}[y] = \alpha_2\frac{d^2y}{dx^2} + \alpha_1\frac{dy}{dx} + \alpha_0y = 0\]

Ansatz: $y_{GS}^{(H)} = Ae^{\lambda x}$

\[
\begin{aligned}
  \mathcal{L}[y_{GS}^{(H)}] &= \alpha_2\lambda^2e^{\lambda x}+ \alpha_1\lambda e^{\lambda x} + \alpha_0e^{\lambda x}\\
  &= e^{\lambda x}[\alpha^2\lambda^2 + \alpha_1\lambda + \alpha_0] = 0
\end{aligned}
\]

So $\forall x$: 
\[\alpha_2\lambda^2 +\alpha_1\lambda + \alpha_0 = 0\]~

\begin{definition}
For a 2nd order linear ODE, the \emph{characteristic equation} is
	\[\alpha_2\lambda^2 +\alpha_1\lambda + \alpha_0 = 0\]
\end{definition}


\[\implies \lambda_{1,2} = \frac{-\alpha_1 \pm \sqrt{\alpha_1^2 -4\alpha_0\alpha_2}}{2\alpha_2}\]

\[
  \begin{rcases*}
  y_1 = e^{\lambda_1x}\\
  y_2 = e^{\lambda_2x}	
  \end{rcases*}
  \mbox{ Are they a basis?}
\]

\emph{Are $y_1$ and $y_2$ linearly independent?} Check the Wronskian: 

\[
\begin{aligned}
  W &= \begin{pmatrix}
 e^{\lambda_1x} & e^{\lambda_2x}\\
 \lambda_1e^{\lambda_1x} & \lambda_2e^{\lambda_2x}	
 \end{pmatrix}\\[0.2cm]
 det(W) &+ e^{(\lambda_1+\lambda_2)x}(\lambda_2-\lambda_1) \neq 0
\end{aligned}
\]

If $\lambda_2 \neq \lambda_1 \implies \mathcal{B} = \{e^{\lambda_1x},e^{\lambda_2x}\}$. Then 

\[y_{CF} = y^{(H)}_{GS}(x;c_1,c_2) = c_1e^{\lambda_1x}+c_2e^{\lambda_2x}\]\vspace*{5pt}

\textsc{Case (i)} \lecturemarker{7}{5 Oct}
\[\alpha_1^2 - 4\alpha_0\alpha_2 > 0\implies \lambda_{1,2} \iR\]

As $x \to \infty$, asymptotically the solution is dominating by a positive decaying exponential. \\


\textsc{Case (ii)}
\[\alpha_1-4\alpha_0\alpha_2 < 0 \quad \left|\frac{\alpha_1^2-4\alpha_0\alpha_2}{4\alpha_2^2}\right| = \omega^2\]
Then our solution is
\[\lambda_{1,2} = -\frac{\alpha_1}{2\alpha_2} \pm i\omega\]

\[
\begin{aligned}
  y_{CF} &= c_1e^{\lambda_1x} + c_2e^{\lambda_2x}\\
  &= c_1 e^{-\frac{\alpha_1}{2\alpha_2}x + i\omega x} + c_2 e^{-\frac{\alpha_1}{2\alpha_2}x - i\omega x}\\
  &= e^{-\frac{\alpha_1}{2\alpha_2}x}[c_1e^{i\omega x} + c_2e^{-i\omega x}]\\
  &= e^{-\frac{\alpha_1}{2\alpha_2}x}\left[c_1(\cos(\omega x) + i\sin(\omega x) + c_2(\cos(\omega x) - i\sin(\omega x)\right]\\
  &= e^{-\frac{\alpha_1}{2\alpha_2}x}[\underbrace{(c_1+c_2)}_{c_1'}\cos\omega x + \underbrace{(c_1-c_2)i}_{c_2'}\sin\omega x]
\end{aligned}
\]

We can write our coefficients as $c_1' = A\cos\phi,~c_2' = A\sin\phi$, so our solution is 
\[y_{CF} = e^{-\frac{\alpha_1}{2\alpha_2}x}A\cdot[\cos(\omega x -\phi)]\]

\begin{itemize}
  \item If $\alpha_1 = 0$, we have solved it already. Harmonic Motion:
  \item If $\frac{\alpha_1}{\alpha_2} >0$. Damped Harmonic Motion:
  \item If $\frac{\alpha_1}{\alpha_2} <0$. Exponential Growth: 
\end{itemize}~

\textsc{Case (iii)}
\[\alpha_1^2 -4\alpha_0\alpha_2 = 0 \implies \lambda_1 = \lambda_2 = -\frac{\alpha_1}{2\alpha_2}\]

We need to find another function to span the space of functions $\{e^{\lambda_1x},?\} = \mathcal{B}$. 

We tried $y_{CF_1} = Ae^{\lambda x} \implies e^{\lambda_1 x} = y_1$. 

Use variation of parameters: 
\[y_{CF_2} = A(x)e^{\lambda_1x}\]
\[
\begin{aligned}
  \mathcal{L}[y_{CF_2}] &= \alpha_0[Ae^{\lambda_1x}] + \alpha_1\left[\frac{dA}{dx}e^{\lambda_1x} + c\frac{dy_1}{dx}\right] + \alpha_2\left[\frac{d^2A}{dx^2}y_1 + 2\frac{dA}{dx}\frac{dy_1}{dx} + A\frac{d^2y}{dx^2}\right]\\
  &= A\left[\alpha_2\frac{d^2y_1}{dx^2} + \alpha_1\frac{dy_1}{dx} + \alpha_0y_1\right] + \frac{dA}{dx}\left[2\alpha_2\frac{dy_1}{dx} + \alpha_1y_1\right] + \frac{d^2A}{dx^2}[\alpha_2y_1]\\
  &= 0
\end{aligned}
\]

Note that: 
\[
\begin{aligned}
  2\alpha_2\frac{dy_1}{dx} +\alpha_1y_1 &= 2\alpha_2\lambda_1e^{\lambda_1x} + \alpha_1e^{\lambda_1x}\\
  \begin{rcases*}
  y_1 = e^{\lambda_1x}\\
  \lambda_1 = -\frac{\alpha_1}{2\alpha_2}	
  \end{rcases*}
  &= e^{\lambda_1x}[2\alpha_2\lambda_1 + \alpha_1]\\
  &= e^{\lambda_1x}[-\alpha_1+\alpha_1]\\
  &= 0
\end{aligned}
\]

Solve for $\dfrac{d^2A}{dx^2} = 0$

\[
\begin{aligned}
  A(x) &= B_2x + B_3\\
  y_{CF_2} &= (B_2x + B_3)e^{\lambda_1x}\\
  y_{CF} &= c_1e^{\lambda_1x} + (B_3 + B_2x)e^{\lambda_1x}\\
  &= De^{\lambda_1x} + B_2xe^{\lambda_1x}\\
  \mathcal{B} &= \{e^{\lambda_1x},xe^{\lambda_1x}\}
\end{aligned}
\]
This function completes the basis, but we need to check linear independence. 

\[
\begin{aligned}
  det(W) &= \begin{vmatrix}
  e^{\lambda_1x} & xe^{\lambda_1x}\\
  \lambda_1e^{\lambda_1x} & e^{\lambda_1x} + x\lambda_1e^{\lambda_1x}
  \end{vmatrix}\\
  &= e^{2\lambda_1x}(1+\lambda_1x - x\lambda_1) \neq 0
\end{aligned}
\]
So we have a basis. 

\textsc{2nd Step:} Find $y_{PI}$ for a given $f(x)$.\\

\begin{example}
\[\mathcal{L}[y] = \frac{d^2y}{dx^2} - 3\frac{dy}{dx} + 2y = f(x)\]	
\textsc{1st Step:} $\mathcal{L}[y_{CF}] = 0$. Characteristic equation is 
\[
  \lambda^2 - 3\lambda + 2 = 0
\]

So $\lambda_1 = 2, \lambda_2 = 1$. Then 
\[y_{CF} = c_1e^{2x} + c_2e^{x}\]
with a basis $\mathcal{B} = \{e^{x}, e^{2x}\}$

\textsc{2nd Step:} $f(x) = x+3$
\[
\begin{aligned}
  \mathrm{MUC} &= y_{PI} = bx + c\\
  \mathcal{L}[y_{PI}] &= -3b + 2(bx + c)\\
  &= x + 3
\end{aligned}
\]
So $\forall x$ 
\[
\begin{aligned}
  2b &= 1 \implies b = \frac{1}{2}\\
  -3b + 2c &= 3 \implies c = \frac{9}{4}
\end{aligned}
\]

Then $y_{PI} = \frac{1}{2}(x+\frac{9}{2})$, so our full solution is:

\[y_{GS} = y_{CF} + y_{PI} = c_1e^{2x} + c_2e^x + \frac{1}{2}\left(x+\frac{9}{2}\right)\]

\end{example}\vspace*{5pt}

\begin{example}\lecturemarker{8}{5 Oct}

$\mathcal{L}[y] = e^{8x}$

\[
\begin{aligned}
  y_{PI} &= Ae^{8x}\\
  \mathcal{L}[y_{PI}] = Ae^{8x}[64 -24 +2] = e^{8x}\\
  \implies A = \frac{1}{42}
\end{aligned}
\]
\end{example}~

\begin{example}
$\mathcal{L}[y] = e^{2x}$, $y_{CF} = c_1e^{2x} +c_2e^{x}$

\[
\begin{aligned}
  y_{PI} &= A(x)e^{2x}\\
  \mathcal{L}[y_{PI}] &= [A'' + 4A' + 4A]e^{2x} -3[A'+2A]e^{2x} + 2[A]e^{2x}\\
  &= e^{2x}
\end{aligned}
\]

So 
\[
\begin{aligned}
  A'' + 4A' + 4A - 3A' - 6A + 2A &= A'' + A'\\
  &= 1
\end{aligned}
\]

\[u = \dfrac{dA}{dx}= A' \quad \frac{du}{dx} + u = 1\]


\[
\begin{aligned}
  \int \frac{du}{1-u} &= \int \,dx\\
  \log(u-1) &= -x + B\\
  \frac{dA}{dx} = u &= B_1\cdot e^{-x} + 1\\
  \implies A &= x -B_1\cdot e^{-x} + B_2
\end{aligned}
\]


\[
\begin{aligned}
  y_{PI} &= (x-B_1e^{-x} + B_2)e^{2x}\\
  &= xe^{2x} -B_1e^{x} + B_2e^{2x}\\
  y_{GS} &= xe^{2x} + \underbrace{D_1}_{C_2-B_1}e^{x} + \underbrace{D^2}_{C_1+B_2}e^{2x}
\end{aligned}
\]
\end{example}~

\begin{example}
Repeated Roots: 
\[\mathcal{L}[y] = \left[\frac{d^2}{dx^2} + 4\frac{d}{dx} + 4\right]y\]	
\[\lambda_1 = \lambda_2 = -2\]
\[\mathcal{B} = \left\{e^{-2x},xe^{-2x}\right\}\]
\[y_{CF} = c_1e^{-2x} + c_2xe^{-2x}\]

Now $\mathcal{L} = e^{-2x}$, so 
\[
\begin{aligned}
  y_{PI} &= A(x)e^{-2x}\\
  \mathcal{L}[y_{PI}] &= (A'' - 4A' + 4A)e^{-2x} + (4A' - 8A)e^{-2x} + 4Ae^{-2x}\\ 
  &= e^{-2x}
\end{aligned}
\]
\[\frac{d^2A}{dx^2} = A'' = 1 \implies A(x) = \frac{x^2}{2} + B_1x + B_2\]

\[y_{PI} = \frac{x^2}e^{-2x} + B_1xe^{-2x} + B_2e^{-2x}\]
\end{example}

\subsubsektion{Repeated Roots in general}

\[\mathcal{L}[~] = \left(\frac{d}{dx} -\lambda\right)\left(\frac{d}{dx} -\lambda\right)\]

\[
\begin{aligned}
  y_{PI} &= A(x)e^{\lambda x}\\
  \mathcal{L}[y_{PI}] &= \left(\frac{d}{dx}-\lambda\right)\left(\frac{d}{dx}-\lambda\right)[A(x)e^{\lambda x}]\\
  &= \left(\frac{d}{dx}-\lambda\right)[A'e^{\lambda x}+\lambda Ae^{\lambda x}-\lambda Ae^{\lambda x}]\\
  &= A''e^{\lambda x}+A'\lambda e^{\lambda x}-\lambda A'e^{\lambda x}\\
  &= A''e^{\lambda x}
\end{aligned}
\]\vspace*{5pt}

\begin{example}
\[\mathcal{L}[y] = \frac{d^2y}{dx^2}+4\frac{dy}{dx}+4y = xe^{-2x}\]	

$\mathcal{L}[y_{CF}] = 0$, $\mathcal{B} = \{e^{-2x},xe^{-2x}\}$

\[
\begin{aligned}
  y_{PI} &= A(x)e^{-2x}\\
  \mathcal{L}[y_{PI}] &= A''e^{-2x} = xe^{-2x}\\
  A'' &= \frac{d^2A}{dx^2} = x\\
  \implies A &= \frac{1}{6}x^3 + B_1x + B_2
\end{aligned}
\]

We see that this leads to a part which falls in the span of $\{xe^{-2x} e^{-2x}\}$, so we can get rid of $B_1x + B_2$. Because we are looking for a $y_{PI}$ all the constants in $y_{PI}$ should not matter (so we can turn them to zero). 
\end{example}\vspace*{5pt}

\begin{theorem}
Polynomials \& exponentials are your friends...	\end{theorem}
So you should try to rewrite problems as linear combinations (or products) of polynomials and exponentials.\\

\begin{example}
	\[
\begin{aligned}
  \mathcal{L}[y] &= \cosh x\\
  &= \frac{1}{2}e^x + \frac{1}{2}e^{-x}\\
  \mathcal{L}[y_{PI_1}] &= f_1(x)\\
  \mathcal{L}[y_{PI_2}] &= f_2(x)\\
  y_{PI} &= y_{PI_1} + y_{PI_2}
\end{aligned}
\]
\end{example}~\vspace*{5pt}

\begin{example}
\[
\begin{aligned}
  \mathcal{L}[y] &= e^x\cos x = e^x\left(\frac{e^{ix} + e^{-ix}}{2}\right)\\
  \mathcal{B} &= \{e^{(1+i)x},e^{(1-i)x}\}\\
  \mathcal{L}[y] &= \frac{1}{2}e^{(1+i)x} + \frac{1}{2}e^{(1-i)x}\\
  y_{PI_1} &= A_1xe^{(1+i)x}\\
  y_{PI_2} &= A_2xe^{(1-i)x}
\end{aligned}
\]
\end{example}~\vspace*{5pt}

\begin{example}
\[\frac{d^2y}{dx^2} + 3\frac{dy}{dx} + 2y = \cosh 2x\]	

$\lambda_1 = -2, \lambda_2 = -1$

\[
\begin{aligned}
  y_{CF} &+ c_1e^{-2x} + c_2e^{-x}\\
  \mathcal{B} &= \{e^{-2x},e^{-x}\}\\
  f(x) &= \cosh 2x = \frac{1}{2}e^{2x} + \frac{1}{2}e^{-2x}\\
  y_{PI_1} &= A_1e^{2x}\\
  y_{PI_2} &= A_2xe^{-2x}\\
  y_{PI} &= A\cosh 2x + B\sin h 2x 
\end{aligned}
\]
This will \emph{not} be a good ansatz because it does not contain $xe^{-2x}$

\end{example}

\subsektion{Summary}

General \lecturemarker{9}{5 Oct} case of order $k$: 
\[\mathcal{L}_{(\alpha_0,\dots,\alpha_k)} = \sum_{i=0}^k \alpha_i\frac{d^i}{dx^i}[y] = f(x),~ a_i\iR\]

The general solution will be
\[y_{GS}(x;c_1,\dots,c_k) = y_{CF} + y_{PI}\]

1st Step: Homogeneous Problem
\[
\begin{aligned}
  \mathcal{L}[y_{CF}] &= 0\\
  y_{CF} &= y_{GS}^{(H)}(x;c_1,\dots,c_k)\\
  &= \sum_{i=1}^kc_iy_i\\
  \mathcal{B} &= \{y_i\}_{i=1}^{k}
\end{aligned}
\]

Ansatz: $y_{CF} = e^{\lambda x}$
\[\mathcal{L}[y_{CF}] = e^{\lambda x}\left[\sum_{i=0}^k\alpha_i\lambda^i\right] = 0\]

\[\sum_{i=0}^k \alpha_i \lambda_i = 0\]

The roots of the polynomial to be found: factorisation, numerically...

\textbf{Case 1:} All roots are distinct. 
\[
\begin{aligned}
  \mathcal{B} &= \{e^{\lambda_1x},\dots,e^{\lambda_kx}\}\\
  y_{CF} &= \sum_{i=1}^k c_ie^{\lambda_ix}
\end{aligned}
\]

\textbf{Case 2:} Repeated roots. 

Suppose the root $\lambda_r$ is repeated $d$ times, then: 

\[
\begin{aligned}
  \mathcal{B} &= \{e^{\lambda_1x},\dots,e^{\lambda_rx},xe^{\lambda_rx},\dots,x^{d-1}e^{\lambda_rx},\dots,e^{\lambda_{k-d+1}x}\}\\[.2cm]
  y_{CF} &= c_1e^{\lambda_1x} + \dots + c_re^{\lambda_rx} + c_{r+1}xe^{\lambda_rx} + \dots 
\end{aligned}
\]~

\textsc{2nd Step:} Solving the full system:
\[\mathcal{L}[y_{PI}] = f(x)\]

If $f(x) = e^bx$, $b \not\in \{\lambda_i\}_{i=1}^k$, then
\[y_{PI} = Ae^{bx}\]

If $f(x) = e^{\lambda_1x}$ and $\lambda_1$ is not a repeated root
\[y_{PI} = Axe^{\lambda_1x}\]

If $f(x) = e^{\lambda_rx}$ and $\lambda_r$ is repeated $d$ times, 
\[y_{PI} = Ax^de^{\lambda_rx} \]

3rd Step: Particularize your solution

\[y_{GS} = y_{CF}(x;c_1,\dots,c_k) + y_{PI}\]

Given $k$ initial conditions $\{y_0,y_0^{'},\dots,y_0^{(k-1)}\}$, we fix the $k$ constants $\{c_i\}_{i=1}^k$. 




\subsektion{Euler Equation}

Differential equations where the degree of $x$ matches the order of the derivative. Useful in Economics and Thermodynamics. We use a change of variables the turns the equation into a linear ODE.\\

\begin{example}[Euler Equation]
\[x^2\frac{d^2y}{dx^2} + 3x\frac{dy}{dx} + y = x^3\]

Change of variables: 
\[\begin{aligned}
\begin{rcases*}
x = e^z\\
z = \log x	
\end{rcases*}
\frac{dy}{dx} &= \frac{dy}{dx}\frac{dz}{dx} = \frac{1}{x}\frac{dy}{dz}\\
x\frac{dy}{dx} &= \frac{dy}{dz}
\end{aligned}\]

The same thing happens at all orders: 

\[
\begin{aligned}
  \frac{d^2y}{dx^2} &= \frac{d}{dx}\left(\frac{dy}{dx}\right) = \frac{d}{dz}\left(\frac{dy}{dx}\right)\cdot\frac{dz}{dx}\\
  &= \frac{1}{x}\left[\frac{d^2y}{dz^2}\cdot\frac{1}{x} + \frac{dy}{dz}\left(\frac{-1}{x^2}\right)\frac{dx}{dz}\right]\\
  &= \frac{1}{x^2}\frac{d^2y}{dz^2} - \frac{1}{x^2}\frac{dy}{dz}\\
  \implies x^2\frac{d^2y}{dx^2} &= \frac{d^2y}{dz^2} - \frac{dy}{dz}
\end{aligned}
\]

So our ODE becomes:

\[
\begin{aligned}
  \left(\frac{d^2y}{dz^2} - \frac{dy}{dz}\right) + 3\frac{dy}{dz} + y &= e^{3z}\\
 \implies \frac{d^2y}{dz^2} + 2\frac{dy}{dz} + y &= e^{3z}
\end{aligned}
\]

We end up with a 2nd Order Linear ODE with constant coefficients. 

\[
\begin{aligned}
  y(z) &= c_1e^{-z} + c_2ze^{-z} + \frac{1}{16}e^{3z}\\
\implies  y(x) &= c_1\frac{1}{x} + c_2\frac{\log x}{x} + \frac{1}{16}x^3
\end{aligned}
\]
\end{example}
\pagebreak


\begin{theorem}
Using the substitution, we get
\[\frac{d^ky}{dz^k} = \sum_{i=1}^k\alpha_i\cdot x^k\frac{d^iy}{dx^i}\]	
\end{theorem}
\begin{proof}
Lots of algebra. 
\end{proof}







\sektion{Systems of Linear Differential Equations}

So far we've considered ODEs with one independent variable, $x$, implicit form
\[F\left(x,y,\dfrac{dy}{dx},\dots,\dfrac{d^ky}{dx^k}\right) = 0\]

%Explicit form - $\dfrac{d^ky}{dx^k} = f\left(x,y,\dfrac{dy}{dx},\dots,\dfrac{d^{k-1}y}{dx^{k-1}}\right)$

When we solve this we try to find the function $y(x) = \R \to \R$. 

Now we consider a system of ODEs: 

\[
\text{System}
\left\{
\begin{aligned}
F_1&\left(x;y_1,\dots,y_d, \frac{dy_1}{dx},\dots,\frac{dy_d}{dx},\dots,\frac{d^{k_1}y_d}{dx^{k_1}}\right) = 0\\
&\vdots\\
F_1&\left(x;y_1,\dots,y_d, \frac{dy_1}{dx},\dots,\frac{dy_d}{dx},\dots,\frac{d^{k_1}y_d}{dx^{k_1}}\right) = 0\\
\end{aligned}\right.
\]

The solution we find is the set of functions $\{y_1(x),y_2(x),\dots,y_d(x)\} = \{y_i(x)\}_{i=1}^d$

The explicit form is 

\[
\left\{
\begin{aligned}
\frac{d^{k_1}y_d}{dx^{k_1}} &= f_1\left(x;y_1,\dots,y_d, \frac{dy_1}{dx},\dots,\frac{dy_d}{dx},\dots,\frac{d^{k_1-1}y_d}{dx^{k_1-1}}\right) = 0\\
&\vdots\\
\frac{d^{k_d}y_d}{dx^{k_1}} &= f_d\left(x;y_1,\dots,y_d, \frac{dy_1}{dx},\dots,\frac{dy_d}{dx},\dots,\right) = 0\\
\end{aligned}\right.
\]

Systems of ODEs are rewritten in terms of the first order derivatives by the remaining variables, i.e. 

\[
\begin{aligned}
  \frac{dy_1}{dx} &= u_1\\[0.2cm]
  \frac{du_1}{dx} &= u_2\\
  \vdots\\
  \frac{du_{k_1-1}}{dx} &= f_1(x,\dots)
\end{aligned}
\]

So in general we consider: \lecturemarker{10}{Whenever}
\[
\text{System}
\left\{
\begin{aligned}
F_1&\left(t; x,y, \frac{dx}{dt}, \frac{d^2x}{dt^2}, \frac{dy}{dt}\right) = 0\\
&\vdots\\
F_2&\left(t; x, y, \frac{dx}{dt}, \frac{dy}{dt}, \frac{d^2y}{dt^2}\right) = 0\\
\end{aligned}\right.
\]

We write the explicit form as: 

\[
\left\{
\begin{aligned}
  \frac{dx}{dt} &= u\\
  \frac{dy}{dt} &= w\\
  \frac{d^2x}{dt^2} &= \frac{du}{dt} = f_1(t, x, y, u w)\\
  \frac{d^2y}{dt^2} &= \frac{dw}{dt} = f_2(t,x,y,u,w)
\end{aligned}
\right. \]

Then 

\[\frac{d}{dt}\equalto{\begin{pmatrix}
x \\ y \\ u \\ w	
\end{pmatrix}}{\vec{x}}
= \begin{pmatrix}
 u \\ w\\ 
 f_1(t,x,y,u,w)\\
 f_2(t,x,y,u,w)	
 \end{pmatrix}
\]

So overall 
\[\boxed{\frac{d\vec{x}}{dt} = \vec{f}(\vec{x}) }\]\vspace*{5pt}


\begin{example}[Lotka-Volterra system]
\[
\begin{aligned}
  \frac{dx}{dt} &= ax - bxy = f_1(x,y)\\
  \frac{dy}{dt} &= -cy + dxy = f_2(x,y)\\
  \vec{y} &= \begin{pmatrix} x \\ y \end{pmatrix}\\
  \frac{d}{dt}\vec{y} &= \begin{pmatrix}
 f_1(x,y) \\ f_2(x,y)	
 \end{pmatrix} = \vec{f}(\vec{y})
\end{aligned}
\]	
\end{example}

\begin{example}
\[m\frac{d^2x}{dt^2} + \eta \frac{dx}{dt} + kx = f(t)\]

\[\left\{
\begin{aligned}
  \frac{dx}{dt} &= y = f_1(y)\\
  \frac{dy}{dt} &= \frac{f(t)}{m} - \frac{\eta}{m}y  -\frac{k}{m}x = f_2(x,y,t)
\end{aligned}\right\}
\]

So overall 
\[\frac{d\vec{y}}{dt} = \begin{pmatrix}
 f_1(y) \\ f_2(x,y,t)	
 \end{pmatrix}
\]
	
\end{example}

\subsektion{Systems of Linear ODEs with constant coefficients} 

\[\left.
\begin{aligned}
  \frac{dy_1}{dt} &= \sum_{i=1}^n \alpha_{1i}y_i + g_1(t)\\
  &\vdots\\
  \frac{dy_n}{dt} &= \sum_{i=1}^n \alpha_{n1}y_i + g_n(t)
\end{aligned}\right\}\text{ System of Linear ODEs}
\]

So $\vec{y} = \begin{pmatrix}
 y_1\\ \vdots\\ y_n	
 \end{pmatrix}$ and our system is
 
 \[\begin{aligned}
 \frac{d\vec{y}}{dt} &= \underbrace{\begin{pmatrix}
 \alpha_{11} & \dots & \alpha_{1n}\\
 \vdots & & \vdots\\
 \alpha_{n1} & \dots & \alpha_{nn}	
 \end{pmatrix}}_{A}\vec{y} + 
\underbrace{\begin{pmatrix}
 g_1(t) \\ \vdots \\ g_n(t)	
 \end{pmatrix}}_{\vec{g}(t)}\\
 \frac{d\vec{y}}{dt} &= A\vec{y} + \vec{g}(t)
 \end{aligned}
\]~\vspace*{5pt}

\begin{example}
\[
\begin{aligned}
  \frac{dx}{dt} &= -4x - 3y\\
  \frac{dy}{dt} &= 2x + 3y\\
  \frac{d\vec{y}}{dt} &= \begin{pmatrix}
 -4 & -3 \\ 2 & 3	
 \end{pmatrix}\vec{y}
\end{aligned}
\]
Where $\vec{y} = \begin{pmatrix} x \\ y \end{pmatrix}$
\end{example}


We find the solution for the general case of systems of linear ODEs with constant coefficients:

\[
\begin{aligned}
  \frac{d\vec{y}}{dt} &= A\vec{y} + \vec{g}(t)\\
  \mathcal{L}\vec{y} &= \left[\frac{d}{dt} - A\right] \vec{y} = \vec{g}(t)
\end{aligned}
\]


by splitting the problem into two steps as before: 

\textsc{1st Step:} Homogenous problem
\[\mathcal{L}[\vec{y}_{GS}^{(H)}] = 0\]

\textsc{2nd Step:} Find a Particular Integral 
\[\mathcal{L}[\vec{y}_{PI}]= \vec{g}(t)\]

Then the complete general solution will be the sum: 
\[\vec{y}_{GS} = \vec{y}_{GS}^{(H)} + \vec{y}_{PI}\]

\subsubsektion{$A$ is diagonalisable} 

\textsc{1st Step:} Solving the Homogenous problem

\[\mathcal{L}[\vec{y}_{CF}] = \left[\frac{d}{dt} - A\right]\vec{y}_{CF} = 0\]

So 
\[\frac{d\vec{y}_{CF}}{dt} = A\vec{y}_{CF}\tag{$*$}\]
where $\vec{y}_{CF}(t;c_1,\dots,c_n)$\\


Assuming $A_{n \times n}$ is diagonalisable, then $\exists V_{n \times n} \text{ such that }$
\[V^{-1}AV = \Lambda = \begin{pmatrix}
 \lambda_1 & & 0 \\
 & \ddots & \\
 0 & & \lambda_n	
 \end{pmatrix}\]

The Eigenvalues of $A \equiv \{\lambda_i\}_{i=1}^n$, and the eigenvectors of $A \equiv \{\vec{v}_i\}_{i=1}^n$, which are found by solving $A\vec{v}_i = \lambda_i\vec{v}_i$. Letting \[V = \begin{pmatrix}
 | & | &  & | \\
 \vec{v}_1 & \vec{v}_2 & \dots & \vec{v}_n\\
 | & | &  & | 
 \end{pmatrix}\]
  we have to solve  $AV = V\Lambda$
 \[
\begin{aligned}
  \implies A(\vec{v}_1\dots \vec{v}_n) &= (\vec{v}_1\dots\vec{v}_n)\begin{pmatrix}
 \lambda_1 & & 0 \\
 & \ddots & \\
 0 & & \lambda_n	
 \end{pmatrix}
\end{aligned}
\]

By $(*)$, $\dfrac{d\vec{y}_{CF}}{dt} = A\vec{y}_{CF}\implies V^{-1}\dfrac{d\vec{y}_{CF}}{dt} = V^{-1}A$.
\begin{align*}
\implies  \frac{d(V^{-1}\vec{y}_{CF})}{dt} &= [V^{-1}AV][V^{-1}\vec{y}_{CF}]
\end{align*}
Letting $\vec{Y} = V^{-1}\vec{y}_{CF}$
\begin{align*}
   \frac{d\vec{Y}}{dt} &= \Lambda \vec{Y}\\[.2cm]
  \begin{pmatrix}
  \frac{dY_1}{dt}\\ \vdots \\ \frac{dY_n}{dt}	
  \end{pmatrix} &=
 \begin{pmatrix}
 \lambda_1 & & 0 \\
 & \ddots & \\
 0 & & \lambda_n	
 \end{pmatrix}
 \begin{pmatrix}
 Y_1 \\ \vdots \\ Y_n	
 \end{pmatrix}
\end{align*}
I.e. 
\begin{align*}
  \frac{Y_1}{dt} &= \lambda_1Y_1 \implies Y_1 = e^{\lambda_1t}c_1\\
  &\vdots\\
    \frac{Y_n}{dt} &= \lambda_nY_n \implies Y_n = e^{\lambda_nt}c_n\\
\end{align*}
Thus
\[
  \vec{Y} = \begin{pmatrix}
 c_1e^{\lambda_1t} \\ \vdots \\ c_ne^{\lambda_nt}	
 \end{pmatrix}
\]
Getting back to $\vec{y}_{CF}$:
\begin{align*}
  \vec{y}_{CF} &= V\vec{Y}\\[-.2cm]
  &= \left(\vec{v_1},\dots,\vec{v_N}\right)\begin{pmatrix}
	 c_1e^{\lambda_1t} \\ \vdots \\ c_ne^{\lambda_nt}	
\end{pmatrix}\\[.2cm]
&= c_1e^{\lambda_1t}\vec{v_1} + \dots + c_ne^{\lambda_nt}\vec{v_n}\\
&= \sum_{i=1}^n c_ie^{\lambda_it}\vec{v_i}.
\end{align*}

Recall a system of linear ODEs \lecturemarker{11}{Someday}
\[
  \frac{d\vec{y}}{dt} = A\vec{y} + \vec{g}(t)
\]
where $\vec{y}_{n \times 1} \in \R^n$, $A_{n \times n } \in \R^{n \times n}$. 

Last time we solved the homogenous problem
\[
  \frac{d\vec{y}_{CF}}{dt} = A\vec{y}_{CF}
\]
and found that 
\[
  \vec{y}_{CF}(t) = \sum_{i=1}^m c_ie^{\lambda_it}\vec{v_i}
\]
where $\lambda_i$ are the eigenvalues and $\vec{v_i}$ are the eigenvectors of $A$.\\

\begin{example}
\[
  \begin{rcases*}
  \dfrac{dx}{dt} = -4x - 3y\\[.2cm]
  \dfrac{dy}{dt} = 2x  +3y	
  \end{rcases*}
  \frac{d\vec{y}}{dt} = \begin{pmatrix}
  -4 & -3 \\ 2 & 3	
 \end{pmatrix}\vec{y} 
\]
where $\vec{y} = \begin{pmatrix}
 x \\ y	
 \end{pmatrix}$. We then need to find the eigenvectors and values of $A$ by solving
\begin{align*}
  &\qquad A\vec{v} = \lambda\vec{v}\\
  &\implies |A -\lambda I| = 0\\
  &\implies \begin{vmatrix}
  -4 -\lambda & -3 \\ 2 & 3-\lambda 	
 \end{vmatrix} = 0\\
 &\implies (\lambda+4)(\lambda-3) + 6 = 0\\
 &\implies \lambda^2 + \lambda - 6 = 0
\end{align*}
Giving $\lambda_1 =2$, $\lambda_2 = -3$. 

We find the eigenvector for $\lambda_1 = 2$: $A\vec{v_1} = \lambda\vec{v_1}$, so 
\begin{align*}
  \begin{pmatrix}
  -4 -2 & -3 \\ 2 & 3-2	
  \end{pmatrix}\begin{pmatrix}
 v_{1x} \\ v_{1y}	
\end{pmatrix}&= 0\\
-6v_{1x} -3v_{1y} &= 0\\
2v_{1x} + 1v_{1y} &= 0\\[.3cm]
\implies \vec{v_1} = \begin{pmatrix}
 1 \\ -2	
 \end{pmatrix}&
 \end{align*}
 
 For $\lambda_2 = -3$, we find that $v_{2y} = -\frac{1}{3}v_{2x}$, so $
  \vec{v_2} = \begin{pmatrix}
 3 \\ -1	
 \end{pmatrix}$.

Thus
\[
  \Lambda = \begin{pmatrix}
 2 & 0 \\ 0& -3	
 \end{pmatrix}\quad 
 V = \begin{pmatrix}
 1 & 3 \\ -2 & -1	
 \end{pmatrix}
\]
Where $V^{-1}AV = \Lambda$. 

Then the homogenous solution is 
\[
  \vec{y}_{CF}(t;c_1,c_2) = c_1e^{2t}\begin{pmatrix}
1 \\ -2	
\end{pmatrix}
+ c_2 e^{-3t}\begin{pmatrix}
3 \\ -1	
\end{pmatrix}
\]
So our complete solution is 
\[
  \vec{y} = \begin{pmatrix}
 x \\ y	
 \end{pmatrix}
 \longrightarrow
 \begin{cases}
 x(t) = c_1e^{2t} + c_23e^{-3t}\\
 y(t) = c_1(-2)e^{2t} + c_2e^{-3t}(-1)	
 \end{cases}
\]
\end{example}

\subsubsektion{Changing an $N$th order linear ODE to a system}

\emph{Remark.} In the previous example, the system was equivalent to a 2nd order linear ODE:
\[
  y = -\frac{1}{3}\left(\frac{dx}{dt} + 4x\right)
\]
\begin{align*}
  \frac{dy}{dt} &= -\frac{1}{3}\left(\frac{d^2x}{dt^2} + 4\frac{dx}{dt}\right)\\[.2cm]
  &= 2x + 3 (-\textstyle{\frac{1}{3}})\left(\dfrac{dx}{dt} + 4x\right)
\end{align*}
Rewriting this:
\[
  \frac{d^2x}{dt^2} + 4\frac{dx}{dt} = -6x + 3\frac{dx}{dt} + 12x
\]

\[
  \frac{d^2x}{dy^2} + \frac{dx}{dt} - 6x = 0
\]

When we solve this using the ansatz we get
\[
  \lambda^2 + \lambda - 6 = 0
\]
which when you solve for $\lambda$, you end up with the previous solution. These two things are equivalent. 

We can also do the reverse; turn a 2nd order ODE into a system of ODEs and solve. Consider
\[
  \frac{d^2}{dt^2} + \eta \frac{dx}{dt} + kx = 0
\]
From ansatz of $e^{\lambda x}$ we would get
\[
  \lambda^2 + \eta \lambda + k = 0
\]

If we instead turn this into a system:
\[
  \begin{rcases*}
  \frac{dx}{dt} = y\\
  	\frac{dy}{dt} = -\eta y - kx
  \end{rcases*}
  \frac{d}{dt}\vec{y} = \begin{pmatrix}
 0 & 1 \\ -k & - \eta 	
 \end{pmatrix}\vec{y}
\]
Solving the system we get 
\begin{align*}
  \begin{vmatrix}
  -\lambda & 1\\ -k & -\eta - \lambda 	
  \end{vmatrix} &= 0\\
  \lambda(\lambda + \eta) + k &= 0\\
  \lambda^2 + \eta\lambda + k &= 0
\end{align*}

The advantage to this approach is that doing eliminations in an $N$th order linear ODE is difficult to do by hand, but if we transform it into a system and end up with an $N \times N$ matrix $A$, then we can use MATLAB to solve the system easily.\\

\begin{example}
\[
  \frac{d\vec{y}}{dt} = \begin{pmatrix}
 3 & 1\\ 2 & 2	
 \end{pmatrix}\vec{y} + \underbrace{\begin{pmatrix}
  -5 \\ -2
\end{pmatrix}}_{\vec{g}(t)}
\]

\textsc{1st Step:} Our operator is
\[
  \mathcal{L} = \left[\frac{d}{dt} -A\right]
\]
We solve the homogenous problem
\begin{align*}
  \mathcal{L}[\vec{y}_{CF}] &= 0\\
  \begin{vmatrix}
  3 - \lambda & 1\\ 2 & 2 -\lambda 	
  \end{vmatrix} &= 0\\
  \implies \lambda^2 - 5\lambda + 4 &= 0
\end{align*}
This has two soltions: $\lambda_1 = 4,\, \lambda_2 = 1$. 

For the eigenvectors of $\lambda_1 = 4$, we solve \[(3-4)v_{1x} + v_{1y} = 0 \implies \vec{v_1} = \begin{pmatrix}
 1 \\ 1	
 \end{pmatrix}\]
 
 Then for $\lambda_2 = 1$, we solve \[(3-1)v_{1x} + v_{1y} = 0 \implies \vec{v_2} = \begin{pmatrix}
  1 \\ -2
\end{pmatrix}\]

We have now solved the homogenous problem:
\[
  \vec{y}_{CF} = c_e^{4t}\begin{pmatrix}
  1 \\ 1
\end{pmatrix} + c_2e^t
\begin{pmatrix}
  1 \\ -2
\end{pmatrix}
\]

\textsc{2nd Step:}
\[
  \mathcal[\vec{y}_{PI}] = \begin{pmatrix}
  -5 \\ -2
\end{pmatrix}
\]
As we have a constant function, our ansatz should be some constants as well: 
\[
  \vec{y}_{PI} = \begin{pmatrix}
  a \\ b
\end{pmatrix}
\longrightarrow \mathrm{MUC}
\]
\begin{align*}
  \mathcal{L}[\vec{y}_{PI}] &= \begin{pmatrix}
  0 \\ 0
\end{pmatrix} - 
\begin{pmatrix}
  3 & 1 \\ 2 & 2
\end{pmatrix}\begin{pmatrix}
  a \\ b
\end{pmatrix}= 
\begin{pmatrix}
  -5 \\ -2
\end{pmatrix}
\end{align*}
We solve 
\[
  A\begin{pmatrix}
  a \\ b
\end{pmatrix} = 
\begin{pmatrix}
  5 \\ 2
\end{pmatrix}
\]
Thus 
\begin{align*}
  \begin{rcases*}
  3a + b = 5\\
  2a + 2b = 2	
  \end{rcases*}\iff 
\begin{pmatrix}
  a \\ b 
\end{pmatrix} = A^{-1}
\begin{pmatrix}
  5 \\ 2 
\end{pmatrix}=
\begin{pmatrix}
  2 \\ -1
\end{pmatrix}
\end{align*}

Hence our complete solution is 
\[
  \vec{y}_{GS} =  \vec{y}_{CF} + 
  \begin{pmatrix}
  2\\ -1
\end{pmatrix}
\]
\end{example}


\subsubsektion{When $A$ is not diagonalisable}


Sometimes we call a non-diagonalisable matrix $A$, a defective matrix. In this case $\not\exists V: V^{-1}AV = \Lambda$. However all matrices can be put into a form which is as close to diagonalisable as possible. 

\begin{theorem}[Existence of Jordan Form]
For all invertible matrices, $W$, there exists a $J$ such that 
\[
  W^{-1}AW = 
  \begin{pmatrix}
  \lambda & 1 & \dots & 0 \\
  0 & \lambda & 1 \dots  & 0 \\
   && \ddots  &\\
  0&&&\lambda 
\end{pmatrix} = J
\]	
\end{theorem}

\emph{Proof.} Omitted.\\

This matrix $J$ has repeated eigenvalues:
\[
  \frac{d\vec{y}}{dt} = A\vec{y}
\]
Then
\[
  \frac{d(W^{-1}\vec{y})}{dy} = \underbrace{W^{-1}AW}_{J}(W^{-1}\vec{y})
\]
Thus
\[
  \frac{d\vec{Y}}{dt} = J\vec{Y}
\]

So now our problem is
\[
  \begin{pmatrix}
  \frac{dY_1}{dt}\\ \vdots\\ \frac{dY_n}{dt}
\end{pmatrix}
= 
 \begin{pmatrix}
  \lambda & 1 & \dots & 0 \\
  0 & \lambda & 1 \dots  & 0 \\
   && \ddots  &\\
  0&&&\lambda 
\end{pmatrix}
\begin{pmatrix}
  Y_1 \\ \vdots \\ Y_n
\end{pmatrix}
\]
This can be solved recursively from the bottom upwards; the lowest row is trivial:
\[
  \frac{dY_n}{dt} = \lambda y_n
  \implies Y_n = c_ne^{\lambda t}
\]

Then we go to the next row above: 
\[
  \frac{dY_{n-1}}{dt} = \lambda Y_{n-1} + Y_n
\]
We already solved for $Y_n$, so in fact we solve
\[
  \frac{dY_{n-1}}{dt} - \lambda Y_{n-1} = c_ne^{\lambda t}
\]
we could consider the complimentary function approach to solving this:
\begin{align*}
  CF: Y_{n-1}^{(CF)} &= c_{n-1}e^{\lambda t}\\
  PI: Y_{n-1}^{(PI)} &= c^*te^{\lambda t} \quad \text{(ansatz)}
\end{align*}
We plug in our ansatz into our operator to find $c^*$: 
\[
  c*[e^{\lambda Y} + \lambda te^{\lambda t} - \lambda t e^{\lambda t}] = c_ne^{\lambda t}
\]
So $c* = c_n$. Thus 
\[
  Y_{n-1} = c_{n-1}e^{\lambda t} + c_nte^{\lambda t}
\]

We continue with the next row:
\[
  \frac{dY_{n-2}}{dt} = \lambda Y_{n-2} + Y_{n-1}
\]
Then 
\[
  Y_{n-2} = c_{n-2}e^{\lambda t} + c_{n-1}te^{\lambda t} + \frac{c_n}{2}t^2 e^{\lambda t}
\]
So recursively you go upwards and continue solving the system. So even if $A$ is defective, you can solve this using linear algebra.\\


\begin{example} Suppose  \lecturemarker{12}{10 Feb}
$A =  \begin{pmatrix}
  1 & -1 \\ 1& 3	
  \end{pmatrix}$, we find the eigenvalues:
  \begin{align*}
  \begin{vmatrix}
  1 -\lambda & -1 \\ 1 & 3-\lambda 	
  \end{vmatrix}&= 0\\
  3 - 3\lambda - \lambda + \lambda^2 + 1 &=0\\
  \l^2 - 4\l + 4 &= 0
\end{align*}
So $\l_1 = \l_2 = 2$. We solve $(1-2)v_{1x} - v_{1y} = 0 \implies v_{1y} = -v_{1x}$, so there is only one eigenvector
\[
  \vec{v_1} = \begin{pmatrix}
  1 \\ -1
\end{pmatrix}
\]

Now 
\begin{align*}
  W^{-1}AW &=
  \begin{pmatrix}
  2 & 1\\ 0 & 2
\end{pmatrix}\\[.3cm]
\implies 
\begin{pmatrix}
  1 & -1 \\ 1 & 3
\end{pmatrix}
\begin{pmatrix}
  1 & \alpha \\ -1 & \beta 
\end{pmatrix} &= \begin{pmatrix}
  1 & \alpha \\ -1 & \beta 
\end{pmatrix}
\begin{pmatrix}
  2 & 1\\ 0 & 2
\end{pmatrix}\\
\implies 
\begin{pmatrix}
  2 & \alpha - \beta \\
   -2 & \alpha + 3\beta
\end{pmatrix}
&= 
\begin{pmatrix}
  2 & 1 + 2\alpha\\ -2 & -1 + 2\beta
\end{pmatrix}
\end{align*}

Thus
\[
  \begin{cases}
  \alpha - \beta = 1 + 2\alpha\\
  \alpha + 3\beta = -1 + 2\beta	
  \end{cases}
 \alpha + \beta = -1
\]
So 
\[
  \vec{W_2} = \begin{pmatrix}
  1\\ -2
\end{pmatrix}\implies 
 W = 
 \begin{pmatrix}
  1 & 1\\ -1 & -2
\end{pmatrix}
\]

We check this: 
\begin{align*}
  W^{-1}AW &= 
  \begin{pmatrix}
  2 & 1\\ -1 & -1
\end{pmatrix}
\begin{pmatrix}
  1 & -1 \\ 1 & 3
\end{pmatrix}
\begin{pmatrix}
  1 & 1\\ -1 & 2
\end{pmatrix}\\
&= 
\begin{pmatrix}
  2 & 1 \\ 0 & 2
\end{pmatrix} = J
\end{align*}

We then recursively solve
\[
  \frac{d\vec{y}}{dt}
  = 
  \begin{pmatrix}
  2 & 1\\ 0 & 2
\end{pmatrix}\vec{y}
\]
The bottom row: 
\[
  \frac{dY_2}{dt} = 2Y_2 \implies Y_2 = c_2e^{2t}
\]
Then
\begin{align*}
  \frac{dY_1}{dt} &= 2Y_1 + Y_2
\end{align*}
We have 
\begin{align*}
 Y_1^{(CF)} &= c_1e^{2t} \\
 Y_1^{(PI)} &+ c_3te^{2t}\longrightarrow  \text{ our ansatz}
\end{align*}
Plugging in our ansatz, we obtain
\[
  Y_1 = c_1e^{2t} + c_2te^{2t}
\]
So 
\[
  \vec{Y} = 
  \begin{pmatrix}
  c_1e^{2t} + c_2te^{2t}\\
  c_2e^{2t}
\end{pmatrix}
\]	
Then 
\begin{align*}
  \vec{y} &= W\vec{Y}\\
  &= (\equalto{\vec{w_1}}{\vec{v_1}},\vec{w_2})
  \begin{pmatrix}
   c_1e^{2t} + c_2te^{2t}\\
  c_2e^{2t}
\end{pmatrix}\\
&= (c_1e^{2t} + c_2te^{2t})
\begin{pmatrix}
  1 \\ -1
\end{pmatrix}
+ c_2e^{2t}
\begin{pmatrix}
  1\\ -2
\end{pmatrix}
\end{align*}
\end{example}

We could have solved this system by changing it into a 2nd order ODE: 

Write the system $\frac{d\vec{y}}{dt} = A\vec{y}$ as two equations and substitute:
\begin{align*}
  \frac{dx}{dt} &= x-y \\
  \frac{dy}{dt} &= x + 3y
\end{align*}
So 
\[
  \frac{d^2x}{dt^2} - 4\frac{dx}{dt} + 4x = 0
\]

Using the ansatz of $e^{\lambda t}$, we get the characteristic equation of $\l^2 -4\l + 4 = 0$, which solves to the eigenvalues $\l_{1,2} = 2$. This corresponds to a basis of $\mathcal{B} = \{e^{2t},te^{2t}\}$ - the same solution that we found using the Jordan form method.


\sektion{Qualitative Analysis of Systems}


\subsektion{Asymptotic Behaviours}

We now work towards a qualitative description of the solution of systems of linear ODEs. We look at the analytical solutions for all possible behaviours of $2 \times 2$ systems. 

So for $2D$ systems: $n = 2$, and so we consider with $A_{2\times 2}$:
\[
  A = 
  \begin{pmatrix}
  a & b \\ c& d
\end{pmatrix}
\]
The solutions are found by solving the eigenvalue equation
\begin{align*}
  \begin{vmatrix}
  a - \l & b \\ c & d-\l	
  \end{vmatrix}&= 0\\
  \implies (a-\l)(a-\l) -bc) &= 0\\
  \implies \l^2 - \underbrace{(a+d)}_{\tau = \mathrm{tr}(A)}\l + \underbrace{(ad-bc)}_{\Delta = \det(A)} &= 0\\[.2cm]
  \implies \l^2 -\tau \l + \Delta &= 0
\end{align*}
The eigenvalues are then given by 
\[
  \lambda_{1,2} = \frac{\tau \pm \sqrt{\tau^2 - 4\Delta}}{2}
\]
So $(\tau,\Delta)$ are responsible for the behaviour of the system. What we're working towards is being able to characterise the behaviour of a system by looking at where $(\tau,\Delta)$ lie on the $\tau -\Delta$ plane. 

\begin{center}
\begin{tikzpicture}[domain = -3:3]
\draw[->] (-3,0) -- (3,0) node [right] {$\tau$};
\draw[->] (0,-2) -- (0,2) node[left]{$\Delta$};
\draw[smooth] plot (\x,{0.2*abs((\x)^2)}) node [right] {$\tau^2 - 4\Delta$};
\end{tikzpicture}	
\end{center}


\textsc{Case (1):} $\Delta < 0$. 

Then $\tau^2 - 4\Delta > 0 \implies \l_{1,2}\iR$. Also $\tau^2 - 4\Delta > \tau^2$, so it follows that $\lambda_1\iR+$ and $\l_2\iR^-$. Then 
\[
  \vec{y} = c_1e^{\l_1t}\vec{v_1} + c_2e^{\l_2t}\vec{v_2}
\]
Asymptotically:

\begin{enumerate}
  \item  As $t \to \infty$, $y \to c_1e^{\l_1t}\vec{v_1}$. 
  \item There is no oscillations because there are only real exponentials.
\end{enumerate}

This takes care of the lower half of the plane.\\

%draw lower half of plane


\textsc{Case (2):} $\Delta  >0$. We then need to split this into further cases. 

\begin{enumerate}
  \item[\textsc{(2.1)}] $\tau^2 - 4\Delta > 0 \implies \l_{1,2} \iR$. Also $\tau^2 - 4\Delta < \tau^2$.
  \begin{enumerate}
  \item[$\bullet$] $\tau > 0 \implies \l_1,\l_2 > 0$.
  \item[$\bullet$] $\tau < 0 \implies \l_1,\l_2 < 0$. 
\end{enumerate}
We then have 
\[
    \vec{y} = c_1e^{\l_1t}\vec{v_1} + c_2e^{\l_2t}\vec{v_2}
\]

For (2.1.1), $\l_1 > \l_2 > 0$, so as $ t \to \infty$, $\vec{y} \to c_1e^{\l_1t}\vec{v_1} \to \begin{pmatrix}
  \infty \\ \infty 
\end{pmatrix}
$. 

For (2.1.2), $\l_2< \l_1 < 0$. So as $t\to \infty$, the $y \to c_1e^{\l_1t}\vec{v_1} \to \begin{pmatrix}
  0 \\ 0
\end{pmatrix}
$.\\


\item[\textsc{(2.2)}] $\tau^2 - 4\Delta < 0 \implies \Delta > \frac{\tau^2}{4}$. Then 
\[
  \left|\frac{\tau^2-4\Delta}{4}\right| = \omega^2
\]
So $\l_{1,2} = \frac{\tau}{2} \pm i\omega$. Then 
\begin{enumerate}
  \item[$\bullet$] $\tau = 0 \implies \l_{1,2} = \pm i\omega$. Then 
  \[
  \vec{y} = c_1e^{i\omega t}\vec{v_1} + c)2e^{-\omega t}\vec{v_2}
\]
This is sinusoidal functions $\implies$ periodic behaviour.

\item[$\bullet$] $\tau > 0$, then 
\begin{align*}
  \vec{y} &= c_1e^{(\frac{\tau}{2} + i\omega)t}\vec{v_1} + c_2e^{(\frac{\tau}{2} -i\omega)t}\vec{v_2}\\
  &= e^{\frac{\tau}{2}t}\left[c_1e^{i\omega t}\vec{v_1} + c_2e^{i\omega t}\vec{v_2}\right]
\end{align*}

\begin{center}
\begin{tikzpicture}[domain=0:6]
\draw[->] (-2,0) -- (6,0) node [right] {$t$};
\draw[->] (0,-2) -- (0,2) node [left] {$x$};	
\draw[smooth] plot (\x,{0.1*exp(\x/2)*sin(\x r*5)});
\draw [Green]  plot [smooth, tension=1] coordinates { (-1,.2) (3,.7) (5.5,2)} node[right]{$e^{\frac{\tau}{2}t}$};
\draw [Green]  plot [smooth, tension=1] coordinates { (-1,-.2) (3,-.7) (5.5,-2)};
\end{tikzpicture}
\end{center} 

\item[$\bullet$] $\tau < 0$. 
\[
  \vec{y} = e^{\frac{\tau}{2}t}\left[c_1e^{i\omega t}\vec{v_1} + c_2e^{i\omega t}\vec{v_2}\right]
\]
\begin{center}
\begin{tikzpicture}[domain=0:5.4]
\draw[->] (-2,0) -- (6,0) node [right] {$t$};
\draw[->] (0,-2) -- (0,2) node [left] {$x$};	
\begin{scope}[xscale=-1, shift = {(-5.4,0)}]
\draw[smooth] plot (\x,{0.1*exp(\x/2)*sin(\x r*5)});
\draw [Green]  plot [smooth, tension=1] coordinates { (-1,.2) (3,.7) (5.3,2)} node[right]{$e^{-\frac{\tau}{2}t}$};
\draw [Green]  plot [smooth, tension=1] coordinates { (-1,-.2) (3,-.7) (5.3,-2)};	
\end{scope}
\end{tikzpicture}	
\end{center}
\end{enumerate}
\end{enumerate}


\textsc{Case (3):}  $\Delta = 0$. \lecturemarker{13}{12 Feb}

Then $\l_1 = \tau$, $\l_2 = 0$, so 
\[
  \vec{y} = c_1e^{\l_1t}\vec{v_1} + c_2\vec{v_2}
\]
\begin{enumerate}
\item[$\bullet$] $\tau > 0$. As $t \to \infty$, asymptotically the solution grows exponentially along $(c_1\vec{v_1})e^{\l_1t}$. 
\item[$\bullet$] $\tau < 0$.  As $\to \to \infty$, $\vec{Y} \to c_2\vec{v_2}$ along $(c_2\vec{v_2})e^{\l_2t}$.
\end{enumerate}\vspace*{10pt}


\textsc{Case (4):} $\tau^2 - 4\Delta = 0$. 

Then $\l_1 = \l_2 = \frac{\tau}{2} \implies $ repeated eigenvalues. 

\begin{enumerate}
\item[\textsc{(4.1)}] $A$ is diagonalisable. 
\begin{align*}
  A = \l\begin{pmatrix}
  1 & 0 \\ 0 & 1
\end{pmatrix}
\end{align*}
Where $\vec{v_1} = 
\begin{pmatrix}
  1 \\ 0
\end{pmatrix}$, $\vec{v_2} = 
\begin{pmatrix}
  0 \\ 1
\end{pmatrix}$. 
\begin{align*}
  \vec{y} &= c_1e^{\l t}
  \begin{pmatrix}
  1 \\ 0
\end{pmatrix}
+ v_2e^{\l t}
\begin{pmatrix}
  0 \\ 1
\end{pmatrix}\\
&= e^{\l t}\left[
c_1\begin{pmatrix}
  1 \\ 0
\end{pmatrix} + c_2
\begin{pmatrix}
  0\\1 
\end{pmatrix}
\right]\\
&= e^{\l t}\begin{pmatrix}
  c_1 \\ c_2
\end{pmatrix}
\end{align*}

\begin{enumerate}
  \item[-]$\tau > 0 \implies \lambda > 0$, so $\vec{y}$ diverges to infinite along $\begin{pmatrix}
  c_1 \\ c_2
\end{pmatrix}$, which is fixed by initial conditions. 
\item[-] $\tau < 0$ then as $t \to \infty$, $\vec{y} \to \begin{pmatrix}
  0 \\ 0
\end{pmatrix}$ along 
$\begin{pmatrix}
  c_1 \\ c_2
\end{pmatrix}$. 
\end{enumerate}

\item[\textsc{(4.2)}] $A$ is non-diagonalisable. We then consider the Jordan form:
\[
  W^{-1}AW = J
\]
Where $W = (\vec{v_1} \vec{w_2})$. Then 
\[
  \vec{y} = (c_1 + tc_2) e^{\l t}\vec{v_1} + c_2e^{\l t}\vec{w_2}
\]


\begin{enumerate}
  \item[$\bullet$] $\tau > 0 \implies \l > 0$, so as $t \to \infty$, the dominant term is $te^{\l t}c_2\vec{v_1}$ and the solution diverges. 
  \item[$\bullet$] $\tau < 0 \implies \l < 0$, so as $t \to \infty$, the dominant term is $te^{\l t}c_2\vec{v_1}$ and the solution $\vec{y} \to \begin{pmatrix}
  0 \\ 0
\end{pmatrix}$.
 

\end{enumerate}

	
\end{enumerate}





\subsektion{Phase Plane Analysis}

We consider $\dfrac{d\vec{y}}{dt} = A\vec{y}$, where $\vec{y} \in \begin{pmatrix}
  x \\ y 
\end{pmatrix}
\in \R^2$, and $\vec{y}(0) = 
\begin{pmatrix}
  x_0 \\ y_0 
\end{pmatrix}$. 

\begin{center}
\begin{tikzpicture}[domain = -3:3]
\draw[->] (-3,0) -- (3,0) node [right] {$x$};
\draw[->] (0,-2) -- (0,2) node[left]{$y$};
\draw[smooth] plot (\x,{0.2*abs((\x)^2)}) node [right] {$\tau^2 - 4\Delta$};
\end{tikzpicture}	
\end{center}


We are interested in finding the general trajectories of $\vec{y}(t)$ on $\R^2$, from an initial condition $(x_0,y_0)$.\\

\begin{definition}
The \emph{phase portrait} of a given system described by $A$ is a collection of the representative trajectories of all distinct behaviours of the system.
\end{definition}\vsp

\begin{proposition}
Trajectories cannot cross (except at special cases)	
\end{proposition}
\emph{Proof.} See \textsc{M2AA1}; this is based on the existence and uniqueness of solutions to ODEs. 




So the main points of phase plane analysis are:\lecturemarker{14}{balh}
\begin{enumerate}
\item $\vec{y}(t)$ is a trajectory on the plane $(x,y)$	
\item $(x(0),y(0)) = \vec{y}(0)$ is the initial condition which is a point on the plane
\item No crossing of trajectories except at special points.
\item Vector field: At every point of $(x,y)$ there is a vector defined at the point $\frac{d\vec{y}}{dt} = A\vec{y} \equiv$ ``velocity", which is always tangent to the trajectory at $\vec{y}$.
\item The eigenvectors define special directions on the plane: invariant under the dynamics. ``If we start on $\vec{v_i}$, we remain on $\vec{v_i}$''
\item Phase portrait: the collection of representative trajectories that describe the possible outcomes of the system.
\item Special points are to be found. 
\end{enumerate}

\begin{example}
$\dfrac{d\vec{y}}{dt} = A\vec{y}$, with $A = \begin{pmatrix}
  -4 & -3 \\ 2 & 3
\end{pmatrix}$.

$\tau = -1$, $\Delta = -6.$ Since $\l^2 -\tau \l + \Delta = 0$, we find $\l_1 = 2$, $\l_2 = -3$ with eigenvectors
\[
  \vec{v_1} = \begin{pmatrix}
  1 \\ -2
\end{pmatrix}
\quad \vec{v_2}
= \begin{pmatrix}
  3\\ -1
\end{pmatrix}
\]


%%% 
%%% Big trajectory plot here!
%%%%


Trajectory on $\vec{v_1}$: 



	
\end{example}








\subsektion{Bifurcations}



%!TEX root = linear-algebra.tex
\stepcounter{lecture}
\setcounter{lecture}{2}

\pagebreak
\addcontentsline{toc}{part}{Multivariable Calculus}{}
\sektion{Partial Differentiation}

\subsektion{Functions of Several Variables}


Up to now we've dealt with \lecturemarker{19}{5 March}

\[f: \mathbb{R} \to \mathbb{R}, \text{ i.e. } f(x) \to \mathbb{R}\]
and 
\[\vec{f}: \mathbb{R} \to \mathbb{R}^n, \text{ i.e. } \vec{f}(x) \to \mathbb{R}^n\]~

\begin{definition}
Given $\vec{x} = (x_1,\dots,x_2) \in \mathbb{R}^n, \exists! f(\vec{x}) \in \mathbb{R}$: 
\[\vec{f}: \mathbb{R}^n \to \mathbb{R}, \text{ i.e. } f(\vec{x}) = f(x_1,\dots,x_2) \to \mathbb{R}\]	
$f$ is then an \emph{multivariable function.} 
\end{definition}

\pagebreak


\subsektion{Partial Differentiation}
Notions of limit and continuity are similar to functions of one variable (see M2P1):

Limit: $\lim_{{\vec{x}}\to{\vec{x}^*}} f(\vec{x}) = c$ iff $\forall \epsilon >0,~\exists \delta > 0 \text{ such that } |\vec{x} - \vec{x}^*| < \delta \implies |f(\vec{x}) - c| < \epsilon$

Continuity: $\lim_{{\vec{x}}\to{\vec{x}^*}} f(\vec{x}) = f(\vec{x}^*)$\\

\begin{definition}[Partial Derivative]
Given $f(x,y)$, the partial derivative of $f$ with respect to $x$: 
\[\frac{\partial f}{\partial x} = \lim_{h\to0}\frac{f(x+h,y) - f(x,y)}{h} = \left.\frac{\del f}{\del x}\right|_y\]
similarly 
\[\frac{\partial f}{\partial y} = \lim_{h\to0}\frac{f(x,y+h) - f(x,y)}{h} = \left.\frac{\del f}{\del y}\right|_x\]

In general for $f(x_1,\dots,x_n) = f(\vec{x}),~\vec{x} \iR^n$
\[\lim_{h\to0}\frac{f(x_1,\dots,x_{i+h},\dots,x_n) - f(\vec{x})}{h} = \frac{\partial f}{\partial x_i}\]
\end{definition}

We can calculate higher derivates as you would expect: 

\[
\begin{aligned}
  \frac{\del f}{\del x} &= g_1(x,y) \longrightarrow \begin{cases}
 \dfrac{\del g_1}{\del x} = \dfrac{\del^2f}{\del x^2}\\[0.1cm]
 \dfrac{\del g_1}{\del y} = \dfrac{\del^2f}{\del y\del x}
 \end{cases}\\[0.1cm]
   \dfrac{\del f}{\del y} &= g_2(x,y) \longrightarrow \begin{cases}
 \dfrac{\del g_2}{\del x} = \dfrac{\del^2f}{\del x\del y}\\[0.1cm]
 \dfrac{\del g_2}{\del y} = \dfrac{\del^2f}{\del y^2}
 \end{cases}
\end{aligned}\]


\begin{theorem} Iff $f, \dfrac{\del f}{\del x}, \dfrac{\del f}{\del y}$ are continuous, then 
$\dfrac{\del^2f}{\del x\del y} = \dfrac{\del^2f}{\del y\del x}$
\end{theorem}
\begin{proof}
See M2AA2: Multivariable Calculus.
\end{proof}


\begin{example}
Consider $u(x,y) = x^2\sin y + y^3$. 

\[
\begin{aligned}
  \frac{\del f}{\del x} = (2x)\sin y&\longrightarrow \begin{cases}
 \dfrac{\del^2f}{\del x^2}= \sin y (2)\\[0.1cm]
 \dfrac{\del^2f}{\del y\del x} = (2x)\cos y
 \end{cases}\\[0.1cm]
   \dfrac{\del f}{\del y} = x^2\cos y + 3y^2 &\longrightarrow \begin{cases}
 \dfrac{\del^2f}{\del x\del y} =x^2(\sin y)+6y\\[0.1cm]
\dfrac{\del^2f}{\del y^2} = \cos y\cdot 2x
 \end{cases}
\end{aligned}\]
\end{example}

Consider 
\[
\begin{aligned}
  \Delta f &= f(x + \Delta x,y + \Delta y) - f(x,y)\\
  &= [f(x + \Delta x,y) - f(x,y)] + [f(x+\Delta x, y + \Delta y) - f(x + \Delta x,y)]\\
  &= \left[\frac{f(x+\Delta x,y)-f(x,y)}{\Delta x}\right]\Delta x
  + \left[\frac{f(x+\Delta x,y+\Delta y) - f(x+\Delta x,y)}{\Delta y}\right]\Delta y
\end{aligned}
\]

Taking $\lim \Delta x \to 0, \Delta y \to 0$ we get:
\[df = \left.\frac{\del f}{\del x}\right|_{(x,y)}dx + \left.\frac{\del f}{\del y}\right|_{(x,y)}dy\]


\begin{definition}[Total Differential]  The total differential evaluates the infinitesimal change of $f(x,y)$ when \emph{all} independent variables are varied

\[df = \frac{\partial f}{\partial x}dx + \frac{\partial f}{\partial y}dy\]

For $f: \R^n \to \R$
\[df = \sum_{i=1}^n \frac{\del f}{\del x_i}dx_i\]
\end{definition}

\subsubsektion{Chain Rule}

Reminder: For $u = u(x),~ x \in \mathbb{R}$, $x = x(t)$, $\dfrac{du}{dt} = \dfrac{du}{dx}\dfrac{dx}{dt}$. 

\emph{What is the equivalent for $f(\vec{x})$?}

For $u = u(x,y)$, $x = x(t),~y = y(t)$, the change of $u$ when $t$ is changed
\[du = \frac{\partial u}{\partial x}dx + \frac{\partial u}{\partial y}dy\]

Since $dx = \dfrac{dx}{dt}dt,~ dy = \dfrac{dy}{dt}dt$, we have
\[\begin{aligned}dy &= \frac{\partial u}{\partial x}\frac{dx}{dt}dt + \frac{\partial u}{\partial y}\frac{dy}{dt}dt\\
&= \left[\frac{\partial u}{\partial x}\frac{dx}{dt} + \frac{\partial u}{\partial y}\frac{dy}{dt}\right]dt
\end{aligned}
\]
\begin{equation}
\implies \boxed{\frac{dy}{dt} = \frac{\partial u}{\partial x}\frac{dx}{dt} + \frac{\partial u}{\partial y}\frac{dy}{dt}}
\end{equation}~

For $u = u(x,y), x = x(t), y=t(t)$\lecturemarker{20}{5 March}
\[
\begin{aligned}
  du &= \frac{\del u}{\del x}dx + \frac{\del u}{\del y}dy\\
  dx &= \frac{dx}{dt}dt\\
  dy &= \frac{dy}{dt}dt\\
  du &= \left[\frac{\del u}{\del x}\frac{dx}{dt} + \frac{\del u}{\del y}\frac{dy}{dt}\right]dt
\end{aligned}
\]

\begin{example}[Chain Rule]
$V = \pi r^2h = V(r,h)$. $r = 2t = r(t)$, $h = 1+t^2 = h(t)$. 

\setlength{\jot}{10pt} 
\[\begin{aligned}\frac{dV}{dt} &= \frac{\partial V}{\partial r}\frac{dr}{dt} + \frac{\partial V}{\partial h}\frac{dh}{dt}\\
	&= 2\pi rh \cdot 2 + \pi r^22t\\ &= 2\pi r(2h+rt)\\ 
	&= 4\pi t(2 + 4t^2)
\end{aligned}
\]

We can check this by substituting first and differentiating as usual: 
\[
\begin{aligned}
  V &= \pi (2t)^2(1+t^2)\\
  \frac{dV}{dt} &= \pi[4(2t)(1+t^2) + (2t)^22t]\\
  &= 8\pi t[1 + t^2 + t^2]
\end{aligned}
\]

\end{example}

For $u(x,y), y = y(x,t)$ we have:

\[
\begin{aligned}
  du &= \frac{\del u}{\del x}dx + \frac{\del u}{\del y}dy\\
  dy &= \frac{\del y}{\del x}dx + \frac{\del y}{\del t}dt\\
  du &= \frac{\del u}{\del x}dx + \frac{\del u}{\del y}\left[\frac{\del y}{\del x}dx + \frac{\del y}{\del t}dt\right]\\
  &= \left[\frac{\del u}{\del x} + \frac{\del u}{\del y}\frac{\del y}{\del x}\right]dx + \left[\frac{\del u}{\del y}\frac{\del y}{\del t}\right]dt\\
  &= \left.\frac{\del u}{\del x}\right|_{t}dx + \left.\frac{\del u}{\del t}\right|_{x}dt
\end{aligned}
\]

\begin{example}[Chain Rule]
	For $u(x,y) = xy + y^2,~ y = x + t$ we have 
	\[du = [y + (x+2y)\cdot 1]dx + [(x+2y)\cdot 1]dt\]
\end{example}

$h = h(x,y),~ x = x(u,v),~y = y(u,v)$ we have 

\[
\begin{aligned}
  dh &= \frac{\del h}{\del x}dx + \frac{\del h}{\del y}dy\\
  dx &= \frac{\del x}{\del u}du + \frac{\del x}{\del v}dv\\
  dy &= \frac{del y}{\del u}du + \frac{\del y}{\del v}dv\\
  dh &= \frac{\del h}{\del x}\left[\frac{\del x}{\del u}du \frac{\del x}{\del v}dv\right] + \frac{\del h}{\del y}\left[\frac{\del y}{\del u}du + \frac{\del y}{\del v}dv\right]\\
  &= \left[\frac{\del h}{\del x}\frac{\del x}{\del u} + \frac{\del h}{\del y}\frac{\del y}{\del u}\right]du + \left[\frac{\del h}{\del x}\frac{\del x}{\del v} + \frac{\del h}{\del y}\frac{\del y}{\del v}\right]dv\\
  &= \frac{\del h}{\del u}du + \frac{\del h}{\del v}dv
\end{aligned}
\]

\subsektion{Implicit Functions}
Reminder: For functions of one variable, the explicit form is $y = f(x),~ f: \R \to \R$. Implicit form is of $F(x,y) = 0$ e.g $x^2 + y^2 -R^2 = 0$\\

\begin{definition}For two variables a function in implicit form: 
\[F(x,y,z) = 0\]
\end{definition}

The total differential in these cases are easy to obtain:
\[dF = \frac{\del F}{\del x}dx + \frac{\del F}{\del y}dy + \frac{\del F}{\del z}dz = 0\]
\begin{example}
$u(x,y) = x^2 + y^2 - 5$

From the explicit form directly we find:
\[\frac{\del u}{\del x} = 2x, \quad \frac{\del u}{\del y} = 2y.\]	

Suppose we cannot express the function in explicit form, so we only have the implicit function:
\[F(x,y,z) = x^2 + y^2 -5 - z = 0\]
\[dF = \frac{\del F}{\del x}dx + \frac{\del F}{\del y}dy + \frac{\del F}{\del z}dz = 0\]

In this case: 
\[2xdx + 2ydy -1dz = 0\]

To find $\dfrac{\del z}{\del x}$ note that:
\[
\begin{aligned}
  dz &= \left[\frac{-\frac{\del F}{\del x}}{\frac{\del F}{\del z}}\right]dx
  + 
  \left[\frac{-\frac{\del F}{\del y}}{\frac{\del F}{\del z}}\right]dy\\
  &= \frac{\del z}{\del x}dx + \frac{\del z}{\del y}dy
\end{aligned}
\]
\[
\begin{aligned}
  \frac{\del z}{\del x} &= \left[\frac{-\frac{\del F}{\del x}}{\frac{\del F}{\del z}}\right] = \frac{-2x}{-1} = 2x\\
  \frac{\del z}{\del y} &=  \left[\frac{-\frac{\del F}{\del y}}{\frac{\del F}{\del z}}\right] = \frac{-2y}{-1} = 2y
\end{aligned}
\]
\end{example}


\sektion{Functions of Two Variables}
\subsektion{Taylor expansion} 

Reminder: \lecturemarker{21}{5 March}
 For functions of one variable, assuming continuity: 
\[f(x_0 + \delta x) = f(x_0) + \left.\frac{df}{dx}\right|_{x_0}\delta x + \frac{1}{2!}\left.\frac{d^2f}{dx^2}\right|_{x_0}(\delta x)^2 + \dots\]

For a function of two variables, $f(\vec{x}): \vec{x} \iR^2$ evaluated at $\vec{x_0} = (x_0,y_0)$ we assume suitably continuity and find similar expansion: 
\[
\begin{aligned}
  f(\vec{x}) &= f(\vec{x}_0 + \delta\vec{x}_0) = f(x_0+\delta x, y_0 + \delta y) \\
  &= f(x_0, y_0 + \delta y) + \left[\frac{\del f}{\del x}(x_0,y_0 + \delta y)\right]\delta x + \frac{1}{2!}\frac{\del^2f}{\del x^2}(x_0,y_0+\delta y)(\delta x)^2 + \mathcal{O}(||\delta x||^3)\\
\end{aligned}
\]

Now 
\setlength{\jot}{10pt}
\[
\begin{aligned}
f(x_0, y_0 + \delta y)&= f(x_0,y_0) + \frac{\del f}{\del y}(x_0,y_0)\delta y + \frac{1}{2!}\frac{\del^2f}{\del y^2}(x_0,y_0)(\delta y)^2 + \mathcal{O}(||\delta x||^3)\\
\left[\frac{\del f}{\del x}(x_0,y_0 + \delta y)\right]\delta x &= \delta x\left[\frac{\del f}{\del x}(x_0,y_0) + \frac{\del^2f}{\del y \del x}(x_0,y_0)\delta y + \frac{1}{2!}\frac{\del^3f}{\del^2y\del x}(x_0,y_0)(\delta y)^2 + \dots\right]\\
\frac{1}{2!}\frac{\del^2f}{\del x^2}(x_0,y_0+\delta y)(\delta x)^2 &= (\delta x)^2\frac{1}{2!}\left[\frac{\del^2f}{\del x^2}(x_0,y_0) + \frac{\del^3f}{\del y\del x}(x_0,y_0)\delta y + \dots \right]
\end{aligned}\]~

So to 2nd order:
\[f(\vec{x}_0 + \delta \vec{x}) = f(\vec{x}) + \left(\frac{\del f}{\del x}(\vec{x}_0)\,\frac{\del f}{\del y}(\vec{x}_0)\right)\begin{pmatrix}
\delta x\\ \delta y	
\end{pmatrix}
+ \frac{1}{2}(\delta x\,\delta y)
\begin{pmatrix}
	\frac{\del^2f}{\del x^2}(\vec{x}_0) & \frac{\del^2f}{\del x\del y}(\vec{x}_0)\\
	\frac{\del^2f}{\del y\del x}(\vec{x}_0) & \frac{\del^2f}{\del y^2}(\vec{x}_0)
\end{pmatrix}
\begin{pmatrix}
\delta x\\ \delta y	
\end{pmatrix}
\]

\begin{definition}
\emph{Gradient} (vector): $\vec{\nabla}f(\vec{x}) = \left(\frac{\del f}{\del x}\, \frac{\del f}{\del y}\right)$

\emph{Hessian} (matrix): 
\[H(\vec{x}) = \begin{pmatrix}
	\frac{\del^2f}{\del x^2} & \frac{\del^2f}{\del x\del y}\\
	\frac{\del^2f}{\del y\del x} & \frac{\del^2f}{\del y^2}
\end{pmatrix}\]
\end{definition}

So the taylor expansion is:
\begin{equation}\boxed{f(\vec{x}_0 + \delta \vec{x}) = f(\vec{x}_0) + \vec{\nabla}f(\vec{x}_0)\delta \vec{x} + \frac{1}{2}\delta\vec{x}\,H(\vec{x}_0)\delta\vec{x} + \mathcal{O}(||\delta\vec{x}||^3)}\end{equation}
\pagebreak
\begin{example}
$u(x,y) = e^{2x-y}$. Find the taylor expansion around $\vec{x}_0 = (0,0)$ to 2nd order. 
\[
\begin{aligned}
  \vec{\nabla}u(\vec{x}) &= (e^{-y}2e^{2x}\, e^{2x}(-1)e^{-y}\\
  \implies \vec{\nabla}u(\vec{x}_0) &= (2\,-1)\\[0.1cm]
  H(\vec{x}) &= \begin{pmatrix}
 4e^{2x}e^{-y} & -2e^{2x}e^{-y}\\
 -2e^{2x}e^{-y} & e^{2x}e^{-y}	
 \end{pmatrix}\\
 \implies H(\vec{x}_0) &= \begin{pmatrix}
 	4 & -2 \\ -2 & 1
 \end{pmatrix}
\end{aligned}
\]
\[
\begin{aligned}
  u(\vec{x}_0 + \delta \vec{x}) &= u(0,0) + (2 \, -1)\delta \vec{x} + \frac{1}{2}\delta \vec{x} \begin{pmatrix}
  4 & -2\\ -2 & 1
  \end{pmatrix}\delta \vec{x} + \mathcal{O}(||\delta\vec{x}||^3)\\
  &= 1 + (2\delta x -\delta y) + (2\delta x^2 -4\delta x \delta y + \frac{1}{2}\delta y^2) + \mathcal{O}(||\delta\vec{x}||^3)
\end{aligned}
\]
\end{example}

\begin{example}
$A = xy = A(x,y),~ \vec{x}_0 = (x_0, y_0)$

\[
\begin{aligned}
  A(\vec{x}_0+ \delta \vec{x}_0) &= A(\vec{x}_0) + (y_0\, x_0)\delta \vec{x} + \frac{1}{2}\delta\vec{x}\begin{pmatrix}
0 & 1\\ 1 & 0	
\end{pmatrix}\delta\vec{x} + \mathcal{O}(||\delta\vec{x}||^3)\\
&= A_0 + (y_0\delta x + x_0\delta y) + \delta x \delta y
\end{aligned}
\]
\end{example}~

\begin{example}[Error Propagation]
$h = x\tan \theta = h(x,\theta)$

\[
\begin{rcases*}
x \pm \delta x \\
\theta \pm \delta \theta 
\end{rcases*} \longrightarrow h + \delta h
\]	

\[
\begin{aligned}
  \delta h &= \frac{\del h}{\del x}\delta x + \frac{\del h}{\del \theta}\delta \theta \\
  &= |\tan\theta||dx| + |x\sec^2\theta||\delta \theta|
\end{aligned}
\]

We use the worst case for error propagation (Hence the absolute values)

The linear approximation: 
\[h(x+\delta x, \theta + \delta \theta) = h(x,\theta) + \frac{\del h}{\del x}\delta x + \frac{\del h}{\del y}\delta y + \mathcal{O}(||\delta\vec{x}||^2) \]
\[\boxed{\delta h \approx \frac{\del h}{\del x}\delta x + \frac{\del h}{\del y}\delta y}\]

\end{example}

\pagebreak


\subsektion{Change of coordinates}
\[
\begin{aligned}
&\begin{cases}\lecturemarker{22}{5 March}
x &= r\cos\theta = x(r,\theta)\\
y &= r\sin\theta = y(r,\theta)
\end{cases}\\[0.2cm]
&\begin{cases}
r &= +\sqrt{x^2 + y^2} = r(x,y)\\
\theta &= \arctan(y/x) = \theta(x,y)	
\end{cases}  
\end{aligned}
\]

Transformation to polar coordinates:
\[
\begin{aligned}
  dx &= \frac{\del x}{\del r}dr + \frac{\del x}{\del \theta} d\theta = \cos\theta dr + (-r\sin\theta)d\theta \\
  dy &= \frac{\del y}{\del r}dr + \frac{\del y}{\del \theta}d\theta = \sin\theta dr + r\cos\theta d\theta\\[0.2cm]
  \begin{pmatrix}
  dx \\dy 	
  \end{pmatrix}
  &= \underbrace{\begin{pmatrix}
 \cos\theta & -r\sin\theta\\
 \sin\theta & r\cos\theta	
 \end{pmatrix}}_{\text{Jacobian matrix }J_{p \to c}}
 \begin{pmatrix}
 dr\\ d\theta 	
 \end{pmatrix}
\end{aligned}
\]

\begin{definition}
The \emph{Jacobian matrix} $J$ is change of coordinate matrix:
\[J = \begin{pmatrix}
 \frac{\del x}{\del r} & \frac{\del x}{\del \theta}\\
 \frac{\del y }{\del r} & \frac{\del y}{\del \theta}	
 \end{pmatrix}
\]	


\end{definition}

We can also find the Jacobian matrix to change from cartesian to polar coordinates: 
\[\begin{pmatrix}
dr \\ d\theta	
\end{pmatrix}
\underbrace{\begin{pmatrix}
\frac{\del r}{\del x} & \frac{\del r}{\del y}\\
\frac{\del \theta}{\del x} & \frac{\del \theta}{\del y}	
\end{pmatrix}}_{J_{c\to p}}
\begin{pmatrix}
dx \\ dy	
\end{pmatrix}
\]

\[
\begin{aligned}
  \frac{\del r}{\del x} &= \frac{12}{2}(x^2 + y^2)^{-1/2}\cdot 2x = \frac{r\cos\theta}{r} =\cos\theta\\
  \frac{\del r}{\del y} &= \frac{12}{2}(x^2 + y^2)^{-1/2}\cdot 2y = \sin\theta\\
  \frac{\del \theta}{\del x} &= \frac{1}{1 + (y/x)^2}y\left(\frac{-1}{x^2}\right) = \frac{-y}{x^2 + y^2} = \frac{-\sin\theta}{r}\\
    \frac{\del \theta}{\del y} &= \frac{1}{1 + (y/x)^2}y\cdot\frac{1}{x} = \frac{x}{x^2 + y^2} = \frac{\cos\theta}{r}
\end{aligned}
\]

\[\implies J_{c\to p} = 
\begin{pmatrix}
\cos\theta & \sin\theta\\
\frac{-\sin\theta}{r} & \frac{\cos\theta}{r}	
\end{pmatrix}
= J^{-1}_{p \to c} 
\]

\pagebreak
\begin{example}
$ds^2 = (dx)^2	+ (dy)^2$

\[
\begin{aligned}
  ds^2 &= (dx\, dy)\begin{pmatrix}
dx \\ dy	
\end{pmatrix}\\
&= (dr\, d\theta) J^TJ \begin{pmatrix}
 dr \\ d\theta	
 \end{pmatrix}\\[0.2cm]
 J^TJ &= \begin{pmatrix}
 \cos\theta & \sin\theta \\
 -r\sin\theta & r\cos\theta 	
 \end{pmatrix}
\begin{pmatrix}
\cos\theta & -r\sin\theta\\
\sin\theta & r\cos\theta 	
\end{pmatrix}\\
&= \begin{pmatrix}
 1 & 0\\
 0 & r^2	
 \end{pmatrix}\\[0.2cm]
\implies  ds^2 &= (dr \, d\theta)
\begin{pmatrix}
1 & 0 \\ 0 & r^2	
\end{pmatrix}
\begin{pmatrix}
dr \\ d\theta 	
\end{pmatrix}\\
&= (dr)^2 + r^2(d\theta)^2
\end{aligned}
\]

\end{example}~

\begin{example}
$dA = dx\,dy$

\[
\begin{aligned}
  J &= \begin{pmatrix}
 \cos\theta & -r\sin\theta\\
 \sin\theta & r\cos\theta 	
 \end{pmatrix}\\
 det(J) &= r\cos^2\theta + r\sin^2\theta = r\\
 dA &= dx\,dy = det(J)\,dr\,d\theta 
\end{aligned}
\]

\end{example}

\begin{theorem}
det$(J)$ gives the increase in area (``volume'') induced by the transformation  $J$\end{theorem}

\begin{proof}

 Consider $(x,0)$ and $(0,y)$ and the matrix transform $U = \begin{pmatrix}
 a & b \\ c & d	
 \end{pmatrix}$ applied to these points. 

We know that the area is given by the magnitude of the cross-product of the two vectors: 
 \[A = || \vec{a} \times \vec{b}||\]
 
Applying $U$ to our points we have: 

\[
\begin{aligned}
  U\begin{pmatrix}
  x\\0
  \end{pmatrix}
  &= \begin{pmatrix}
 ax\\ cx	
 \end{pmatrix}
 = x\begin{pmatrix}
a \\ c	
\end{pmatrix}\\
  U\begin{pmatrix}
  0\\y
  \end{pmatrix}
 &= y\begin{pmatrix}
b \\ d
\end{pmatrix}
\end{aligned}
\]

In our case we have: 
\[
\begin{aligned}
  A &= \begin{vmatrix}
 \vec{i} & \vec{j} & \vec{k}\\
 xa & xc & 0\\
 by & dy & 0	
 \end{vmatrix}
= || (xyad - bcxy)\vec{k}||\\
&= xy(ad -bc)
\end{aligned}
\]	

Thus $A = \det(U)A_0$. Hence $dx\,dy = \det(J)\,dr\,d\theta$ in the previous example. This also extends to 3D with volume.
\end{proof}~


\begin{example}[Change of variables]
	$u(x,y) \longrightarrow u(r,\theta)$
	
	\[
\begin{aligned}
  \frac{\del u}{\del x} &= \frac{\del u}{\del r}\frac{\del r}{\del x} + \frac{\del u}{\del \theta} \frac{\del \theta}{\del x}\\
  \frac{\del u}{\del y} &= \frac{\del u}{\del r}\frac{\del r}{\del y} + \frac{\del u}{\del \theta} \frac{\del \theta}{\del y}
\end{aligned}
\]

\[\begin{pmatrix}
\frac{\del u}{\del x}\\ \frac{\del u}{\del y}
\end{pmatrix}
= \left( \frac{\del u}{\del r}\, \frac{\del u}{\del \theta}\right)
\underbrace{\begin{pmatrix}
\frac{\del r}{\del x} & \frac{\del r}{\del y}\\
\frac{\del \theta}{\del x} & \frac{\del \theta}{\del y}	
\end{pmatrix}}_{J^{-1}}
\]
\end{example}


\subsektion{Partial Differential Equations}
Main Application: \lecturemarker{23}{5 March}
Partial Differential Equations (PDEs)

\[f(\vec{x}), \vec{x} \iR^2, \vec{x} = \begin{pmatrix}
 x \\ y	
 \end{pmatrix}
\]

\[F\left(x,y,f,\frac{\del f}{\del x}, \frac{\del f}{\del y}, \frac{\del^2f}{\del x^2}, \frac{\del^2f}{\del x\del y}\frac{\del^2f}{\del y^2},\dots\right) = 0\]~


\subsubsektion{Laplace Equation}
\[u(x,y),\quad \frac{\del^2u}{\del x^2} + \frac{\del^2u}{\del y^2} = 0\]

Motivation: 
\begin{enumerate}
\item Complex Analysis: $f = u(x,y) + i\,v(x,y)$

Analytic Functions:

(a) Cauchy Riemann Conditions:
$\dfrac{\del u}{\del x} = \dfrac{\del u}{\del y}, \quad \dfrac{\del u}{\del y} = -\dfrac{\del v}{\del x}$

(b) Continuous $\implies \dfrac{\del^2u}{\del x\del y} = \dfrac{\del^2u}{\del y\del x}$

Together these $\implies \dfrac{\del^2u}{\del x^2} + \dfrac{\del^2u}{\del y^2} = 0$\\

\item Physical Fields: 
(as in Mechanics) When you have a conservative field, it can be written in terms of a gradient of a function. There is a similar statement (see Multivariable calculus) that follows from the divergence of a vector which immediately shows that the electrostatic potential will fulfil the Laplace equation: 
\[\dfrac{\del^2u}{\del x^2} + \dfrac{\del^2u}{\del y^2} = 0\]

$u(x,y)$ will be the electrostatic potential. In reality we look for a solution in terms of $u(r,\theta)$. In many cases solutions are simpler in polar coordinates. Find the Laplace Equation for: 
\[\begin{cases} 
x &= r\cos\theta\\
y &= r\sin\theta 	
\end{cases}\longrightarrow u(r,\theta) \text{ i.e. in polar coordinates}
\]

\[\frac{\del u}{\del x} = \frac{\del u}{\del r}\frac{\del r}{\del x} + \frac{\del u}{\del \theta}\frac{\del \theta}{\del x} = \cos\theta \frac{\del u}{\del r} + \left(\frac{-\sin\theta}{r}\right)\frac{\del u}{\del \theta}\]

Note: $\dfrac{\del u}{\del r} = \left[\cos\frac{\del}{\del r} - \frac{\sin\theta}{r}\frac{\del}{\del \theta}\right]u$

\emph{What is $\dfrac{\del^2u}{\del x^2}$}?
\setlength{\jot}{8pt}

\begin{align*}
  \frac{\del^2u}{\del x^2} 
  &= \frac{\del}{\del x}\left[\frac{\del}{\del x}\right][u]\\
  &= \left[\cos\theta\frac{\del}{\del r} - \frac{\sin\theta}{r}\frac{\del}{\del \theta}\right]\left[\cos\theta \frac{\del}{\del r} - \frac{\sin\theta}{r}\frac{\del}{\del \theta}\right]u\\
  &= \cos\theta\frac{\del}{\del r}\left[\cos\theta\frac{\del u}{\del r}\right] - \cos\theta\frac{\del}{\del r}\left[\frac{\sin\theta}{r}\frac{\del u}{\del \theta}\right] - \frac{\sin\theta}{r}\frac{\del}{\del \theta}\left[\cos\theta\frac{\del u}{\del r}\right]\\
     &= \cos^2\theta\frac{\del^2u}{\del r^2}-\cos\theta\sin\theta\frac{\del}{\del r}\left[\frac{1}{r}\frac{\del u}{\del \theta}\right] - \frac{\sin\theta}{r}\frac{\del}{\del \theta}\left[\cos\theta\frac{\del u}{\del r}\right] + \frac{\sin\theta}{r^2}\frac{\del}{\del \theta}\left[\sin\theta\frac{\del u}{\del \theta}\right]
  \end{align*}
\begin{align*}
 \implies  \frac{\del^2u}{\del x^2} 
  &= \cos^2\theta\frac{\del^2u}{\del r^2} - \cos\theta\sin\theta\left[-\frac{1}{r^2}\frac{\del u}{\del \theta} + \frac{1}{r}\frac{\del^2u}{\del r\del \theta}\right]\\ 
  &- \frac{\sin\theta}{r}\left[(-\sin\theta)\frac{\del u}{\del r} + \cos\theta \frac{\del^2u}{\del \theta}{\del r}\right] + \frac{\sin\theta}{r^2}\left[\cos\theta\frac{\del u}{\del \theta} + \sin\theta\frac{\del^2u}{\del \theta^2}\right]\\
  &= \cos^2\theta \frac{\del^2u}{\del r^2} + 2\frac{\cos\theta\sin\theta}{r^2}\frac{\del u}{\del \theta} - \frac{2\cos\theta\sin\theta}{r}\frac{\del^2u}{\del r\del \theta} + \frac{\sin^2\theta}{r}\frac{\del u}{\del r} + \frac{\sin^2\theta}{r^2}\frac{\del^2u}{\del\theta^2}
\end{align*}

\[
\begin{aligned}
  \frac{\del^2u}{\del y^2} 
  &= \left[\sin\theta\frac{\del}{\del r} + \frac{\cos\theta}{r}\frac{\del }{\del \theta}\right]
  \left[\sin\theta \frac{\del u}{\del r} + \frac{\cos\theta}{r}\frac{\del u}{\del \theta}\right] \\
  &= [\dots] \text{ (problem set)}\\
  &= \sin^2\theta \frac{\del^2u}{\del r^2} - 2\frac{\cos\theta\sin\theta}{r^2}\frac{\del u}{\del \theta} +\frac{2\cos\theta\sin\theta}{r}\frac{\del^2u}{\del r\del \theta} + \frac{\cos^2\theta}{r}\frac{\del u}{\del r} + \frac{\cos^2\theta}{r^2}\frac{\del^2u}{\del\theta^2}
\end{aligned}
\]\vsp

Thus the Laplace Equation in Polar Coordinates is: 
\[\boxed{\frac{\del^2u}{\del x^2} + \frac{\del^2}{\del y^2} = \frac{\del^2u}{\del r^2} + \frac{1}{r}\frac{\del u}{\del r} + \frac{1}{r^2}\frac{\del^2u}{\del \theta^2}}\]
\end{enumerate}


\subsubsektion{Wave Equation}

\[u = u(x,t) \quad \frac{\del^2u}{\del x^2} - \frac{1}{c^2}\frac{\del^2u}{\del t^2} = 0\]

The D'Alembert solution:
\[\boxed{u = u(\zeta) \quad \zeta = x -ct}\]

\[
\begin{rcases*}
du = \frac{\del u}{\del \zeta} d\zeta\\
d\zeta = dx - cdt	
\end{rcases*}
du = \equalto{\frac{du}{d\zeta}dx}{\frac{\del u}{\del x}} \equalto{- c\frac{du}{d\zeta}}{\frac{\del u}{\del t}}dt
\]

\[
\begin{aligned}
  \frac{\del^2u}{\del x^2} &= \frac{\del}{\del x}\left[\frac{\del u}{\del x}\right] = \frac{\del}{\del x}\left[\frac{\del u}{\del \zeta}\right]\\
  d\left(\frac{\del u}{\del \zeta}\right) &=  \frac{\del^2u}{\del \zeta^2}d\zeta = \frac{\del^2u}{\del \zeta^2}[dx - cdt]\\
  &= \frac{\del}{\del x}\left[\frac{\del u}{\del \zeta}\right]dx + \frac{\del}{\del t}\left[\frac{\del u}{\del \zeta}\right]dt\\
  \frac{\del}{\del x}\left[\frac{\del u}{\del \zeta}\right] &= \frac{\del^2u}{\del \zeta^2} = \frac{\del^2u}{\del x^2}\\
  \frac{\del}{\del t}\left[\frac{\del u}{\del \zeta}\right] &= -c\frac{\del^2u}{\del \zeta^2}\\
  \implies \frac{\del^2u}{\del t^2} &= c^2\frac{\del^2u}{\del \zeta^2}
\end{aligned}
\]

So $\frac{\del^2u}{\del x^2} -\frac{1}{c^2}\frac{\del^2u}{\del t^2} = 0$ has solution $u(\zeta)$ if $\frac{d^2u}{d\zeta^2}$ exists, where $\zeta = x-ct$. 

\subsektion{Exact Differential Equations}
Reminder: 1st Order ODEs
\[\frac{dy}{dx} = F(x,y) = f(x)g(y)\]

Separable: $\displaystyle{\int \frac{dy}{g(y)} = \int f(x)\,dx\quad y = y(x)}$. 

\emph{What about non-separable equations?}

Consider the implicit function $u(x,y) = 0$

\[du = \frac{\del u}{\del x}dx + \frac{\del u}{\del y}dy = 0\]

$u(x,y)$ then fulfils the ODE $\dfrac{dy}{dx} = \dfrac{-\frac{\del u}{\del x}}{\frac{\del u}{\del y}}$, and if $u$ is continuous $\frac{\del^2u}{\del x\del y}= \tfrac{\del^2u}{\del y\del x}$. $u(x,y) = 0$ is the solution of the ODE $(1)$ in implicit form because $\frac{\del F}{\del y} = \frac{\del^2u}{\del y\del x} = \frac{\del G}{\del x} = \frac{\del^2u}{\del x\del y}$\\


\begin{definition}
	 The non-separable ODE: 
\[\frac{dy}{dx} = \frac{-F(x,y)}{G(x,y)}\tag{1}\]
is an \emph{exact ODE} if it satisfies the conditions of integrability: 
\[\dfrac{\del F}{\del y} = \dfrac{\del G}{\del x} \implies u(x,y)\text {is a solution}\]
Then $u(x,y) = 0$ is a solution to $(1)$, which is found by solving: 
\[F(x,y) = \frac{\del u}{\del x}\quad G(x,y) = \frac{\del u}{\del y}\]
\end{definition}\vspace*{5pt}


\begin{example}\lecturemarker{24}{5 March}
\[\frac{dy}{dx} = \frac{-2xy-\cos x\cos y}{x^2-\sin x\sin y} \tag{$*$}\]	

\[\underbrace{(2xy + \cos x \cos y)}_{F(x,y)}dx + \underbrace{(x^2-\sin x\sin y)}_{G(x,y)}dy = 0\]
\[
\begin{aligned}
  \begin{rcases*}
  \frac{\del F}{\del y} = 2x + \cos x(-\sin y)\\
  \frac{\del G}{\del x} = 2x - \sin y \cos x	
  \end{rcases*}
  \frac{\del F}{\del y} = \frac{\del G}{\del x}
\end{aligned}
\]
Hence this fulfils the conditions for integrability. 

\[
\begin{aligned}
  F &= \frac{\del u}{\del x} = 2xy + \cos x \cos y\\
  u &= ux^2 + \cos y \sin x + c_1 + f(y)\\
  \frac{\del u}{\del y} &= x^2 - \sin y\sin x + \frac{\del f}{\del y} \implies \frac{df}{dy} = 0\\[0.2cm]
  \implies \Aboxed{u(x,y) &= yx^2 + \cos y\sin x + C = 0}
\end{aligned}
\]

This is the solution for $(*)$. 
\end{example}

Another class of equations are not integrable, but can be made integrable by an \emph{integrating factor}. 
\[F(x,y)dx + G(x,y)dy = 0, \quad  \frac{\del F}{\del y} \neq \frac{\del G}{\del x} \text{ (not exact)}\]

 \begin{definition}
	$\lambda(x)$ or $\lambda(y)$ is the \emph{integrating factor} that turns our equations into exact form: 
\end{definition}

We find $\lambda(x)$ or $\lambda(y)$ by solving:
\[\lambda(x)F(x,y)dx + \lambda(x)G(x,y)dy = 0\]
which is done by integrating and solving the ODE: 
\[\frac{\del[\lambda F]}{\del y} = \frac{\del[\lambda G]}{\del x}\]


\begin{example}[Integrating Factor]
	\[\underbrace{(xy - 1)}_{F}dx + \underbrace{(x^2 -xy)}_{G}dy = 0\]
	\[\frac{\del F}{\del y} = x \neq \frac{\del G}{\del x} = 2x-y\]
	This equation is \emph{not exact.}
	
	Consider an integrating factor: $\lambda(x)$
	\[[\lambda(xy-1)]dx + [\lambda(x^2-xy)]dy = 0\]
	\[
\begin{aligned}
  \frac{\del[\lambda F]}{\del y} &= \frac{\del[\lambda G]}{\del x}\\
  \lambda(x)\frac{\del F}{\del y} &= \frac{\del \lambda}{\del x}G + \lambda\frac{\del G}{\del x}\\
\Aboxed{  \lambda x &= \frac{d\lambda}{dx}(x^2-xy) + \lambda(2x-y)}
\end{aligned}
\]

\[
\begin{aligned}
  \frac{d\lambda}{dx}(x^2-xy) + \lambda(x-y) &= 0\\
  (x-y)\left[\frac{d\lambda}{dx}x+\lambda\right] &= 0\\
  \int \frac{d\lambda}\lambda = -\int \frac{dx}x\\
  \lambda x &= C
\end{aligned}
\]
So our integrating factor is $\lambda = \frac{c}{x} = \frac{1}{x}$.

Check: $\lambda\cdot (1)$ gives 
\[\underbrace{(y-\textstyle{\frac{1}{x})}}_{H}dx + \underbrace{(x-y)}_{J}dy = 0\]
Then $\dfrac{\del H}{\del y} = 1 = \dfrac{\del J}{\del x}$, so our equation is exact!

\[
\begin{aligned}
  \frac{\del u}{\del x} &= y -\frac{1}{x}\\
 \implies  u &= yx - \log x + f(y) + c_1\\[0.4cm]
  \frac{\del u}{\del y} &= x + \frac{df}{dy} = (x-y)\\
  \implies \frac{df}{dy} &= -y \implies f = -\frac{1}{2}y^2 + c_2
\end{aligned}
\]
Thus our solution is 
\[u(x,y) = xy - \log x -\frac{1}{2}y^2 + C = 0\]


\end{example}


Recap of an old friend: 1st order linear ODE
\[
\begin{aligned}
  \frac{dy}{dx} + F(x)y &= G(x)\\
  [F(x)y - G(x)]dx + [1]dy &= 0
\end{aligned}
\]

This ODE is not exact. We can find a $\lambda(x)$ that will make it exact:

\[
\begin{aligned}
  \lambda(x)[F(x)y - G(x)]dx + [\lambda(x)]dy &=0\\
  \frac{\del}{\del y}[\lambda(x)[F(x)y-G(x)]] &= \frac{d\lambda}{dx}
\end{aligned}
\]

\[
\begin{aligned}
  \implies \lambda(x)F(x) &= \frac{d\lambda}{dx}\\
  \implies \int \frac{d\lambda}{\lambda} &= \int F(x)\,dx\\
  \implies \lambda &= K\exp\left[\int F(x)\,dx\right]
\end{aligned}
\]
As in lecture 2. This integrating factor is always possible to solve. 






\subsektion{Stationary Points}\vspace*{5pt} % (fold)

For functions of one-variable, the stationary points occur when $\frac{df}{dx} = 0$. We find the maximum / minimum by looking at the sign of $\frac{d^2f}{dx^2}$. We can extend this to functions of two-variables:\\

\lecturemarker{25}{5 March}

\begin{definition}
For $u(\vec{x})$ with $\vec{x} \in \mathbb{R}^2$, the \emph{stationary points}, $\vec{x}^*$, occur when
\[\frac{\del u}{\del x}(\vec{x}^*) = \frac{\del u}{\del y}(\vec{x}^*) = 0\]
\end{definition}

\textit{What about the character? Maximum / Minimum / Saddle Point.}~\\

\begin{example}Looking at $f(x)$, $x \in \mathbb{R}$:
\[f(x) = f(x^*) + \cancel{\frac{df}{dx}(x^*)\delta x} + \frac{1}{2} \frac{d^2f}{dx^2}(x^*)(\delta x)^2 + \cal{O}(\delta x^3)\]
\[\implies f(x) - f(x^*) = \frac{1}{2} \frac{d^2f}{dx^2}(x^*)(\delta x)^2 + \cal{O}(\delta x^3)\]

So  $\frac{d^2f}{dx^2}(x^*) > 0 \iff f(x) - f(x^*) > 0$. Hence local minimum when $f(x) -f(x^*) > 0$.
\end{example}~

For $u = u(x,y)$, consider the taylor expansion about a stationary point $(x^*,y^*) = \vec{x}^*$  (so $\frac{\del u}{\del x}(\vec{x}^*) = \frac{\del u}{\del y}(\vec{x}^*) = 0$):
\[u(\vec{x}^* + \delta \vec{x}) = u(\vec{x}^*) + \cancel{\left[\frac{\del u}{\del x}(\vec{x}^*)\delta x + \frac{\del u}{\del y}(\vec{x}^*)\delta y \right]} 
+ \frac{1}{2} \delta\vec{x}^* \equivto{\begin{pmatrix}
 \frac{\del^2u}{\del x^2} & \frac{\del^2u}{\del x\del y}\\
 \frac{\del^2u}{\del y \del x} &\frac{\del^2u}{\del y^2}	
 \end{pmatrix}_{\vec{x}^*}}{H(\vec{x}^*)}\hspace*{-10pt}\delta \vec{x} + \mathcal{O}(||\delta \vec{x}||^3)
\]
Thus:
\[u(\vec{x}^* + \delta\vec{x}) - u(\vec{x}^*) = \frac{1}{2}\delta \vec{x}^* H(\vec{x}^*) \delta \vec{x} + \mathcal{O}(||\delta \vec{x}||^3)\]
So we get:
\[
\text{ Local }
\begin{cases}
\text{Minimum: } &\forall \delta\vec{x}^*,~\delta \vec{x}^* H(\vec{x}^*) \delta \vec{x} > 0 \\
& ||\delta\vec{x}|| \text{ is small }\\

\text{Maximum: } &\forall \delta\vec{x}^*,~\delta \vec{x}^* H(\vec{x}^*) \delta \vec{x} < 0 \\
& ||\delta\vec{x}|| \text{ is small }

\end{cases}\]~

Note that $\forall \vec{x},~ \vec{x}^*A\vec{x} > 0 \iff A \text{ is positive definite} \iff \text{All eigenvalues } \lambda_i \text{ are positive.}$

So for $H(\vec{x}^*) = \begin{pmatrix}
 A & B \\ B & C	
 \end{pmatrix}
$ (continuity $\implies \frac{\del^2u}{\del x \del y} = \frac{\del^2u}{\del y \del x} \implies H(\vec{x}^*) = H^T(\vec{x}^*)$), we have that:
\begin{enumerate}
\item[(1)] $H(\vec{x}^*)$ is always diagonalisable: 
\[v^{-1}Hv = \Lambda = \begin{pmatrix}
 \lambda_1 & 0 \\ 0 & \lambda_2	
 \end{pmatrix}
\]

\item[(2)] $v$ is orthogonal (i.e. $v^{-1} = v^T$), so: 
\[v^THv = \Lambda \implies H = v \Lambda v^T\]

\item[(3)] All eigenvalues are real:
\[\begin{pmatrix}
A & B \\ B & C	
\end{pmatrix},~
\lambda = \frac{\tau \pm \sqrt{\tau^2 - 4\Delta}}{2}
,~\tau^2-4\Delta > 0\]

\[\begin{aligned}
(A+C)^2 - 4(AC - B^2) &= A^2 + C^2 + 2AC - 4AC + 4B^2\\
&= A^2 + C^2 - 2AC + 4B^2\\
&= (A-C)^2 + 4B^2 > 0	
\end{aligned}
\]

\end{enumerate}


\begin{theorem}
$H = H^T$ is positive definite $\iff \lambda_1,\lambda_2$ are positive.	
\end{theorem}

\begin{proof}
$\lambda_1,\lambda_2 > 0 \implies$ positive definite:
\[\begin{aligned}\vec{x}^TH\vec{x} = \vec{x}^TV\Lambda V^T\vec{x} 
&= \vec{x}^T(\vec{v_1}~\vec{v_2})\begin{pmatrix}
\lambda_1 & 0 \\ 0 & \lambda_2	
\end{pmatrix}
\begin{pmatrix}
\vec{v_1}^T\\ \vec{v_2}^T	
\end{pmatrix}\vec{x}\\
&= (\vec{x}^T\cdot\vec{v_1} ~\vec{x}^T\cdot\vec{v_2})
\begin{pmatrix}
\lambda_1 & 0 \\ 0 & \lambda_2	
\end{pmatrix}
\begin{pmatrix}
\vec{v_1}^T\cdot\vec{x}\\ \vec{v_2}^T\cdot\vec{x}	
\end{pmatrix}\\
&= \lambda_1(\vec{x}^T\vec{v_1})^2 + \lambda_2(\vec{x}^T\vec{v_2})^2 > 0
\end{aligned}
\]

Positive definite $\implies \lambda_1,\lambda_2 > 0$:
\[\vec{x}^TH\vec{x} = \lambda_1(\vec{x}^T\vec{v_1})^2 +\lambda_2(\vec{x}^T\vec{v_3})^2  \]
Assume $\vec{x} = \vec{v_1}$, then 
$\begin{cases}
 	\vec{v_1}^T\cdot\vec{v_2} = 0\\
    \vec{v_1}^T\cdot\vec{v_1} = 1
 \end{cases}
 $
from $VV^T = V^TV = I$

\[\vec{v_1}^TH\vec{v_1} = \lambda_1\cdot 1 + \lambda_2 \cdot 0 \text{ and } \vec{v_2}^TH\vec{v_2} = \lambda_2\]
So $\lambda_1,\lambda_2 >0$ is necessary so that $\vec{x}^TH\vec{x} > 0$
\end{proof}

Going back to the character of stationary points: \begin{enumerate}
 \item Minimum: $\delta \vec{x}H(\vec{x}^*)\delta \vec{x} > 0\ \iff \lambda_1,\lambda_2$ of $H(\vec{x}^*) > 0 \iff \tau, \Delta >0$
 \item Maximum: $\delta \vec{x}H(\vec{x}^*)\delta \vec{x} < 0\ \iff \lambda_1,\lambda_2$ of $H(\vec{x}^*) < 0 \iff \tau <0, \Delta >0$
 \item Saddle-Point: $\lambda_1 >0, \lambda_2 < 0 \iff \Delta < 0$
 \item If $\lambda_1$ or $\lambda_2$ or both are zero, we need to go to higher derivatives. 
 \end{enumerate}~



\begin{example}
$u(x,y) = (x-y)(x^2 + y^2 -1)$
\begin{enumerate}
\item Contour lines for $u= 0$:
\[(x-y)(x^2 + y^2 - 1) = 0 \implies\begin{cases}
y = x \\ x^2 + y^2 = 1	
\end{cases}
\]
\item Stationary points: $\dfrac{\del u}{\del x} = \dfrac{\del u}{\del y} = 0$:
\[
\begin{aligned}
&\begin{cases}
\dfrac{\del u}{\del x} = (x^2 + y^2 - 1) + (x-y)2x = 0\\
\dfrac{\del u}{\del y} = -(x^2 + y^2 -1) + (x-y)2y = 0	
\end{cases}\\
&\implies 2(x-y)(x+y) = 0\\
&\implies y^* = \pm x^*
\end{aligned}
\]
\begin{enumerate}
\item $x^* - y^* = 0$
\[2x^{*2} -1 = 0 \implies x^* = \pm \frac{1}{\sqrt{2}} = y^*\]
\[\implies P_1 = \left(\frac{1}{\sqrt{2}},\frac{1}{\sqrt{2}}\right) 
\quad P_2 = \left(-\frac{1}{\sqrt{2}},-\frac{1}{\sqrt{2}}\right)\]

\item $x^* + y^* = 0$
\[-(2x^{*2}-1) -4x^{*2} = 0\implies x^* = -y^* = \pm \frac{1}{\sqrt{6}}\]
\[\implies P_3 = \left(\frac{1}{\sqrt{6}},-\frac{1}{\sqrt{6}}\right) 
\quad P_4 = \left(-\frac{1}{\sqrt{6}},\frac{1}{\sqrt{6}}\right)\]
\end{enumerate}
\item Character of the stationary points:\lecturemarker{26}{5 March}
\[\begin{pmatrix}
\frac{\del^2u}{\del x^2} & \frac{\del^2u}{\del x\del y}\\
\frac{\del^2u}{\del y \del x} & \frac{\del^2u}{\del y^2}	
\end{pmatrix}
= H(\vec{x}) = 
\begin{pmatrix}
6x - 2y & 2y - 2x\\
2y - 2x & 2x - 6y	
\end{pmatrix}
\]

$P_1$: 
$H(P_1) = \begin{pmatrix}
 4\frac{1}{\sqrt{2}}& 0 \\ 0 & -4\frac{1}{\sqrt{2}}	
 \end{pmatrix}
$
\[\begin{rcases*}
\lambda_1 > 0, \lambda_2 < 0 \\
\tau = 0, \Delta < 0	
\end{rcases*}\implies \text{ Saddle-point}
\]

$P_2$: 
$H(P_2) = \begin{pmatrix}
 -4\frac{1}{\sqrt{2}}& 0 \\ 0 & 4\frac{1}{\sqrt{2}}	
 \end{pmatrix}
$
\[\begin{rcases*}
\lambda_1 > 0, \lambda_2 < 0 \\
\tau = 0, \Delta < 0	
\end{rcases*}\implies \text{ Saddle-point}
\]

$P_3$: 
$H(P_3) = \begin{pmatrix}
 \frac{8}{\sqrt{6}}& \frac{-4}{\sqrt{6}} \\ \frac{-4}{\sqrt{6}} & \frac{6}{\sqrt{6}}	
 \end{pmatrix}
$
\[\begin{rcases*}
\tau = \frac{16}{\sqrt{6}}, \Delta = 8
\end{rcases*}\implies \text{ Minimum}
\]

$P_4$: 
$H(P_4) = \begin{pmatrix}
 -\frac{8}{\sqrt{6}}& \frac{4}{\sqrt{6}} \\ \frac{4}{\sqrt{6}} & -\frac{6}{\sqrt{6}}	
 \end{pmatrix}
$
\[\begin{rcases*}
\tau = -\frac{16}{\sqrt{6}}, \Delta = 8
\end{rcases*}\implies \text{ Maximum}
\]


\end{enumerate}
\vspace*{150pt}

Asymptotic's:

$\lim x \to +\infty, y \to -\infty, u(x,y) = +\infty$

$\lim x \to -\infty, y \to +\infty, u(x,y) = -\infty$
\end{example}



\sektion{Vector Calculus}\vspace*{5pt}

\subsektion{Grad, Div and Curl}

Consider $f(\vec{x}): \vec{x} \iR^n, f \iR$\\

\begin{definition}
The \emph{gradient} of $f$ is
\[\vec{\nabla}f = \left(\frac{\del f}{\del x_1}, \frac{\del f}{\del x_2},\dots,\frac{\del f}{\del x_n}\right) = \left(\frac{\del}{\del x_1}, \frac{\del}{\del x_2},\dots,\frac{\del}{\del x_n}\right)f \iR^n\]

The \emph{divergence} of $\vec{g} \iR^n$ is 
\[\vec{\nabla}\cdot \vec{g} = \left(\frac{\del}{\del x_1},\frac{\del}{\del x_2},\dots,\frac{\del}{\del x_n}\right)\cdot(g_1,\dots,g_n) = \frac{\del g_1}{\del x_1} + \frac{\del g_2}{\del x_2} + \dots + \frac{\del g_n}{\del x_n}\]
\end{definition}

So in $\R^2$, the divergence of $\vec{g}$ is the dot product with $\vec{\nabla}$:
\[\vec{\nabla}\cdot\vec{g} = \left(\frac{\del}{\del x}, \frac{\del}{\del y}\right) \cdot(g_1(x,y),g_2(x,y)) = \frac{\del g_1}{\del x} + \frac{\del g_2}{\del y}\]

\begin{definition}
The \emph{curl} (rotational) of $\vec{v}$ is the cross product with $\vec{\nabla}$. 
	
For $\vec{v} = \begin{pmatrix}
 v_x \\ v_y \\ v_z	
 \end{pmatrix}
 = \begin{pmatrix}
 v_x(x,y,z)\\ v_y(x,y,z) \\ v_z(x,y,z)	
 \end{pmatrix}
 \iR^3$, the curl is:\\
 
\[\begin{aligned}
\vec{\nabla} \times \vec{v} &= \begin{vmatrix}
 \hat{i} & \hat{j} & \hat{k}\\
 \dfrac{\del}{\del x} & \dfrac{\del}{\del y} & \dfrac{\del}{\del z}\\
 v_x & v_y & v_z
 \end{vmatrix}\\
	&= \hat{i}\left(\frac{\del v_z}{\del y} - \frac{\del v_y}{\del z}\right)
	+ \hat{j}\left(\frac{\delta v_x}{\del z} - \frac{\del v_z}{\del x}\right)
	+ \hat{k}\left(\frac{\del v_y}{\del x} - \frac{\del v_x}{\del y}\right)
\end{aligned}
\]
\end{definition}~

\begin{example}
	$f(x,y,z) = x^2 + y^2 + z^2, \vec{x} \iR^3, f \iR$
	
	\[
\begin{aligned}
  \vec{\nabla}f &= \left(\frac{\del}{\del x}, \frac{\del}{\del y}, \frac{\del}{\del z}\right)f =  \left(\frac{\del f}{\del x}\frac{\del f}{\del y}, \frac{\del f}{\del z}\right)\\
  &= (2x,2y,2z) = 2x\hat{i} + 2y\hat{j} + 2z\hat{k}
\end{aligned}
\]

\[
\begin{aligned}
  \vec{\nabla}\cdot(\vec{\nabla}f) &= \left(\frac{\del}{\del x}, \frac{\del}{\del y}, \frac{\del}{\del z}\right) \cdot \left(\frac{\del f}{\del x}, \frac{\del f}{\del y}, \frac{\del f}{\del z}\right)\\
  &= 2 + 2 + 2 = 6\\
  &= \underbrace{\frac{\del ^2f}{\del x^2} + \frac{\del^2f}{\del y^2} + \frac{\del^2f}{\del z^2} = \nabla^2f}_{\mbox{Laplacian (next year)}}
\end{aligned}
\]

\[
\begin{aligned}
  \vec{\nabla} \times \vec{\nabla}f &= 
  \begin{vmatrix}
 \hat{i} & \hat{j} & \hat{k}\\
 \dfrac{\del}{\del x} & \dfrac{\del}{\del y} & \dfrac{\del}{\del z}\\
 2x & 2y & 2z
 \end{vmatrix}\\
 &= 0\hat{i} + 0\hat{j} + 0\hat{k} = 0v
\end{aligned}
\]

(This happens in general, to be seen next year)
\end{example}

\subsektion{Directional Derivative}



\lecturemarker{27}{5 March}

\sektion{Optimization}
\subsektion{Lagrange Multipliers}


\sektion{Applications of Integration}

\subsektion{Moments of Inertia}
\subsektion{Final Examples}

\textbf{Felina.} 


  \begin{center}
  \textsf{\textbf{- End of Mathematical Methods II -}}	
  \end{center}
  
\end{document}