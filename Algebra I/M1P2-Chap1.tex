%!TEX root = M1P2.tex

\stepcounter{lecture}
\setcounter{lecture}{1}
\pagebreak
\addcontentsline{toc}{part}{Linear Algebra}{}
\setcounter{page}{4}
\sektion{Vector Spaces}

\subsektion{Spanning Sets and Bases (M1GLA Review)}
\lecturemarker{1}{15 Jan} Let $V$ be a vector space, and let $S = \{v_1, \dots, v_k\}$ be a finite subset of $V$. Then the \emph{span}\index{Span} of $S$ is the set $\{\lambda_1v_1 + \lambda_2v_2 + \dots + \lambda_kv_k, \lambda_1,\dots,\lambda_k \text{ scalar}\}.$

\begin{itemize}
\item[*] Span $S$ is a subspace of $V$.
\item[*] If Span $S = V$, then we say that $S$ is a \emph{spanning set}\index{Spanning Set} for $V$. 
\item[*] If $V$ has a finite spanning set, then we say that $V$ is finite dimensional.
\end{itemize}


Assume from now on that $V$ is finite dimensional. The set $S$ is \emph{linearly independent}\index{Linear Independence} if the only solution to the equation $\lambda_1v_1 + \lambda_2v_2 + \dots + \lambda_kv_k = \mathbf{0}$ is $\lambda_1 = \lambda_2 = \dots = \lambda_k = 0.$\\

A \emph{basis}\index{Basis} for $V$ is a linearly independent spanning set.

\begin{itemize}
\item[*] $V$ has a basis
\item[*] Every basis of $V$ has the same size. This is the \emph{dimension}\index{Dimension} of $V$, written dim $V$. 
\end{itemize}

Suppose that dim $V = n$.
\begin{itemize}
\item[*] Any spanning set of size $n$ is a basis. 
\item[*] Any linearly independent set of size $n$ is a basis.
\item[*] Every spanning set contains a basis as a subset.
\item[*] Every linearly independent set is contained as a subset in a basis.
\item[*] Any subset of $V$ of size $< n$ is \textbf{not} a spanning set.
\item[*] Any subset of $V$ of size $>n$ is \textbf{not} linearly independent. 
\item[*] If $W$ is a subspace of $V$ then dim $W \leq $ dim $V$ eith equality only when $W = V$. 
\end{itemize}

Every vector space has associated with it a set of \emph{scalars}. E.g. $\mathbb{R}^n$ has the scalar set $\mathbb{R}.$ The scalars come as a structure called a \emph{field} (To be defined in the Ring Theory Section). I'll write $F$ for the field of scalars. It will usually be safe to assume that $F = \mathbb{R}$. Other fields include $\mathbb{C}, \mathbb{Q}$, integers and $(\mathbb{Z}\backslash p \mathbb{Z})^{\times}$.
\subsektion{More on Subspaces}

\begin{definition} Let $V$ be a vector space, and let $U$ and $W$ be subspaces of $V$. The \emph{intersection}\index{Intersection} of $U$ and $W$ is $U \cap W = \{v: v \in U \text{ and } v \in W\}$. The \emph{subspace sum}\index{Subspace Sum} (or just \emph{sum}) of $U$ and $W$ is $U + W = \{u + w: u \in U, w \in W\}$
\end{definition}\vspace*{10pt}

\begin{remark}  Note that $\mathbf{0} \in U$ and $\mathbf{0} \in W$, so if $u \in U$, then $u = \mathbf{0} \in U + W$, and similarly if  $w \in W$, then $w = \mathbf{0} \in U + W$. So $U \subseteq U + W$ and $W \subseteq U + W$. ($U+W$ usually contains many other vectors)
\end{remark}\vspace*{10pt}

\begin{example} Let $V \in \mathbb{R}^2.$ Let $U = \text{Span}\{(1,0)\}$, and  $W = \text{Span}\{(0,1)\}$. So $U = \{(\lambda, 0): \lambda \in \mathbb{R}\}, ~W = \{(0,\lambda: \lambda \in \mathbb{R}\}$.

We see that $U + W$ contains $(\lambda, \mu)$ for all $\lambda, \mu \in \mathbb{R}$ and so $U + W = V$.
\end{example}\vspace*{10pt}



\begin{proposition} $U \cap W$ and $U + W$ are both subspaces of $V$. 	
\end{proposition}

\begin{proof}
Do $U+W$ first. Checking the subspace axioms:
\begin{enumerate}
\item  $\mathbf{0} \in U$ and $\mathbf{0} \in W \implies \mathbf{0} = \mathbf{0} + \mathbf{0} \in U + W$. So $U + W \neq \emptyset$. 
\item Suppose $v_1, v_2 \in U + W$. Then $v_1 = u_1 + w_1$ and $v_2 = u_2 + w_2,$ where $u_1,u_2 \in U$ and $w_1, w_2 \in W$.\\

Now $v_1 + v_2 = u_1 + w_1 + u_2 + w_2 = (u_1 + u_2) + (w_1 + w_2) \in U + W \implies$ closed under addition.

\item Let $v \in U + W$. Then $v = u + w$ where $u \in U$ and $w \in W.$ Let $\lambda \in F$. Then $\lambda v = \lambda(u+w) = \lambda u + \lambda w \in U + W$.
\end{enumerate}

Now do $U \cap W$:
\begin{enumerate}
\item $\mathbf{0} \in U$ and $\mathbf{0} \in W$, so $\mathbf{0} \in U \cap W$ by definition.
\item Suppose $v_1, v_2 \in U \cap W.$ Then $v_1, v_2 \in U$ and so $v_1 + v_2 \in U$ since $U$ is closed. Similarly for $v_1 + v_2 \in W$. Hence $v_1 + v_2 \in U \cap W$. 
\item Suppose $v \in U \cap W$, and $\lambda \in F.$ Then $v \in U$ and so $\lambda v \in U$. $v \in W \implies \lambda v \in W.$ Hence $\lambda v \in U \cap W$.\qedhere 
\end{enumerate}
\end{proof}\vspace*{10pt}

\begin{proposition}Let \lecturemarker{2}{5 oct}
 $V$ be a vector space and let $U ~\&~ W$ be subspaces. Suppose that $U = \text{Span}\{u_1,\dots,u_r\}$ and $W = \text{Span}\{w_1,\dots,w_s\}$. Then $U + W = \text{Span}\{u_1,\dots,u_r,w_1,\dots,w_s\}$. 
\end{proposition}
\begin{proof}
By inclusion both ways:

Notice that $u_i \in U \subseteq U + W$ (by Remark 2) and similarly $w_i \in W \subseteq U + W, ~\forall i$. Since $U + W$ is a subspace of $V$, so $U + W$ is closed under linear combinations, so $\text{Span}\{u_1,\dots,u_r,w_1,\dots,w_s\} \subseteq U + W.$\\

For the reverse inclusion, let $v \in U + W$.\\ Then $v = u + w$ for some $u \in U, w \in W$. Since $U = \text{Span}\{u_1,\dots,u_r\}$, we have $u = \lambda_1u_1 + \dots + \lambda_ru_r$, for some $\lambda_1,\dots,\lambda_r \in F$. Similarly $w = \mu_1w_1 + \dots + \mu_sw_s$, for some $\mu_1,\dots,\mu_s \in F$.\\

Now $v = \lambda_1u_1 \dots \lambda_ru_r + \mu_1w_1 \dots \mu_sw_s \in \text{Span}\{u_1,\dots,u_r,w_1,\dots,w_s\}$. Hence $U + W \subseteq \text{Span}\{u_1,\dots,u_r,w_1,\dots,w_s\}$.

\end{proof}

\begin{examples}\item \textbf{Question:} 
 Let $V = \mathbb{R}^4, U = \text{Span}\{(1,1,2,-3),(1,2,0,-3)\}$  and\\ $W = \text{Span}\{(1,0,5,-4),(-1-3,0,5)\}.$ Find a basis for $U + W.$ \\

\textbf{Answer:} By Proposition 5, we have:\\

 $U + W = \text{Span}\{(1,1,2,-3),(1,2,0,-3),(1,0,5,-4),(-1,-3,0,5)\}$.\\ 
 
 We then just row reduce the matrix:
 
 \[\begin{pmatrix}
 1 & 1 & 2 & -3\\
 1 & 2 & 0 & -3\\
 1 & 0 & 5 & -4\\
 -1 & -3 & 0 & 5
 \end{pmatrix} \rightarrow \textit{Echelon Stuff} \rightarrow
 \begin{pmatrix}
 1 & 1 & 2 & -3\\
 0 & 1 &-2 & 0\\
 0 & 0 & 1 & -1\\
 0 & 0 & 0 & 0
 \end{pmatrix}  \]

The three non-zero rows are linearly independent, and have the same span as the original four vectors, so a basis for $U + W$ is:
\[\{(1,1,2,-3),(0,1,-2,0),(0,0,1,-1)\} \text{ (or just the first 3 vectors).}\]

\item \textbf{Question:} What about a basis for $U \cap W$?\\

\textbf{Answer:} If $v \in U \cap W$, then $v \in U = \text{Span}\{(1,1,2,-3),(1,2,0,-3)\}.$ So $v = a(1,1,2,-3) + b(1,2,0,-3)$ for $a,b \in \mathbb{R}$.\\

 And $v \in W = \text{Span}\{(1,0,5,-4),(-1,-3,0,5)\}$. So $v = c(1,0,5,-4) + d(-1,-3,0,5)$. So we have:
\[a(1,1,2,-3) + b(1,2,0,-3) -c(1,0,5,-4) -d(-1,3,0,5) = \mathbf{0} ~(*)\]

$(*)$ gives us 4 simultaneous equations, which we can encode as a matrix equation:
\[
\begin{pmatrix}
1 & 1 & -1 & 1\\
1 & 2 & 0 & -3\\
2 & 0 & -5 & 0\\
-3 & -3 & 4 & -5
\end{pmatrix}
\begin{pmatrix}
a\\b\\c\\d
\end{pmatrix}
=
\mathbf{0} \]

We find the solution space by row reducing:
\[
\begin{pmatrix}
1 & 1 & -1 & 1\\
1 & 2 & 0 & -3\\
2 & 0 & -5 & 0\\
-3 & -3 & 4 & -5
\end{pmatrix} \rightarrow \textit{Echelon Stuff} \rightarrow
 \begin{pmatrix}
 1 & 0 & 0 & -5\\
 0 & 1 & 0 & 4\\
 0 & 0 & 1 & -2\\
 0 & 0 & 0 & 0
 \end{pmatrix}  \]

There is one line of solutions, given by $a = 5d, b = -4d, c = 2d.$ So pick $d = 1$, then $a = 5, b = -4, c = 2, d =1$.\\ 

So $v = a(1,1,2,-3) + b(1,2,0,-3) = (5,5,10,-15) - (4,8,0,-12) = (1,-3,10,3).$\\

We can check our solutions with $c,d$: $v = c(1,0,5,-4) + d(-1,-3,0,5) = (1,-3,10,3)$ as expected. So $U \cap W$ is 1-dimensional, and a basis is $\{(1,-3,10,3)\}$.
\end{examples}


\begin{theorem} Let $V$ be a vector space, and let $U$ and $W$ be subspaces. Then dim $U + W = $ dim $U + $ dim $W - $ dim $U \cap W$.	
\end{theorem}


\begin{proof}
Let dim $U = r, $ dim $W = s, $ dim $U \cap W = m$. Let $\{v_1, \dots, v_m\}$ be a basis for $U \cap W$.\\

Then $\{v_1,\dots,v_m\}$ is a linearly independent subset of $U$. So it is contained in some basis for $U$. So there exists $u_1,\dots, u_{r-m}$ in $U$ such that $\{v_1,\dots,v_m,u_1,\dots,u_{r-m}\}$ is a basis for $U$. Similarly, there exists $w_1,\dots,w_{s-m}$ such that $\{v_1,\dots,v_m,w_1,\dots,w_{s-m}\}$.\\


Now let $B = \{v_1,\dots,v_m,u_1,\dots,u_{r-m},w_1,\dots,w_{s-m}\}$. Then Span $B = U + W$ by Proposition 5.\\

\textbf{Claim.} $B$ is linearly independent.

Proof of claim:

Suppose that $\alpha_1v_1 + \dots \alpha_mv_m + \beta_1u_1 + \dots \beta_{r-m}u_{r-m} + \gamma_1w_1 + \dots \gamma_{s-m}w_{s-m} = \mathbf{0}.$ For $\alpha_i, \beta_i, \gamma_i \in F$. Then:

\[\sum_{i=1}^{s-m} \gamma_iw_i = -\sum_{i = 1}^{m} \alpha_iv_i - \sum_{i=1}^{r-m} \beta_iu_i \in U \]

\[\implies \sum_{i=1}^{s-m} \in U \cap W \implies \sum_{i=1}^{s-m} = \sum_{i=1}^{m} \delta_iv_i, \text{ for some } \delta_i \in F\]

\[\text{Hence:~ }\sum_{i=1}^{m} \delta_iv_i - \sum_{i=1}^{s-m} \gamma_iw_i = \mathbf{0}.\]

But $\{v_1,\dots,v_m,w_1,\dots,w_{s-m}\}$ is linearly independent (being a basis for $W$). So $\gamma_i =\delta_i = 0, \forall i$. Since $\gamma_i = 0 ~\forall i$, we have:

\[\sum_{i=1}^{m} \alpha_iv_i + \sum_{i=1}^{r-m} \beta_iu_i = \mathbf{0}.\]
 
 But $\{v_1,\dots,v_m,u_1,\dots,u_{r-m}\}$ is linearly independent (being a basis for $U$). So $\alpha_i =\beta_i = 0, \forall i$. Hence $B$ is linearly independent. So $B$ is a basis for $U+V$. So dim $U + W = |B| = m + (r-m) + (s-m) = r + s - m$.
\end{proof}\vspace*{10pt}
 
\begin{example} \textbf{Question:} \lecturemarker{3}{20/01} 
 Let $V = \mathbb{R}^3$, and let $U= \{(x,y,z) \in \mathbb{R}^3: x + y + z = 0\}.$ Similarly let $W = \{(x,y,z) \in \mathbb{R}^3: -x + 2y + z = 0\}.$ Find bases for $U, W, U \cap W, U + W$. \\

\textbf{Answer:} A general element of $U$ is $(x,y,-x-y) = x(1,0,-1) + y(0,1,-1) \implies U = \text{Span}\{(1,0,-1),(0,1,-1)\}$. Since this set is clearly linearly independent, it's a basis for $U$.\\

A general element of $W$ is $(x,y,x-2y) = x(1,0,1) + y(0,1,-2) \implies W = \text{Span}\{(1,0,1),(0,1,-2)\}$. Again, clearly linearly independent, so a basis for $W$.\\

Suppose that $v = (x,y,z)$ lies in $U \cap W$. Then $v \in U$, and so $z = -x-y$. But also $v \in W$, and so $z = x - 2y$. Hence $-x-y = x-2y \implies y = 2x$.\\

So a general element of $U \cap W$ is $(x,2x,-3x) = x(1,2,-3) \implies U\cap W = \{(1,2,-3)\} $ is a basis for $U\cap W$.\\


As in the proof of theorem 7, we find a basis for $U$, and $W$, each of which contain a basis for $U \cap W$. So any linearly independent subset of $U$ of size 2 is a basis for $U$, i.e. $U = \{(1,2,-3),(1,0,-1)\}$. Similarly $W = \{(1,2,-3),(1,0,1)\} $. So a spanning set for $U + W = \{(1,2,-3),(1,0,-1),(1,0,1)\}$.\\

By Theorem 7, we know dim $U+W = $ dim $U + $ dim $W - $ dim $U\cap W = 2 + 2 -1 = 3.$ Hence our spanning set is a basis for $U + W$.
\end{example}~


\subsektion{Rank of a Matrix}~

\begin{definition}Let $A$ be an $m \times n$ matrix with entries from $F$. Define the \emph{row-span}\index{Row-Span} of $A$ by $\text{RSp}(A) = \text{Span}\{\text{rows of }A\}$. This is a subspace of $F^n$. The \emph{column-span}, $\text{CSp}(A)$ of $A$ is  $\text{Span}\{\text{columns of }A\}$, again a subspace of $F^n$.	

The \emph{row-rank}\index{Row-Rank} of $A$ is dim $\text{RSp}(A)$. The \emph{column-rank} of $A$ is dim $\text{CSp}(A)$ i.e. the number of linearly independent rows / columns.
\end{definition}


\begin{example}
$A = \begin{pmatrix}
 3 & 1 & 2\\
 0 & -1 & 1
 \end{pmatrix},~ F = \mathbb{R}$\\
 
 $\text{RSp}(A) = \text{Span}\{(3,1,2),(0,-1,1)\}$. Since the two vectors are linearly independent, we have that the row-rank = dim $\text{RSp}(A) = 2$.\\
 
 $\text{CSp}(A) = \text{Span}\left\{
 \begin{pmatrix}
 3\\0 
 \end{pmatrix}, 
 \begin{pmatrix}
 1 \\ -1
 \end{pmatrix},
 \begin{pmatrix}
 2\\1
 \end{pmatrix}
\right\}$. \\

This is linearly dependent, since $
\begin{pmatrix}
3\\0
\end{pmatrix}
= 
\begin{pmatrix}
1\\-1
\end{pmatrix}
+ \begin{pmatrix}
 2\\1
 \end{pmatrix}
$.\\~\\

So $\text{CSp}(A) = \text{Span}\left\{
\begin{pmatrix}
 1 \\ -1
 \end{pmatrix},
 \begin{pmatrix}
 2\\1
 \end{pmatrix}
\right\} \implies \text{CSp}(A) = 2.$
\end{example}



\textit{How do we calculate the row-rank of a matrix $A$?}\\


\begin{proc}~\\

\noindent \textbf{Step 1:} Use row operations to reduce the matrix $A$, to row echelon form.

\[A_{ech} =\left(
    \begin{array}{ccccc}
    1    &       &    &     & \\ \cline{1-1}
    \bord & 1       &    &  \scalebox{1.5}{*}   & \\ \cline{2-2}
          & \bord    & 1     &    & \\ \cline{3-3}
       ~\scalebox{1.5}{0}   &  & \bord & 1     &  \\ \cline{4-5}
          &          &       &  &  \\ %\cline{5-5}
  \end{array}\right)\]

\noindent \textbf{Step 2:} The row-rank of $A$ is the number of non-zero rows in $A_{ech}$, and the non-zero rows of $A_{ech}$ form a basis for $\text{RSp}(A).$ [i.e. you don't need to go back to original vectors get the basis.]
\end{proc}

\textbf{Justification} We need to show:
\begin{enumerate}
\item $\text{RSp}(A) = \text{RSp}(A_{ech})$
\item The non-zero rows of $A_{ech}$ are linearly independent. 
\end{enumerate}

To show (1), recall that $A_{ech}$ is obtained from $A$ by a series of row-operations:
\[
\begin{cases}
r_i := r_i + \lambda r_j & (i\neq j)\\
r_i := \lambda r_i & (\lambda \neq 0)\\
r_i \longleftrightarrow r_j & (i \neq j)
\end{cases} \]

Suppose that $A'$ is obtained from $A$ by one row-operation. Then it is clear than every row of $A'$ lies in $\text{RSp}(A)$. So $\text{RSp}(A') \subseteq \text{RSp}(A)$. But every row operation is invertible by another row operation. i.e. 

\[\begin{cases}
r_i := r_i + \lambda r_j & \text{has inverse ~} r_i := r_i - \lambda r_j \\
r_i := \lambda r_i & \text{has inverse ~} r_i := r_i - 1/\lambda r_j \\
r_i \longleftrightarrow r_j & \text{has inverse ~}  r_i \longleftrightarrow r_j
\end{cases} \]

So we have $\text{RSp}(A) \subseteq \text{RSp}(A')$, and so the row spaces are equal. It follows that $\text{RSp}(A_{ech}) = \text{RSp}(A)$.\\

For (2), consider the form of $A_{ech},$ and denote $r_1,\dots r_k$ as the non-zero rows. Say $r_i$ has it's leading entry in column $c_i$. Suppose that $\lambda_1r_1 + \dots + \lambda_kr_k = \mathbf{0} ~(*).$\\
  
  Look at the co-ordinate corresponding to column $c_1$. The only contribution is $\lambda_1r_1$ since all of the other rows have 0 in that co-ordinate. Since $r_1$ has 1 in this co-ordinate $\implies \lambda_1 = 0$.\\

Since $\lambda_1 = 0$, the only contribution to $c_2$ is $\lambda_2r_2$. So $\lambda_2 = 0$. We can continue this argument for $c_2, \dots, c_i, \dots, c_k \implies$ the only solution to $(*)$ is $\lambda_1, \dots, \lambda_k = 0 \implies $ linearly independence of vectors.
  

\begin{example} \lecturemarker{4}{21 Jan} 
Find the row-rank of $A = \begin{pmatrix}
 1 & 2 & 5\\ 2 & 1 & 0 \\ -1& 4 & 15
 \end{pmatrix}
$ \\~\\

\noindent \textbf{Answer:} Reduce $A$ to echelon form: 

\[A  = \begin{pmatrix}
 1 & 2 & 5\\ 2 & 1 & 0 \\ -1& 4 & 15
 \end{pmatrix} \to \begin{pmatrix}
1 & 2 & 5 \\ 0 & 1 & 10/3 \\ 0 & 0 & 0
\end{pmatrix} = A_{ech}\]

Since $A_{ech}$ has two non-zero rows, the row-rank of $A$ is 2. (Note: Scaling the second row to make the leading entry 1 was not necessary)
\end{example}\vspace*{10pt}

\begin{example} Find the dimension of $W = \text{Span}\{(-1,1,0,1),(2,3,1,0),$\\$(0,1,2,3)\} \subseteq \mathbb{R}^4$

 \textbf{Answer:} Notice that $W = \text{RSp} \begin{pmatrix}
 -1 & 1 & 0 & 1\\ 2 & 3 & 1 & 0\\ 0 & 1 & 2 & 3
 \end{pmatrix} = A$\\
 
 $\implies A_{ech} =  
 \begin{pmatrix}
 -1 & 1 & 0 & 1 \\ 0 & 5 & 1 & 2 \\ 0 & 0 & 9 & 12
 \end{pmatrix}
 $\\
 
There are 3 non-zero rows in $A_{ech}$, so the row rank is 3.\\ Hence dim $W = 3$.
\end{example}\vspace*{10pt}

\begin{proc}The columns of $A$ are the rows of $A^T$. So apply Procedure 11 to the matrix $A^T$. (Alternatively, use column operations to reduce $A$ to ``column echelon form'', and then count the no. of non-zero columns).\end{proc}

\begin{example} Let $A = \begin{pmatrix}
 1 & 2 & 5\\ 2 & 1 & 0\\ -1 &4 & 15
 \end{pmatrix}
 $\\
 
 The column rank of $A$ is the row-rank of $A^T$.
 
 \[A^T = \begin{pmatrix}
 1 & 2 & -1\\ 2 & 1 & 4\\ 5 & 0 & 15
 \end{pmatrix}
 \to 
 \begin{pmatrix}
 1 & 2 & -1\\ 0 & -3 & 6\\ 0 & 0 & 0
 \end{pmatrix}\]
 
 There are two non-zero rows, so row-rank($A^T$) = 2, and so column-rank $(A)$ = 2. A basis for $\text{RSp}(A^T)$ is $\{(1,2,-1),(0,-3,6)\}$, so a basis for $\text{CSp}(A)$ is the transpose of these vectors.
 \end{example}
 
\begin{theorem} For any matrix $A$, the row-rank of $A$ is equal to the column-rank of $A$.\end{theorem}
 
 \begin{proof}
 Let the rows of $A$ be $r_1, \dots, r_m$, so $r_i = (a_{i1}, a_{i2},\dots,a_{in})$.\\
 
 Let the columns of $A$ be $c_1,\dots,c_n$, so $c_j = (a_{1j}, a_{2j},\dots,a_{mj})^T$.\\
 
 Let $k$ be row-rank of $A$. Then $\text{RSp}(A)$ has basis $\{v_1,\dots,v_k\}, v_i \in F^n$. Every row $r_i$ is a linear combination of $v_1,\dots,v_k$. Say that:
 \[r_i = \lambda_{i1}v_1 + \lambda_{i2}v_2 + \dots + \lambda_{il}v_k ~(*)\]
 
 Let $v_i = (b_{i1}, \dots, b_{in})$. Looking at the $j$th co-ordinate in $(*)$, we have $a_{ij} = \lambda_{i1}b_{1j} + \lambda_{i2}b{2j} + \dots + \lambda_{ik}b_{kj}$
 
 \[c_j = \begin{pmatrix}
 a_{1j}\\a_{2j}\\ ... \\ a_{mj}
\end{pmatrix}
= 
\begin{pmatrix}
\lambda_{11}b_{1j} + \lambda_{12}b_{2j} + \dots + \lambda_{1k}b_{kj}\\
\lambda_{21}b_{2j} + \lambda_{22}b_{2j} + \dots + \lambda_{2k}b_{kj}\\
\dots \\
\lambda_{m1}b_{1j} + \lambda_{m2}b_{2j} + \dots + \lambda_{mk}b_{kj}\\
\end{pmatrix}\]~

So every column of $A$ is a linear combination of $(\lambda_{1i},\lambda_{2i},\dots,\lambda_{mi})^T$ for $1 \leq i \leq k$. So the column space of $A$ is spanned by $k$ vectors and so column-rank$(A) \leq k = $ row-rank$(A)$.\\

 But row-rank$(A$) = column-rank($A^T)$, column-rank($A)$ = row-rank($A^T$).\\
 
  By the argument above, column-rank$(A^T) \leq$ row-rank($A$). So row-rank$(A) \leq$ column-rank($A$). Hence row-rank$(A)$ = column-rank$(A$).
 \end{proof}

 
\begin{definition} Let $A$ be matrix. The \emph{rank}\index{Rank} of $A$, written $rk(A)$, is the row-rank of $A$. (which is also the column-rank of $(A$).)	
\end{definition}\vspace*{10pt}


\begin{example}Let $A = \left(\begin{smallmatrix}
 1 & 2 & -1 & 0\\ -1 & 1 & 0 & 1\\ 0 & 3 & -1 & 1
 \end{smallmatrix}\right)$\\
 
 Notice that $r_3 = r_1 + r_2$. So a basis for $RSp(A)$ is $\{(1,2,-1,0),(-1,1,0,1)\}$\\
 
 Write the rows of $A$ as linear combinations of $\{v_1,v_2\}$:
 \[r_1 = 1v_1 + 0v_2~~~ r_2 = 0v_1 + 1v_2~~~ r_3 = 1v_1 + 1v_2\] (The scalars here are the $\lambda_{ij}$ from the proof of Theorem 16)\\
 
 According to the proof, a spanning set of for $CSp(A)$ is given by: \[\{(1,0,1)^T,(0,1,1)^T\}\] We verify this - 
 We have $c_1 = (1,-1,0)^T = w_1 - w_2$. $c_2 = (2,1,3)^T = 2w_1 + w_2$, $c_3 = (-1,0,-1)^T = -w_1$, and $c_4 = (0,1,1)^T = w_2$.
 \end{example}\vspace*{10pt}

\begin{proposition}\lecturemarker{5}{23/01} 
 Let $A$ be an $n \times n$ (square) matrix. Then the following statements are equivalent:
\begin{enumerate}
\item $rk(A) = n$ (A has ``full rank'')
\item The rows of $A$ form a basis for $F^n$
\item The columns of $A$ form a basis of $F^n$
\item A is invertible (so det$A \neq 0$, row reduced to $I$ etc.)
\end{enumerate}
\end{proposition}

\begin{proof}
(1) $\iff$ (2):\\
\[\begin{aligned}
\text{rk}(A) = n & \iff \text{dim RSp}(A) = n\\
&\iff \text{RSp}(A) = F^n\\
& \iff \text{Rows of } A \text{ are spanning a set for } F^n \text{ of size } n\\
& \iff \text{The rows of } A \text{ form a basis for }F^n
\end{aligned}
\]

(1) $\iff$ (3): The same, using columns instead.\\

(1) $\iff$ (4): 
\[
\begin{aligned}
\text{rk}(A) = n &\iff A_{ech} = I\\
&\iff A \text{ can be row-reduced to } I\\
&\iff A \text{ is invertible.}
\end{aligned}\]
\end{proof}

\subsektion{Linear Transformations}
Suppose that $V$ and $W$ are vector spaces over a field $F$. Let $T: V \to W$ be a function.

\begin{itemize}
\item[*] Say that $T$ ``preserves addition'', if whenever $T: V \mapsto W$ and $T: v_2 \mapsto w_2$, we also have $T: v_1 + v_2 \mapsto w_1 + w_2$. (Briefly: $T(v_1 + v_2)=Tv_1 +Tv_2$.)
\item[*] Say that $T$ ``preserves scalar multiplcation'', if whenever $T: v \mapsto w$ and $\lambda \in F \implies T: \lambda v \mapsto \lambda w$. (Briefly: $T(\lambda v) = \lambda T(v))$
\end{itemize}

\begin{definition}The function $T: V \to W$ is a \emph{linear transformation}\index{Linear Transformation} (or \emph{linear map}), if it preserves addition and scalar multiplication. So:
\[T(v_1 + v_2)=Tv_1 +Tv_2 \text{ and }T(\lambda v) = \lambda(T(v)), \forall v_1,v_2,v \in V ~\&~ \lambda \in F\]
\end{definition}\vspace*{10pt}


\begin{examples}
\begin{itemize}
\item[(a)] $T: \mathbb{R}^2 \to \mathbb{R}, T(x,y) = x+y$. I claim this is a linear transformation.\\
Check it preserves addition: \[
\begin{aligned}
T( (x_1, y_1) + (x_2,y_2) ) &= T ((x_1 + x_2, y_1  + y_2))\\
&= x_1 + x_2 + y_1 + y_2 \\
&= x_1 + y_1 + x_2 + y_2 \\
&= T((x_1,y_1)) + T((x_2,y_2))
\end{aligned}\]

And T also preserves scalar multiplicaton, since if $\l \in \mathbb{R}$, then:
\[
T(\l(x,y)) = T((\l x,\l y)) = \l x + \l y = \l(x+y) = \l T((x,y))\]


\item[(b)] $T: \mathbb{R}^2 \to \mathbb{R}, T(x,y) = x+y + 1$. This is not linear. \\
For example, 2$T$((1,0)) = 4, but $T((2,0)) = 3$, so it doesn't preserve scalar multiplication $\implies$ not a linear map.

\item[(c)] $T: \mathbb{R} \to \mathbb{R}$ $T(x) = \sin(x)$, is not linear.\\
$2T(\pi/2) = 2$, but $T(2\times \pi/2) = T(\pi) = 0$. Again it doesn't preserve scalar multiplication, so not a linear map.

\item[(d)] Let $V$ be the space of all polynomials in a single variable $x$ with co-efficients from $\mathbb{R}$. Define $T:v \to V$ by $T(f(x)) = \frac{d}{dx}f(x)$. Then $T$ is a linear transformation.
\begin{itemize}
\item[(I)] $\frac{d}{dx}(f(x) + g(x)) = \frac{d}{dx}f(x) + \frac{d}{dx}g(x) \implies$ preserves addition
\item[(II)] $\frac{d}{dx}(\l f(x)) = \l\frac{d}{dx}f(x) \implies $ preserves scalar multiplication
\end{itemize}

\end{itemize}
\end{examples}\vspace*{10pt}

\begin{proposition} Let $A$ be an $m\times n$ matrix over $F$. Define $T: F^n \to F^m$ by $T(b) = Av$. This is a linear transformation. (We say that $T$ is a \emph{matrix transformation}\index{Matrix Transformation}.)	
\end{proposition}


\begin{proof}
\[
\begin{aligned}
1.~~ T(v_1 + v_2) &= A(v_1 + v_2)\\
&= Av_1 + Av_2 \\
&= Tv_1 + Tv_2~~ \forall v_1,v_2 \in F^n \implies T \text{ preserves addition.}
\end{aligned}\]
\[\begin{aligned}
2.~~ T(\l v) &= A(\l v) = \l A v \\
&= \l Tv ~~ \forall v \in F^n, \l \in F \implies T \text{ preserves scalar multiplication.}\qedhere
\end{aligned}
\]
\end{proof}

\begin{examples}
\begin{itemize}
\item[(a)] 
Define a map $T: \mathbb{R}^3 \to \mathbb{R}^2$ by:
\[
T\Spvek{a_1;a_2;a_3} = \Spvek{a_1 - 2a_2 + a_3; a_1 + a_2 - 2a_3}
\]
Then $T$ is linear because:
\[
T\Spvek{a_1;a_2;a_3} = \begin{pmatrix}
 1 & -3 & 1 \\ 1 & 1 & -2
 \end{pmatrix}
\Spvek{a_1;a_2;a_3}
\]
and so $T$ is a matrix transformation. So Proposition 22 applies.

\item[(b)] Define $\rho_{\theta}: \mathbb{R}^2 \to \mathbb{R}^2$ to be a rotation through an angle of $\theta$ (aniticlockwise). Then $\rho_{\theta}$ is linear since it is given by the matrix:\[
\begin{pmatrix}
\cos\theta & -\sin\theta \\ 
\sin\theta & \cos\theta
\end{pmatrix} \]

\end{itemize}
\end{examples}\vspace*{10pt}


\begin{proposition}\emph{(Basic properties of linear transformations)} \\

Let $T: V \to W$ be a linear transformation.

\begin{enumerate}
\item[(i)] If $\mathbf{0}_v$ is the zero vector in $V$ and $\mathbf{0}_w$ is the zero vector in $w$, then $T(\mathbf{0}_v) = \mathbf{0}_w$
\item[(ii)] Suppose that $v_1, \dots, v_k \in V$ and that $v = \l_1v_1 + \dots \l_kv_k ~(\l_i \in F)$. Then $Tv = \l_1Tv_1 + \l_2Tv_2 + \dots + \l_kTv_k$.
\end{enumerate}
\end{proposition}


\begin{proof} (i). Since $T$ preserves scalar multiplication, for any $v \in V$, we have $T(0v) = 0Tv$, so $T(\mathbf{0}_v) = \mathbf{0}_w$.

\end{proof}


\begin{proof} 
(ii)
We observe:
\lecturemarker{6}{27/01} 
\[
\begin{aligned}
T(v) &= T(\lambda_1v_1 + \dots \lambda_kv_k)\\
&= T(\lambda_1v_1 + \dots \lambda_{k-1}v_{k-1}) + T(\lambda_kv_k)\\
&= T(\lambda_1v_1 + \dots \lambda_{k-1}v_{k-1}) + \lambda_kv_k\\
\end{aligned}\]

Now a straightforward argument by induction tells us that: \[T(v) = \lambda_1T(v_1) + \lambda_2T(v_2) + \dots \lambda_kT(v_k) \qedhere\]
\end{proof}

\begin{example} \textbf{Question:} Find a linear transformation, $T: \mathbb{R}^2 \to \mathbb{R}^3$, which sends $(1,0) \to (1,-1,2)$ and sends $(0,1) \to (0,1,3)$.\\

\textbf{Answer:} Notice that $\{(1,0), (0,1)\}$ is a basis for $\mathbb{R}^2$. So a general element of $\mathbb{R}^2$ is $a(1,0) + b(0,1)$, for $a,b \in \mathbb{R}$. \\

So if $T$ is a solution to the question, then we must have $T(a(1,0) + b(0,1)) = a(1,-1,2) + b(0,1,3)$, by Proposition 24(ii). \\

So we are \emph{forced} to take $T(a,b) = (a,b-a,2a+3b)$. This is indeed a linear transformation, since it is a matrix transformation. And we do have $T(0,1) = (1,-1,2)$ and $T(0,1) = (0,1,3)$ as required. So this is a solution and it is the unique solution.
\end{example}\vspace*{10pt}


\begin{proposition}Let $V$ and $W$ be vector spaces over $F$. Let $\{v_1,\dots,v_n\}$ be a basis for $V$. Let $\{w_1,\dots,w_n\}$ be any $n$ vectors in $W$. Then there is a unique linear transformation $T: V \to W$ such that $T(v_i) = w_i ~\forall i$.	
\end{proposition}\vspace*{10pt}


\textit{Remark.} The vectors $w_1, \dots w_n$ don't have to be linearly independent, or even distinct.
\begin{proof}
Suppose that $v \in V.$ Then there exists unique scalars $\lambda_1,\dots,\lambda_n$ such that $v = \l_1v_1 + \dots + \l_nv_n$.\\

Define $T: V \to W$ by $T(v) = \l_1w_1 + \dots \l_nw_n$. (This makes sense, since the scalars $\l_i$ are uniquely determined by $v$.)\\

Show that $T$ is linear:\\
Take $u,v \in V$. Write $u = \mu_1v_1 + \dots + \mu_nv_n$, $v = \l_1v_1 + \dots \l_nv_n$. Then $u+v = (\mu_1 + \l_1)v_1 + \dots + (\mu_n + \l_n)v_n$. Now by the definition of $T$, we have:

\[T(u) = \mu_1w_1 + \dots + \mu_nw_n \text{ and }T(v) = \l_1w_1 + \dots + \l_nw_n\]
\[\text{Also }T(u+v) = (\mu_1 + \l_1)w_1 + \dots + (\mu_n + \l_n)w_n\]

So $T(u+v) = T(u) + T(v)$, so $T$ preserves addition.\\

Now let $\pi \in F$. We have $\pi = \pi\l_1v_1 + \dots + \pi\l_nv_n$
\end{proof}

\begin{remark} Once we know what a linear transformation does on a basis, we know all about it. This gives a convenient shorter way of defining a linear transformation. 	
\end{remark}


\begin{example} Let $V$ be the vector space of polynomials in a variable $x$ over $\mathbb{R}$ of degree $\leq 2$. A basis for $V$ is $\{1,x,x^2\}$.\\

Pick three ``vectors'' in $V$; $w_1 = 1+x$, $w_2 = x-x^2$, $w_3 = 1+x^2$. By Proposition 26, there should be a unique linear transformation, $T: V\to V$ such that $T(1) = w_1, T(x) = w_2, T(x^2) = w_3$.\\

Let's work out what $T$ does to an arbitrary polynomial, $c + bx + ax^2$ from $V$. We must have $c + bx + ax^2 \longmapsto c(1+x) + b(x-x^2) + a(1+x^2) = (a+c) + (b+c)x + (a-b)x^2$.
\end{example}

  
 \subsektion{Kernels and Images} 
 
\begin{definition}  \lecturemarker{7}{28/01} 
 Let $T: V \to W$ be a linear transformation. The \emph{image}\index{Image} of $T$ is the set $\{Tv : v\in V\} \subseteq W$. The \emph{kernel}\index{Kernel} of $T$ is the set of $\{ v \in V: Tv = \mathbf{0} \}$. We write Im$(T)$ for the image, and Ker$(T)$ for the kernel. 
 \end{definition}

\begin{example} Let $T : \mathbb{R}^3 \to \mathbb{R}^2$ be defined by:
\[ 
T\begin{pmatrix}
x_1 \\
x_2 \\
x_3 \end{pmatrix}
= 
\begin{pmatrix}
3 & 1& 2\\
-1 & 0 & 1\end{pmatrix}
\begin{pmatrix}
x_1 \\
x_2 \\
x_3 \end{pmatrix}
\] 

So: 
 \[ 
T\begin{pmatrix}
x_1 \\
x_2 \\
x_3 \end{pmatrix} 
= 
\begin{pmatrix}
3x_1 + x_2 + 2x_3\\
-x_1 + x_3\end{pmatrix} 
\] 

So Im$(T)$ is the set:
 \[ 
 \begin{aligned}
&~~ \left\{  \begin{pmatrix}
3x_1 + x_2 + 2x_3\\
-x_1 + x_3\end{pmatrix} 
: x_1, x_2, x_3 \in \mathbb{R}
\right\} \\
&= \left\{
x_1 \begin{pmatrix}3\\-1\end{pmatrix} + 
x_2 \begin{pmatrix}1\\0\end{pmatrix}+
x_3\begin{pmatrix}2\\1\end{pmatrix}
: x_1, x_2, x_3 \in \mathbb{R}
\right\} \\
&= CSp(A)
\end{aligned}
\] \vspace*{5pt}


The kernel of $T$ is the set: 
\[
\left\{
\left(\begin{smallmatrix}x_1\\x_2\\x_3\end{smallmatrix}\right) \in \mathbb{R}^3: 
3x_1 + x_2 + 2x_3 = 0,~ -x_1 + x_3 = 0
\right\}\]

This is $\{v \in \mathbb{R}^3: Av = \mathbf{0}\}$, the solution space of $Av = \mathbf{0}$. (Solved this in M1GLA). (In this case the kernel is the span $\{ (1,-5,1)^T \}$)
\end{example}\vspace*{10pt}

\begin{proposition} Let $T : V \to W$ be a linear transformation. Then:
\begin{enumerate}
\item Im$(T)$ is a subspace of $W$
\item Ker$(T)$ is a subspace of $V$
\end{enumerate}
\end{proposition}

(In general we write $U \leq V$ to mean that $U$ is a subspace of $V$. So Proposition 31 says Im$(T) \leq W, $ Ker$(T) \leq V$). 

\begin{proof}
(i) Im$(T) \leq W$: \\

Certainly $T(\mathbf{0}) \in $ Im$(T)$, so Im$(T) \neq \emptyset$.\\

Suppose that $w_1, w_2 \in $ Im$(T)$. Then there exists $v_1, v_2 \in V$ such that $Tv_1 = w_1$ and $Tv_2 = w_2$. Now $T(v_1 + v_2) = Tv_1 + Tv_2 = w_1 + w_2$. (Since $T$ preserves addition). So $w_1 + w_2 \in Im ~T$, so Im$(T)$ is closed under addition. \\

Now suppose that $w \in $ Im$T$ and $\lambda \in F$. Then there exists $v \in V$ such that $Tv = w$. Now $T(\lambda v) = \lambda T(v) = \lambda w$. (Since $T$ preserves scalar multiplication). So $\lambda w \in $ Im$(T),$ so Im$(T)$ is closed under scalar multiplication.\\

\noindent (ii) Ker$(T) \leq V$:\\

We know that $T(\mathbf{0}) = \mathbf{0}w.$ So $\mathbf{0}v \in Ker~ T \implies Ker~ T \neq \emptyset.$\\

Suppose that $v_1, v_2 \in $ Ker$(T)$. So: $Tv_1 = Tv_2 = \mathbf{0}.$ Now $T(v_1 + v_2) = \mathbf{0} + \mathbf{0} = \mathbf{0}$ (Since $T$ preserves addition). So $v_1 + v_2 \in $ Ker$(T)$, and so Ker$(T)$ is closed under addition.\\

Now suppose we have $v \in \text{Ker}(T), \lambda \in F.$ Then $Tv = \mathbf{0}.$ Now $T(\lambda v) = \lambda Tv = \lambda \mathbf{0} = \mathbf{0}.$ So $\lambda v \in $ Ker$(T).$ So Ker$(T)$ is closed under scalar muliplication. 
\end{proof}\vspace*{5pt}

\begin{example} Let $V_n$ be the vector space of polynomials in a variable $x$ over $\mathbb{R}$ of degree $\leq n$.\\

We have $V_0 \leq V_1 \leq V_2 \leq \dots$\\

Define $T: V_n \to V_{n-1}$ by $T(f(x)) = \dfrac{d}{dx} f(x)$. \\

We have Ker$(T) = \{\text{constant polynomials} \} = V_0$.\\

If $g(x) = V_{n-1}$, Let $f(x)$ be the antiderivative (integral) of $g(x)$. Since deg $g(x) \leq n-1$, we have deg $f(x) \leq n$. And $T(f(x)) = \frac{d}{dx}f(x) = g(x)$. \\

So $g(x) \in $ Im$(T)$, and so Im$(T) = V_{n-1}$.
\end{example}\vspace*{10pt}

\begin{proposition} Let $T : V \to W$ be a linear transformation. Let $v_1, v_2 \in V$. Then $Tv_1 = Tv_2 \iff T(v_1 -v_2) = \mathbf{0}$. $v_1 - v_2 \in $ Ker$(T)$.	
\end{proposition}


\begin{proof}
$Tv_1 = Tv_2 \iff Tv_1 - Tv_2 = 0 \iff T(v_1 - v_2) = 0$\\ (Since T preserves addition and multiplication by $-1$). 
\end{proof} \vspace{10pt}

\begin{proposition} Let $T: V \to W$ be a linear transformation. Suppose that $\{v_1, \dots, v_n\}$ is a basis for $V$. Then Im$(T) = \text{Span}\{Tv_1, \dots, Tv_n\}$.	
\end{proposition}


\begin{proof}
Let $w \in $ Im$(T)$. Then there exists $v \in V$ such that $T(v) = w$. We can write $v$ as a linear combination of basis vectors:\[v = \lambda_1v_1 + \lambda_2v_2 + \dots \lambda_nv_n. \]

Now $Tv_1 = \lambda_1 Tv_1 + \dots \lambda_n Tv_n$ by Proposition 24(ii). So $w = \lambda_1Tv_1 + \dots + \lambda_nTv_n \in \text{Span}\{Tv_1, \dots, Tv_n\} \geq $ Im$(T)$\\
 
 Since $Tv_i \in $ Im$(T)$ for all $i$, $\text{Span}\{Tv_1, \dots Tv_n\} \leq $ Im$(T)$. So Im$(T) = \text{Span}\{Tv_1, \dots, Tv_n\}.$
 \end{proof}
\vspace{10pt}

 
\begin{proposition} \lecturemarker{8}{30/01} Let $A$ be an $m \times n$ matrix. Let $T: \mathbb{R}^n \to \mathbb{R}^m$ be given by $Tx = Ax$. \begin{enumerate}
 \item[(i)] Ker$(T)$ is the solution space to the equation $Ax = \mathbf{0}$. 
 \item[(ii)] Im$(T)$ is the column space $\text{CSp}(A)$.
 \item[(iii)] dim Im$(T)$ is the rank $\text{rk}(A)$.
 \end{enumerate}
 
 (Compare with Example 30).
  \end{proposition}

 
 \begin{proof}
 (i): This is immediate from the definitions. \\
 
 (ii): Take the standard basis $e_1,\dots,e_n$ for $\mathbb{R}^n$, that is $e_i = (0,0,\dots,1,0,0)^T$, where the 1 is in the $i$th position. $T(e_i) = A(0,0,\dots,1,0,0)^T = c_i$, the $i$th column of $A$. By Proposition 34, Im$(T) = \text{Span}\{T_{e_1},\dots,T_{e_n}\} = \text{Span}\{c_1,\dots,c_n\} = \text{CSp}(A)$.\\
 
 (iii): By (ii), dim Im$(T) = $ dim $\text{CSp}(A) =$ column-rank = $\text{rk}(A)$
 \end{proof}\vspace*{10pt}

\begin{example} Define $T: \mathbb{R}^3 \to \mathbb{R}^3$ by:

\[Tx = \begin{pmatrix}
 1 & 2 & 3\\ -1 & 0 & 1\\ 1 & 4 & 7
 \end{pmatrix}x 
 \quad
 x = \begin{pmatrix}
 x_1\\x_2\\x_3
 \end{pmatrix}\]
 
 \textbf{Question:} Find a basis for Ker$(T)$ and Im$(T)$.\\
 
 \textbf{Answer:} To find Ker$(T)$, we solve $Ax = \mathbf{0}$.
 
 \[\begin{amatrix}{3}
 1 & 2 & 3 & 0 \\ -1 & 0 & 1 &  0\\ 1 & 4 & 7 & 0
 \end{amatrix}
 \to 
\begin{amatrix}{3}
1 & 0 & -1 & 0\\ 0 & 1 & 2 & 0\\ 0 & 0 & 0 & 0\\
\end{amatrix}\]

$\implies x_1 - x_3 = 0; x_2 + 2x_3 = 0$. So we must have $x_1 = x_3, x_2 = -2x_3$\\

So a basis for Ker$(T)$ is $\{(1,-2,1)^T\}$\\

\noindent For Im$(T)$, we notice from the row-reduced matrix above, that $rk(A) = 2$. So dim Im$(T)$ is 2. Since Im$(T)$ is $CSp(A)$, a basis can be obtained by taking any two lin. indep. columns. So $\{(1,-1,1)^T,(2,0,4)^T\}$ is a basis.
\end{example}\vspace*{10pt}

\begin{theorem}[Rank Nullity Theorem.]\index{Rank Nullity Theorem|idxbf} Let $T: V \to W$ be a linear transformation. Then dim Im$(T) + $ dim Ker$(T) = $ dim$(V)$.
 (rank $T$ = dim Im$(T),$ nullity $ = $ dim Ker$(T)$)
 \end{theorem}

\begin{proof}
Let $\{u_1,\dots,u_s\}$ be a basis for Ker$(T)$, and $\{w_1,\dots,w_r\}$ be a basis for Im$(T)$. For each $i \in \{1,\dots,r\}$, there exists some $v_i \in V$ such that $T(v_i) = w_i$ (since $w_i \in $ Im$(T)$).\\

I claim that $B = \{u_1,\dots,u_s\} \cup \{v_1,\dots,v_r\}$ is a basis for $V$.\\
This needs to be proved:\\

(i) Show that $V = \text{Span}(B)$\\

Take $v \in V$. Then $Tv \in $ Im$(T)$, and so $Tv = \mu_1w_1 + \dots \mu_rw_r$, for some $\mu_i \in F$. Define $\bar{v} \in V$ by $\mu_1v_1 + \dots \mu_rv_r$. Then $T\bar{v} = \mu_1w_1 + \dots \mu_rw_r = Tv$.\\ So $v - \bar{v} \in $ Ker$(T)$ by Proposition 33. So $v - \bar{v} = \l_1u_1 + \dots + \l_su_s$ for some $\l_i \in F$. Now $v = \bar{v} + \l_1u_1 + \dots \l_su_s = \mu_1v_1 + \dots + \mu_rv_r + \l_1u_1 + \dots + \l_su_s \in \text{Span}(B)$.\\

(ii) Show that $B$ is linearly independent. Suppose that:
 \[\l_1u_1 + \dots \l_1u_s + \mu_1v_1 + \dots + \mu_rv_r = \mathbf{0} ~(*)\]
 
 We want to show that $\l_i = 0$ and $\mu_i = 0$ for all $i$. Apply $T$ to both sides of $(*$). Since $Tu_i = \mathbf{0}$ for all $i$, we get $\mu_1w_1 + \dots + \mu_rw_r = \mathbf{0}$. But $\{w_1,\dots,w_r\}$ is linearly independent (a basis for Im$(T)$). So $\mu_i = 0~~ \forall i$. Now from $(*)$, we have:
\[\l_1u_1 + \dots \l_su_s = \mathbf{0}\]

But $\{u_1,\dots,u_s\}$ is linearly independent (a basis for Ker$(T)$). So $\l_i = 0$ for all $i$. Hence $B$ is a basis.\\

 So dim $V = |B| = r + s = $ dim Im$(T) + $dim Ker$(T)$.
\end{proof}\vspace*{10pt}


\begin{corollary} Let $A$ be an $m\times n$ matrix. Let $U$ be the solution space to $Ax = \mathbf{0}$. Then:
\[\text{dim }U = n - \text{rk}(A)\]
\end{corollary}


\begin{proof}
Let $T$ be the matrix transformation $\mathbb{R}^n \to \mathbb{R}^m$ given by $Tx = Ax$. Then $rk(A) = $dim Im$(T)$, and Ker$(T) = U$. So the result follows by Theorem 37.
\end{proof}

\pagebreak
\subsektion{Matrix of Linear Transformation}

\begin{definition} \lecturemarker{9}{03/02}
 Let $V$ be a vector space over a field $F.$ Let $B = \{v_1,\dots,v_n\}$ be a basis for $V$. (Actually we should write $(v_1,\dots,v_n)$, because we care about the order of the basis vectors. But that notation could be confusing, so we'll continue to use set brackets).
 
 Any $v \in V$ can be written uniquely as a linear combination $v = \lambda_1v_1 + \dots \l_nv_n$. Define the \emph{vector of $V$ with respect to $B$} to be $[V]_B$ = $(\l_1 \dots \l_n)^T$.
 \end{definition}\vspace*{10pt}


\begin{examples} \begin{itemize}
 \item[(a)] Let $V = \mathbb{R}^n$, and let $E$ be the standard basis $\{e_1,\dots,e_n\}$, where $e_i = (0\dots,1,\dots,0)^T$ (1 in the $i$th position). Let $v = (a_1, \dots ,a_n)^T.$ Then $[V]_E = V$, since $v = a_1e_1 + \dots + a_ne_n$.
 \item[(b)] Let $V = \mathbb{R}^2$, and let $B = \{\underset{v_1}{(1,1)^T},\underset{v_2}{(0,1)^T}\}$. Let $v = (1,3)^T$. What is $[v]_B$? We have to solve $V = \l_1v_1 + \l_2v_2$. We have the matrix equation:
 \[\begin{pmatrix}
| &  | \\ v_1 & v_2 \\ | & |
\end{pmatrix} 
\begin{pmatrix}
 \l_1 \\ \l_2 \end{pmatrix} = v. \quad \text{ So: }
 \begin{pmatrix}
1 & 0 \\ 1 & 1
\end{pmatrix} 
\begin{pmatrix}
 \l_1 \\ \l_2 \end{pmatrix} = \begin{pmatrix}
 1 \\ 3
 \end{pmatrix}\]


Solve by row reducing:
\[\begin{amatrix}{2}
1 & 0  & 1 \\ 1 & 1 & 3
\end{amatrix}
\rightarrow 
\begin{amatrix}{2}
1 & 0 & 1 \\ 0 & 1 & 2 
\end{amatrix}
\]

from which we find $\l_1 = 1, \l_2 = 2$. So $[v]_B = (1,2)^T$
\end{itemize} 
\end{examples}\vspace*{10pt}

\begin{definition} Let $V$ be a vector space of dimension $n$ over $F$. Let $B = \{v_1,\dots,v_n\}$ be a basis. Let $T : V \to V$ be a linear transformation. For all $i \in \{1,\dots,n\}$, we have $Tv_i = a_{1i}v_1 + a_{2i}v_2 + \dots a_{ni}v_n$.

\noindent The matrix of $T$ with respect to the basis $B$, is:

\[[T]_B = \begin{pmatrix}
 a_{11} & a_{12} & \dots & a_{1n}\\
 \vdots & &  & \vdots \\
 a_{n1} & a_{n2} & \dots & a_{nn}
 \end{pmatrix}
\]
\end{definition}

Notice that the $i$th column of $[T]_B$ is $[Tv_i]_B$.\\


\begin{examples}
\begin{itemize}
\item[(a)] $T: \mathbb{R}^2 \to \mathbb{R}^2$ is given by:
\[
T\begin{pmatrix}
x_1 \\ x_2
\end{pmatrix}
=
\begin{pmatrix}
2x_1 - x_2 \\ x_1 + 2x_2
\end{pmatrix}
= 
\begin{pmatrix}
2 & -1 \\ 1 & 2 
\end{pmatrix}\begin{pmatrix}
x_1 \\ x_2
\end{pmatrix}
\]


Take $E$ to be the standard basis $\{\underset{e_1}{(1,0)^T},\underset{e_2}{(0,1)^T}\}$\\
Notice that $Te_1 = (2,1)^T = 2e_1 + e_2$, $Te_2 = (-1,2)^T = -e_1 + 2e_2$\\

So $[T]_E = \begin{pmatrix}
 2 & -1 \\ 1 & 2
 \end{pmatrix}
$, the matrix we started with. 

\item[(b)] Let $B = \{\underset{v_1}{(1,1)^T},\underset{v_2}{(0,1)^T}\}$. Let $T$ be as above. We have $T(v_1) = (1,3)^T$, which we know from Example 40, is $v_1 + 2v_2$. And $T(v_2) = (-1,2)^T$, which we can calculate is $-v_1 + 3v_2$.\\
 So $[T]_B = \begin{pmatrix}
 1 & -1 \\ 2 & 3
 \end{pmatrix}$\\

 (Observe that the matrices $\begin{pmatrix}
 2 & 1 \\ 1 & 2
 \end{pmatrix}$ and $\begin{pmatrix}
 1 & -1 \\ 2 & 3
 \end{pmatrix}$ have many features in common - same determinants and same trace. So the same characteristic polynomial. This is not an accident). 

\item[(c)] Let $V$ be the space of polynomials of degree $\leq 2$ in a variable $x$ over $\mathbb{R}$. Let $T: V \to V$ be defined by $T(f(x)) = \frac{d}{dx}f(x)$.\\

We have the basis $B = \{1,x,x^2\}$ for $V$. Then:
\[
\begin{aligned}
T(1) = 0 = 0.1 + 0.x + 0.x^2 \\
T(x) = 1 = 1.1 + 0.x + 0.x^2 \\
T(x^2) = 2x = 0.1 + 2.x + 0.x^2
\end{aligned}\]


So $[T]_B = \begin{pmatrix}
 0 & 1 & 0 \\ 0 & 0 & 2 \\ 0 & 0 & 0
 \end{pmatrix}$.\\
 
 Take a polynomial $f(x) \in V$. Write $f(x)$ in terms of $B$ as $f(x) = a.1 + b.x + c.x^2$.\\
 
 So $[xf(x)]_B = \begin{pmatrix}
 a \\ b \\ c
 \end{pmatrix}
$. Now $\begin{pmatrix}
 0 & 1 & 0 \\ 0 & 0 & 2 \\ 0 & 0 & 0

\end{pmatrix}\begin{pmatrix} 
a \\ b \\ c
\end{pmatrix} = \begin{pmatrix}
 b \\ 2c \\ 0
 \end{pmatrix}$\\
 
 And $\frac{d}{dx} f(x) = b + 2cx$, so $[Tf(x)]_B = (b, 2c, 0)^T = [T]_B[f(x)]_B$\\
 This \emph{always} happens. 
\end{itemize}
\end{examples}\vspace*{10pt}

\begin{proposition} Let $T: V \to V$ be a linear transformation. Let $B$ be a basis for $V$. Let $v \in V$. Then $[Tv]_B = [T]_B[v]_B$. 	
\end{proposition}


\begin{proof}
Let $B = \{v_1,\dots,v_n\}$. Let $v = \l_1v_1 + \dots \l_nv_n$. Let $[T]_B = (a_{ij})$\\
\[
\begin{aligned}
\text{We  have: } Tv &= T(\l_1v_1 + \dots \l_nv_n) \\
& = \l_1Tv_1 + \dots \l_nTv_n
\end{aligned}\]

But $Tv_i = a_{1i}v_1 a_{2i}v_2 + \dots + a_{ni}v_n$\\
 \[\begin{aligned}
\text{So } Tv &= \sum_{i = 1}^{n} \l_i(\sum_{j = 1}^n a_{ji}v_j) \\
&= \sum_{j = 1}^n \left(\sum_{i=1}^n \l_i a_{ji}\right) v_j
\end{aligned}\]
(interchanging the order of summation).
So  \[
\begin{aligned}
[Tv]_B &= \begin{pmatrix}
 \displaystyle{\sum_{i=1}^n \l_i a_{1i}}\\
 \displaystyle{\sum_{i=1}^n \l_i a_{2i}}\\
 \vdots \\
\displaystyle{ \sum_{i=1}^n \l_i a_{ni} }
 \end{pmatrix}
 = 
 \begin{pmatrix}
 a_{11} & \dots & a_{1n} \\
 \vdots & & \vdots \\
 a_{n1} & \dots & a_{nn} 
 \end{pmatrix}
 \begin{pmatrix}
 \l_1 \\ \vdots \\ \l_n
 \end{pmatrix} \\\\
   &= [T]_B[v]_B \text{ as required.}
  \end{aligned}
\]
\end{proof}

\pagebreak

\subsektion{Eigenvalues and Eigenvectors}

\begin{definition}\lecturemarker{10}{04/02} 
 Let $T: V \to V$ be a linear transformation. We say that $v$, is an \emph{eigenvector}\index{Eigenvector} of $T$ if $v \neq \mathbf{0}$, and $Tv = \l v$ for some $\l \in F$. We say that $\l$ is an \emph{eigenvalue}\index{Eigenvalue} of $T$.\end{definition}\vspace*{10pt}

\begin{example} Let $V = \mathbb{R}^2$ and let $T\begin{pmatrix}
x_1 \\ x_2
\end{pmatrix} = \begin{pmatrix}
 3x_1 + x_2 \\ -x_1 + x_2
 \end{pmatrix}.$\\
 
 \noindent We see that $T\left(\begin{smallmatrix} 1\\-1 \end{smallmatrix}\right) = \left(\begin{smallmatrix}2\\-2 \end{smallmatrix}\right)  = 2\left(\begin{smallmatrix}1\\-1 \end{smallmatrix}\right) $, and so $\left(\begin{smallmatrix}1\\-1 \end{smallmatrix}\right) $
 is an eigenvector of $T$ with eigenvalue 2. \\
 
 Note that  $T\Spvek{x_1;x_2} = A\Spvek{x_1;x_2}$ where $A = \Spvek{3 ~~ 1; -1 ~~ 1}$. \\ \vspace*{10pt}
 
 We have $T\Spvek{x_1;x_2} = \l\Spvek{x_1;x_2} \iff A\Spvek{x_1;x_2} = \l\Spvek{x_1;x_2}$.\vspace*{5pt}
 
and so the eigenvectors and eigenvalues of $T$ are the same as those of $A$. (We know how to find those from M1GLA). This will always work if $T$ is a matrix transformation.
\end{example}
 
 \emph{How do we find eigenvalues and eigenvectors in general?}\\
 Use the fact that we can represent any $T: V \to V$ as a matrix transformation.\\
 
 \begin{proposition} Let $T: V \to V$ and let $B = \{v_1,\dots,v_n\}$ be a basis for $V$. Then:
 \begin{enumerate}
 \item[(i)] Eigenvalues of $T$ are the same as eigenvalues of the matrix $[T]_B$
 \item[(ii)] The eigenvectors of $T$ are those vectors $v$, such that $[v]_B$ is an eigenvector of $[T]_B$.  (So if $[v]_B = (\l_1,\dots,\l_n)^T$, then $v = \l_1v_1 + \dots \l_nv_n$.) 
 \end{enumerate}
 \end{proposition}

\begin{proof}
\[
\begin{aligned}
Tv = \l v & \iff [Tv]_B = [\l v]_B\\
& \iff [T]_B[v]_B = \l[v]_B \text{ by Proposition 43}\\
& \iff [v]_B \text{ is an eigenvector for } [T]_B \text{ with eigenvalue } \l
\end{aligned}\]
\end{proof}


\begin{example} $V$ = Space of polynomials in $x$ of degree $\leq 2$ over $\mathbb{R}$. Let $T: V \to V$ be given by $T(f(x)) = f(x+1) - f(x)$ [Ex: Check $T$ is linear]\\

\textbf{Question:} Calculate the eigenvalues and eigenvetors of $T$.\\

\textbf{Answer:} Let $B = \{1,x,x^2\},$ a basis for $V$. We have $T(1) = 0, T(x) = x+1 -x = 1, T(x^2) = (x+1)^2 - x^2 = 2x+1$.
\[[T]_B = \begin{pmatrix}
0 & 1 & 1 \\ 0 & 0 & 2 \\ 0 & 0 & 0
\end{pmatrix}
\] The characteristic polynomial is then:
\[
\begin{vmatrix}
\l & -1 & -1 \\ 0 & \l & -2 \\ 0 & 0 & \l
\end{vmatrix} = \l^3 \]

So the only eigenvalue is 0. Find eigenvectors $[T]_B$ by solving: 
\[\begin{amatrix}{3}
0 & 1 & 1 & 0\\
0 & 0 & 2 & 0\\
0 & 0 & 0 & 0
\end{amatrix}
\]

The solutions are vectors of the form $(a,0,0)^T$, so the eigenvectors of $[T]_B$ are $\{(a,0,0)^T ~~ a \in F\backslash \{0\} \}$. So the eigenvectors of $T$ are polynomials $f(x)$ such that $[f(x)]_B =  (a,0,0)^T ~~(a \neq 0)$\\

So these are polynomials $a.1 + 0.x + 0.x^2$, which are the non-zero constant polynomials.\end{example}

\subsektion{Diagonalisation of Linear Transformations}
\begin{proposition} Let $T: V \to V$ be a linear transformation. Let $B$ be a basis for $V$. Then $[T]_B$ is a \emph{diagonal}\index{Diagonal Matrix} if and only if every basis vector in $B$ is an eigenvector for $T$.	
\end{proposition}


\begin{proof}
Let $e_1,\dots,e_n$ be the standard basis vectors, $(1,0,\dots,0)^T, (0,1,\dots,0)^T$ etc. in $F^n$, where $n =$ dim$V$. Let $A$ be an $n \times n$ matrix. Then $A$ is diagonal  if and only if $e_1$ is an eigenvector of $A$ for all $i$.\\

 So $[T]_B$ is diagonal if and only if all the $e_i$ are eigenvectors.\\
 
 But $e_i = [v_i]_B,$ where $B= \{v_1,\dots,v_n\}$, since $v_i = 0v_1 + 0v_2 + \dots + 1v_i + \dots 0v_n$. So $e_i$ is an eigenvector for $[T]_B$ if and only if $v_i$ is an eigenvector for $T$, by Proposition 46. 
\end{proof}

\begin{definition} A linear transformation $T: V \to V$ is \emph{diagonalisable}\index{DIagonalisability} if there is a basis of $V$ such that every element of $V$ is an eigenvector of $T$.\end{definition}\vspace*{10pt}


\begin{examples}\begin{itemize}
\item[(a)] $V,T$ as in Example 47. $T(f(x)) = f(x+1) - f(x)$.\\
 \emph{Is $T$ diagonalisable?}.\\
 
We calculated earlier that the eigenvectors of $T$ all lie in $Span\{1 + 0.x + 0.x^2\}$ which is a one-dimensional subspace of $V$. So the eigenvectors do not span $V$, and so there is no basis of $V$ consisting of the eigenvector of $T$. So $T$ is not diagonalisable. 

\item[(b)] $V$ as before (polynomial space of degree $\leq 2$)\\
Define $T: V\to V$ by $T(f(x)) = f(1-x)$, a basis for $V$.\\ (Exercise: check $T$ is linear).\\ We have $T(1) = 1,~ T(x) = 1-x,~ T(x^2) = (1-x)^2 = 1-2x + x^2.$ So:
\[
\begin{pmatrix}
1 & 1 & 1 \\ 0 & -1 & -2 \\ 0 & 0 & 1
\end{pmatrix}
\]

The characteristic polynomial is $(\l-1)^2(\l+1)$ [the diagonals are the roots]. So the eigenvalues are 1,-1. \\

We need to know whether there exists two linearly independent eigenvectors with eigenvalue 1. Using M1GLA techniques...
\[
\left(\begin{smallmatrix}
1 \\ 0 \\0
\end{smallmatrix} \right) \text{ and } \left(\begin{smallmatrix}
0 \\ 1 \\-1
\end{smallmatrix} \right)\]
are both eigenvectors with eigenvalue 1. They are linearly independent. We have $\left(\begin{smallmatrix}
1 \\ -2 \\0
\end{smallmatrix} \right)
$ as an eigenvector with eigenvalue -1.
\[\text{So } C = \{(1,0,0)^T,(0,1,-1)^T,(1,-2,0)^T\}\] is a basis for $V$, whose elements are eigenvectors of $T.$\\ (Or $C = \{1, x-x^2,1-2x\}$). \\

So $T$ is diagonalisable and $[T]_C = \begin{pmatrix}
 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & -1
 \end{pmatrix}$
 
 \end{itemize}
 \end{examples}
 
 \subsektion{Change of Basis}
 

 Let   \lecturemarker{11}{06/02} 
 $V$ be a vector space. Let $B = \{v_1,\dots,v_n\}$ and $C = \{w_1,\dots,w_n\}$. \\
 For $j$ in $\{1, \dots,n\}$, we can write $w_j = \l_{1j}v_1 + \l_{2j}v_2 + \dots \l_{nj}v_n $.
 
 Write:\[
 P =
 \begin{pmatrix}
 \l_{11} & \l_{12} & \dots & \l_{1n}\\
 \vdots & &  & \vdots \\
 \l_{n1} & \l_{n2} & \dots & \l_{nn}
 \end{pmatrix} = (\l_{ij})
 \]
 
 So the $j$th column of this matrix of $P$ is $[w_j]_B$.\\
 
\begin{proposition}~ \begin{enumerate}
 \item[(i)] $P = [T]_B$, where $T$ is the unique linear transformation $V \to V$, such that $Tv_i = w_i$, for all $i$.
 \item[(ii)] For all vectors $v \in V$, we have $P[v]_C = [v]_B$. 
 
 \end{enumerate}
 \end{proposition}

\begin{proof} (i)
We know that $[T]_B[v_i]_B = [Tv_i]_B = [w_i]_B$. And  $[T]_B$ is the only matrix with this property. So it is enough to show that $P[v_i]_B = [w_i]_B$.\\

But $[v_i]_B = e_i = (0,\dots,1,\dots,0)^T$ (1 in $i$th row), and so $P[v_i]_B =  Pe_i = i$th column of $P$ is $[w_i]_B$, as above.
\end{proof}

\begin{proof} (ii)
First note that $P[w_i]_C = Pe_i = [w_i]_B$, as we saw in (i). \\

Now if $v \in V$ then we can write $v = a_1w_1 + \dots + a_nw_n, ~a_i \in F.$ Now $[v]_C = (a_1,\vdots,a_n)^T = a_1e_1 + \dots a_ne_n$. So:
\[
 \begin{aligned}P[v]_C &= a_1Pe_1 + \dots + a_nPe_n\\
 & = a_1[w_1]_B + \dots a_n[w_n]_B \\
 &= [a_1w_1 + \dots a_nw_n]_B\qedhere
 \end{aligned}
\]
\end{proof}\vspace*{10pt}

\begin{definition}The Matrix $P$ as defined above is the \emph{change of basis matrix}\index{Change of Basis Matrix} from $B$ to $C$.	
\end{definition}\vspace*{10pt}


\begin{proposition} Let $V, B, C, P$ be all as above. Let $T: V \to V$ be a linear transformation. 
\begin{enumerate}
\item[(i)] $P$ is invertible, and its inverse is the change of basis matrix from $C$ to $B$.
\item[(ii)] $[T]_C = P^{-1}[T]_BP$
\end{enumerate}
\end{proposition}

\begin{proof} (i)
Let $Q$ be the change of basis matrix from $C$ to $B$. Then $Q[v]_B = [v]_C$, for all $v \in V$. We calculate:

\[PQe_i = PQ[v_i]_B = P[v_i]_C = [v_i]_B = e_i\]

It follows that $PQ = I$, and so $Q = P^{-1}$.
\end{proof}

\begin{proof} (ii)
\[\begin{aligned}
P^{-1}[T]_BPe_i &= P^{-1}[T]_BP[w_i]_C\\
 &= P^{-1}[T]_B[w_i]_B\\
 &= P^{-1}[Tw_i]_B \\
 &=  [Tw_i]_C\\
 &= [T]_C[w_i]_C\\
 &= [T]_C~e_i
\end{aligned}
\]

So the $i$th column of $P^{-1}[T]_BP$ is the same as the $i$th column of $[T]_C$ for all $i$, and so $P^{-1}[T]_BP = [T]_C$, as required.
\end{proof} \vspace*{10pt}

\begin{example} Let $V = \mathbb{R}^2$, and let $T: V \to V$ be given by:
\[
T\Spvek{x_1;x_2} = 
\Spvek[c]{x_2; -2x_1 + 3x_2}
= 
\begin{pmatrix}
0 & 1 \\ -2 & 3
\end{pmatrix}
\Spvek{x_1;x_2}
\]

Let $B = E = \{e_1,e_2\}$. Let $C = \{(1,1)^T,(1,2)^T\}$. The basis elements of $C$ are eigenvectors of the matrix $\begin{pmatrix}
0 & 1 \\ -2 & 3
\end{pmatrix}$. \\

The change of basis matrix from $B$ to $C$ is $P = \begin{pmatrix}
 1 & 1\\ 1 & 2
 \end{pmatrix}$.\\
 
 We have $[T]_B = \begin{pmatrix}
 0 & 1 \\ -2 & 3
 \end{pmatrix}$\\
 
 \[
 \begin{aligned}
 [T]_C &= P^{-1}[T]_BP\\
 &= \begin{pmatrix}
 1 & 1 \\ 1 & 2
 \end{pmatrix}^{-1}
 \begin{pmatrix}
 0 & 1 \\ -2 & 3
 \end{pmatrix}
\begin{pmatrix}
 1 & 1\\ 1 & 2
 \end{pmatrix}
 \end{aligned}\]
 
 so we have diagonalised $T$. (M1GLA - $D = P^{-1}AP$)
 \end{example}
 
 
\begin{center}
  
  \textsf{\textbf{- End of Linear Algebra -}}	
  \end{center}




